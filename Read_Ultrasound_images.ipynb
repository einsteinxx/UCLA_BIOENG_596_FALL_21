{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Read_Ultrasound_images",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpXcozzJ+jaG1i9pw+l9Iq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/einsteinxx/UCLA_BIOENG_596_FALL_21/blob/main/Read_Ultrasound_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NkBEnltCrpg",
        "outputId": "b380eba4-a5b5-41a1-d3b4-01b2c51f7b6a"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import gc  #debug memory leaks in matplotlib\n",
        "import csv #read in description files\n",
        "import random #used to select random slice for patches\n",
        "import cv2\n",
        "\n",
        "\n",
        "!pip3 install -q torchinfo\n",
        "!pip3 install -Uqq ipdb\n",
        "################################################################################\n",
        "#ULTRASOUND NEEDS\n",
        "import PIL\n",
        "#from PIL import Image\n",
        "# Open the image form working directory\n",
        "#image = Image.open(full_file)\n",
        "from matplotlib import image\n",
        "from ast import literal_eval #used to break out bounding boxes from strings\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import torchinfo\n",
        "\n",
        "from torchinfo import summary\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "\n",
        "\n",
        "import torchinfo\n",
        "\n",
        "\n",
        "import ipdb\n",
        "################################################################################\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "#\n",
        "# Read Data from google drive\n",
        "#\n",
        "from google.colab import drive #for loading gdrive data\n",
        "from google.colab import files\n",
        "\n",
        "# install dependencies not included by Colab\n",
        "# use pip3 to ensure compatibility w/ Google Deep Learning Images \n",
        "!pip3 install -q pydicom \n",
        "!pip3 install -q tqdm \n",
        "!pip3 install -q imgaug\n",
        "!pip3 install -q pickle5\n",
        "\n",
        "import pydicom #to read dicom files\n",
        "from pydicom import dcmread\n",
        "import pickle5 as pickle; #generic storage of image arra\n",
        "\n",
        "# Load data from google drive\n",
        "#\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data_dir = '/content/gdrive/Shareddrives/BreastUS'\n",
        "local_dir = '/content/gdrive/My Drive/BreastUS' #for local storage\n",
        "\n",
        "\n",
        "'''\n",
        "top_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES' \n",
        "csv_dir = '/content/gdrive/My Drive/DBT_DATA/TRAINING_DATA/'\n",
        "\n",
        "\n",
        "#output patch save dir\n",
        "patch_normal_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES/NORMAL'\n",
        "patch_actionable_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES/ACTIONABLE' \n",
        "patch_benign_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES/BENIGN'\n",
        "patch_cancer_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES/CANCER'\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "### Enable GPU, if present\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if (train_on_gpu):\n",
        "    !nvidia-smi -L\n",
        "    !nvidia-smi \n",
        "    dev=torch.device(\"cuda\")\n",
        "else:\n",
        "    print('GPU NOT FOUND!!! USING CPU INSTEAD!!!!!')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-e94b80b6-9cee-1bd7-11e3-15d5aaad0e0d)\n",
            "Fri Nov 26 09:20:51 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    29W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdVnU-1rU__i"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jui81Q6f872p",
        "outputId": "b1b627fe-6af8-4ade-bb53-81cb00f53fbf"
      },
      "source": [
        "#\n",
        "# Get DICOM info\n",
        "#\n",
        "local_files = os.listdir(local_dir)\n",
        "\n",
        "for dicom_file in local_files:\n",
        "    filename = os.path.join(local_dir,dicom_file)\n",
        "    if (os.path.isdir(filename) == 1):\n",
        "        #skip any directories found in list\n",
        "        continue\n",
        "    if ('dcm' in filename):\n",
        "        print(filename)\n",
        "        ds = dcmread(filename, force=True)\n",
        "        for element in ds:\n",
        "            print(element)\n",
        "\n",
        "    else:\n",
        "        print('Non-dicom file found ',filename)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/BreastUS/1.dcm\n",
            "(0008, 0005) Specific Character Set              CS: 'ISO_IR 100'\n",
            "(0008, 0008) Image Type                          CS: ['DERIVED', 'PRIMARY', 'SMALL PARTS']\n",
            "(0008, 0012) Instance Creation Date              DA: '20211111'\n",
            "(0008, 0013) Instance Creation Time              TM: '132938'\n",
            "(0008, 0016) SOP Class UID                       UI: Ultrasound Multi-frame Image Storage\n",
            "(0008, 0018) SOP Instance UID                    UI: 1.2.840.113663.1500.1.365952523.3.2.20210204.132938.671\n",
            "(0008, 0020) Study Date                          DA: '20211111'\n",
            "(0008, 0021) Series Date                         DA: '20211111'\n",
            "(0008, 0023) Content Date                        DA: '20211111'\n",
            "(0008, 002a) Acquisition DateTime                DT: '20211111'\n",
            "(0008, 0030) Study Time                          TM: '130201'\n",
            "(0008, 0031) Series Time                         TM: '130201'\n",
            "(0008, 0033) Content Time                        TM: '132938'\n",
            "(0008, 0050) Accession Number                    SH: 'A_TR6Z2X0X'\n",
            "(0008, 0060) Modality                            CS: 'US'\n",
            "(0008, 0068) Presentation Intent Type            CS: 'FOR PRESENTATION'\n",
            "(0008, 0070) Manufacturer                        LO: 'Philips Medical Systems'\n",
            "(0008, 0080) Institution Name                    LO: 'BKWIC,SM,CA 90404 RM2'\n",
            "(0008, 0090) Referring Physician's Name          PN: 'LEE^MINNA'\n",
            "(0008, 1010) Station Name                        SH: 'USSM'\n",
            "(0008, 1030) Study Description                   LO: 'RIGHT BREAST BIOPSY ULTRASOUND GUIDED'\n",
            "(0008, 1032) Procedure Code Sequence             SQ: <Sequence, length 1>\n",
            "(0008, 103e) Series Description                  LO: 'MM US RT BREAST BIOPSY'\n",
            "(0008, 1070) Operators' Name                     PN: ''\n",
            "(0008, 1090) Manufacturer's Model Name           LO: 'iU22'\n",
            "(0008, 1111) Referenced Performed Procedure Step SQ: <Sequence, length 1>\n",
            "(0008, 2111) Derivation Description              ST: ''\n",
            "(0009, 0000) Private Creator                     UL: 14\n",
            "(0009, 0010) Private tag data                    LO: 'GEIIS'\n",
            "(0010, 0010) Patient's Name                      PN: '10088_1_0K0L7T04'\n",
            "(0010, 0020) Patient ID                          LO: '10088_1_0K0L7T04'\n",
            "(0010, 0021) Issuer of Patient ID                LO: '001R41:20090813:023040546:015090'\n",
            "(0010, 0030) Patient's Birth Date                DA: '19000101'\n",
            "(0010, 0040) Patient's Sex                       CS: ''\n",
            "(0010, 1020) Patient's Size                      DS: None\n",
            "(0010, 1030) Patient's Weight                    DS: None\n",
            "(0010, 1040) Patient's Address                   LO: 'MQ DE-IDENTIFIED'\n",
            "(0010, 2160) Ethnic Group                        SH: 'MQ DE-IDENTIFIED'\n",
            "(0018, 1000) Device Serial Number                LO: '365952523'\n",
            "(0018, 1020) Software Versions                   LO: 'PMS5.1 Ultrasound iU22_6.0.2.144'\n",
            "(0018, 1030) Protocol Name                       LO: 'Free Form'\n",
            "(0018, 1063) Frame Time                          DS: '31.154'\n",
            "(0018, 1088) Heart Rate                          IS: '0'\n",
            "(0018, 5010) Transducer Data                     LO: ['L12_5', '', '']\n",
            "(0018, 5020) Processing Function                 LO: 'SM_PRTS_ADV_BREAST'\n",
            "(0018, 6011) Sequence of Ultrasound Regions      SQ: <Sequence, length 1>\n",
            "(0020, 000d) Study Instance UID                  UI: 1.2.840.114350.2.300.2.798268.2.463966434.1\n",
            "(0020, 000e) Series Instance UID                 UI: 1.2.840.113663.1500.1.365952523.2.1.20210204.130201.984\n",
            "(0020, 0010) Study ID                            SH: '465241817'\n",
            "(0020, 0011) Series Number                       IS: '1'\n",
            "(0020, 0013) Instance Number                     IS: '1'\n",
            "(0028, 0002) Samples per Pixel                   US: 3\n",
            "(0028, 0004) Photometric Interpretation          CS: 'RGB'\n",
            "(0028, 0006) Planar Configuration                US: 0\n",
            "(0028, 0008) Number of Frames                    IS: '1'\n",
            "(0028, 0009) Frame Increment Pointer             AT: (0018, 1063)\n",
            "(0028, 0010) Rows                                US: 600\n",
            "(0028, 0011) Columns                             US: 800\n",
            "(0028, 0014) Ultrasound Color Data Present       US: 0\n",
            "(0028, 0100) Bits Allocated                      US: 8\n",
            "(0028, 0101) Bits Stored                         US: 8\n",
            "(0028, 0102) High Bit                            US: 7\n",
            "(0028, 0103) Pixel Representation                US: 0\n",
            "(0028, 0301) Burned In Annotation                CS: 'YES'\n",
            "(0028, 2110) Lossy Image Compression             CS: '01'\n",
            "(0028, 2112) Lossy Image Compression Ratio       DS: '0.0'\n",
            "(0032, 1032) Requesting Physician                PN: 'LEE^MINNA^K.^^MD'\n",
            "(0032, 1033) Requesting Service                  LO: 'Unspecified'\n",
            "(0040, 0244) Performed Procedure Step Start Date DA: '20211111'\n",
            "(0040, 0245) Performed Procedure Step Start Time TM: '130201'\n",
            "(0040, 0253) Performed Procedure Step ID         SH: '20211111'\n",
            "(0040, 0254) Performed Procedure Step Descriptio LO: 'MM US RT BREAST BIOPSY'\n",
            "(0040, 0260) Performed Protocol Code Sequence    SQ: <Sequence, length 1>\n",
            "(0040, 0280) Comments on the Performed Procedure ST: 'Breast'\n",
            "(0903, 0010) Private Creator                     LO: 'GEIIS PACS'\n",
            "(0903, 1010) [Reject Image Flag]                 US: 0\n",
            "(0903, 1011) [Significant Flag]                  US: 0\n",
            "(0903, 1012) [Confidential Flag]                 US: 0\n",
            "(0905, 0010) Private Creator                     LO: 'GEIIS'\n",
            "(0905, 1030) [Assigning Authority For Patient ID LO: '001R41:20090813:023040546:015090'\n",
            "(2050, 0020) Presentation LUT Shape              CS: 'IDENTITY'\n",
            "(7fd1, 0000) Private Creator                     UL: 350\n",
            "(7fd1, 0010) Private tag data                    LO: 'GEIIS'\n",
            "(7fd1, 1010) [GE IIS Compression ID]             UL: 17\n",
            "(7fd1, 1020) [GE IIS Multiframe Offsets]         UL: Array of 79 elements\n",
            "(7fe0, 0000) Group Length                        UL: 113760012\n",
            "(7fe0, 0010) Pixel Data                          OB: Array of 1440000 elements\n",
            "Non-dicom file found  /content/gdrive/My Drive/BreastUS/Copy of us_status_index.pickle\n",
            "Non-dicom file found  /content/gdrive/My Drive/BreastUS/us_status_index2nd.pickle\n",
            "Non-dicom file found  /content/gdrive/My Drive/BreastUS/us_status_index.pickle\n",
            "Non-dicom file found  /content/gdrive/My Drive/BreastUS/counters_to_remove.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afyEeFFcVJCa"
      },
      "source": [
        "def get_csv_data(filename):\n",
        "    fields = []\n",
        "    rows = []\n",
        "\n",
        "    # reading csv file\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        # creating a csv reader object\n",
        "        csvreader = csv.reader(csvfile)\n",
        "        \n",
        "        # extracting field names through first row\n",
        "        fields = next(csvreader)\n",
        "\n",
        "        # extracting each data row one by one\n",
        "        for row in csvreader:\n",
        "            rows.append(row)\n",
        "\n",
        "\n",
        "        # lines present\n",
        "        print(\"found rows: %d\"%(csvreader.line_num))\n",
        "    return fields, rows"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X-TI57eLjrY"
      },
      "source": [
        "#Load a dicom image and convert it using dcm_read to get view type (needs this \n",
        "#to correctly align image))\n",
        "\n",
        "csv_dir = os.path.join(data_dir,'Annotated data')\n",
        "\n",
        "annotated_dir = os.path.join(data_dir,'Annotated data')\n",
        "data_files = os.listdir(annotated_dir)\n",
        "\n",
        "\n",
        "label_data_dir = os.path.join(annotated_dir,'LabelMe_3.0_format_updated')\n",
        "label_files = os.listdir(label_data_dir)\n",
        "\n",
        "\n",
        "\n",
        "#sample_dicom = os.path.join(dicom_dir,dicom_files[0]) #for now, just use first\n",
        "\n",
        "#!ls '/content/gdrive/Shareddrives/BreastUS'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzhCbP7tbCX8"
      },
      "source": [
        "# GET CSV INFO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrtsfNXHWsUX",
        "outputId": "787a8b46-07cb-43d5-bdd2-ab539861b680"
      },
      "source": [
        "#READ CSV FILES\n",
        "#pull out the box information and label info\n",
        "csv_list = os.listdir(csv_dir)\n",
        "\n",
        "for csv_file in csv_list:\n",
        "    filename = os.path.join(csv_dir,csv_file)\n",
        "    if (os.path.isdir(filename) == 1):\n",
        "        #skip any directories found in list\n",
        "        continue\n",
        "    if ('_final' in csv_file):\n",
        "        annotation_fields, annotation_rows = get_csv_data(filename)\n",
        "\n",
        "    else:\n",
        "        print('Non-archive file found ',filename)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found rows: 8843\n",
            "Non-archive file found  /content/gdrive/Shareddrives/BreastUS/Annotated data/annotations_updated.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnV_94fziBBe"
      },
      "source": [
        "#Sort data into fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3nGek1gaHBY",
        "outputId": "000df5ba-de6a-4b4d-ae11-6c63bb1526a1"
      },
      "source": [
        "\n",
        "\n",
        "print(np.shape(annotation_rows))\n",
        "\n",
        "print(annotation_fields)\n",
        "\n",
        "array_rows = np.array(annotation_rows)\n",
        "mrn = array_rows[:,1]\n",
        "accession = array_rows[:,2]\n",
        "video_id = array_rows[:,3]\n",
        "frame_id = array_rows[:,4]\n",
        "image_path = array_rows[:,5]\n",
        "bounding_box = array_rows[:,6]\n",
        "diagnosis = array_rows[:,7]\n",
        "biopsy_site = array_rows[:,8]\n",
        "diagnosis2 = array_rows[:,9]\n",
        "first50 = array_rows[:,10]\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8842, 11)\n",
            "['', 'coded_mrn', 'coded_accession', 'video_id', 'frame_id', 'image_path', 'bounding box', 'Final Diagnosis', 'Bx Site', 'Final Diagnosis.1', 'First 50']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne5HF42dCdpe"
      },
      "source": [
        "# CROPPING IMAGES "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RuH5Med3z7n"
      },
      "source": [
        "def crop_us_image(img, debug =0):\n",
        "################################################################################\n",
        "#\n",
        "## Look for coordinates to cut the US data out of the image overlay\n",
        "################################################################################\n",
        "\n",
        "\n",
        "#\n",
        "# Get valid rows for the embedded US image\n",
        "#\n",
        "    nrows, ncols = np.shape(img)\n",
        "    middle_column = np.uint(ncols/2)\n",
        "    middle_row = np.uint(nrows/2)\n",
        "\n",
        "    #find the 0 gaps before and after the image on the column\n",
        "    data_strip = img[:,middle_column]\n",
        "    index = np.where(data_strip == 0) #find first section of 0 values\n",
        "    first_row_gap = index[0][0]\n",
        "    diff_row = np.diff(index)\n",
        "\n",
        "\n",
        "    #find the large jump in diff, that will likely be US image then down to background 0\n",
        "    #second_row_gap = np.where(diff_row >= 150)\n",
        "    if (debug == 1):\n",
        "        print('diff in func ',diff_row)\n",
        "\n",
        "    leading_found = 0\n",
        "    trailing_found = 0\n",
        "    leading_row_edge=0\n",
        "    for counter,ii in enumerate(diff_row[0]):\n",
        "        case_status = 1 #assume case will be usable, if not change flag\n",
        "        if ((counter > 2) and \n",
        "            (counter < (np.size(diff_row)-6) )):\n",
        "\n",
        "            #find a set of diff 0s that skip a distance (edge)\n",
        "            if ( (diff_row[0][counter-3] <5 ) and\n",
        "                (diff_row[0][counter-2] <5) and\n",
        "                (diff_row[0][counter-1] <5) and\n",
        "                (ii> 10) and\n",
        "                (leading_found ==0)):\n",
        "                \n",
        "                leading_row_edge = index[0][counter]\n",
        "                leading_found = 1\n",
        "                \n",
        "                #print('**found a leading edge at counter, val, image row', counter,ii, leading_row_edge)\n",
        "                continue\n",
        "                 #only need first point\n",
        "            '''\n",
        "            if ( (diff_row[0][counter+3] ==1 ) and\n",
        "                (diff_row[0][counter+2] ==1) and\n",
        "                (diff_row[0][counter+1] ==1) and\n",
        "                (ii>=2) and \n",
        "                (leading_found == 1)):\n",
        "            '''\n",
        "            if (leading_found ==1): #(counter > leading_row_edge):\n",
        "\n",
        "                for jj in range(0,6):\n",
        "                    if (diff_row[0][counter+jj] <=5):\n",
        "                        numzero =1\n",
        "                    else:\n",
        "                        numzero = 0\n",
        "                        break\n",
        "                \n",
        "                if (numzero == 1):\n",
        "                    #print('finding trail at ',counter)\n",
        "                    trailing_row_edge=index[0][counter]\n",
        "                    trailing_found = 1 #fix this\n",
        "                    #print('!!found a trail edge at counter, val, image row',\n",
        "                    #    counter,ii, trailing_row_edge)\n",
        "                    break\n",
        "                 #only need first point\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        low_row = leading_row_edge #index[0][second_row_gap[1][0]]\n",
        "        high_row = trailing_row_edge #index[0][second_row_gap[1][0]+1] \n",
        "    except:\n",
        "        print('could not find a second row for img')\n",
        "        \n",
        "        skip_points = [0,0,0,0]\n",
        "        case_status = 0\n",
        "        return img, skip_points, case_status #0 signifies this one failed, remove it\n",
        "    #\n",
        "    # Clip columns by finding rows with continuous 0 vals. Find the diff between \n",
        "    # elements and take the first point where the black transitions to a non-zero \n",
        "    # value. This is the first cutoff. The second is where the transition from \n",
        "    # regular data to zeros occurs\n",
        "    #\n",
        "    data_rows = img[(np.uint(nrows*0.25), np.uint(nrows*0.5), np.uint(nrows*0.75)),:]\n",
        "    \n",
        "    if (debug == 1):\n",
        "        plt.figure()\n",
        "        x=np.arange(0,ncols)\n",
        "        plt.plot(x,data_rows[0,:],'r.-',x,data_rows[1,:], 'b.-',x,data_rows[2,:],'k.-')\n",
        "        \n",
        "\n",
        "    #top 75% row should be clear of all burned images at the beginning\n",
        "    xx = np.ediff1d(data_rows[2,:])\n",
        "    start= np.where(xx >00)\n",
        "\n",
        "    \n",
        "    if (not start): #empty list is FALSE\n",
        "        case_status = 0\n",
        "        skip_points = [0,0,0,0]\n",
        "        print('Could not find enough column points, skipping')\n",
        "        return img, skip_points, case_status #0 signifies this one failed, remove it\n",
        "\n",
        "    try:\n",
        "        start_column = start[0][0] +1 #since it's a diff, add one pixel\n",
        "        final_column = start[0][-1] -1 #last point where diff >0\n",
        "    except:\n",
        "        print('Failed to get a \"where\" point. Skipping')\n",
        "        case_status = 0\n",
        "        skip_points = [0,0,0,0]\n",
        "        return img, skip_points, case_status #0 signifies this one failed, remove it\n",
        "\n",
        "    '''\n",
        "    print(start[0])\n",
        "    plt.figure()\n",
        "    plt.imshow(img[low_row:high_row,start_column:final_column],cmap='gray')\n",
        "    plt.title('cropped')\n",
        "    '''\n",
        "\n",
        "    #print('inside cropping, shape is ', np.shape(img))\n",
        "    skip_points = [low_row, high_row,start_column, final_column]\n",
        "    return img[low_row:high_row,start_column:final_column], skip_points, case_status"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z7OLshP-VcO"
      },
      "source": [
        "if(0):\n",
        "    num_mrn = set(mrn)\n",
        "    num_mrn\n",
        "    image_path[0:3]\n",
        "\n",
        "    print(image_path[0])\n",
        "    filename = os.path.basename(image_path[0]) \n",
        "    [_,fpath] =image_path[0].split('drive/MyDrive/Annotated data/')\n",
        "    full_file = os.path.join(annotated_dir,fpath)\n",
        "    print(full_file)\n",
        "\n",
        "    print('filename exists: ',os.path.exists(full_file))\n",
        "    import imageio\n",
        "\n",
        "    #import PIL\n",
        "    #from PIL import Image\n",
        "    # Open the image form working directory\n",
        "    #image = Image.open(full_file)\n",
        "\n",
        "    #from matplotlib import image\n",
        "    #from matplotlib import pyplot\n",
        "    img = image.imread(full_file)\n",
        "\n",
        "\n",
        "\n",
        "    ## Convert the RGB input into grayscale\n",
        "    R, G, B = img[:,:,0], img[:,:,1], img[:,:,2]\n",
        "    imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.imshow(imgGray, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "    cropped_image,skip_points,case_status = crop_us_image(imgGray,0)\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.imshow(cropped_image, cmap='gray')\n",
        "    plt.title('cropped image')\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5jgDw8p9Lg3"
      },
      "source": [
        "if(0):\n",
        "    #del cropped_image,  img_data\n",
        "    img_data = image.imread(idata)\n",
        "    ## Convert the RGB input into grayscale\n",
        "    R, G, B = img_data[:,:,0], img_data[:,:,1], img_data[:,:,2]\n",
        "    imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "    print('shape of imgGray is ', np.shape(imgGray))\n",
        "    cropped_image, _ = crop_us_image(imgGray,0)\n",
        "    if (np.size(cropped_image) <1):\n",
        "        print('Failure during cropping for ', file_name)\n",
        "    print('shape is ', np.shape(cropped_image))\n",
        "    plt.figure()\n",
        "    plt.imshow(cropped_image)\n",
        "\n",
        "    cropped_image = cv2.resize(cropped_image, dsize=(480, 600),\n",
        "                                interpolation=cv2.INTER_CUBIC)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xClpqcOB55BT"
      },
      "source": [
        "### SAVE IMAGES TO DISK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "039DO_cZorqH"
      },
      "source": [
        "if(0):\n",
        "    #['', 'coded_mrn', 'coded_accession', 'video_id', 'frame_id', 'image_path', 'bounding box', 'Final Diagnosis', 'Bx Site', 'Final Diagnosis.1', 'First 50']\n",
        "    '''\n",
        "    mrn = array_rows[:,1]\n",
        "    accession = array_rows[:,2]\n",
        "    video_id = array_rows[:,3]\n",
        "    frame_id = array_rows[:,4]\n",
        "    image_path = array_rows[:,5]\n",
        "    bounding_box = array_rows[:,6]\n",
        "    diagnosis = array_rows[:,7]\n",
        "    biopsy_site = array_rows[:,8]\n",
        "    diagnosis2 = array_rows[:,9]\n",
        "    first50 = array_rows[:,10]\n",
        "    '''\n",
        "\n",
        "    mrn_list = set(mrn)\n",
        "\n",
        "    acc =set()\n",
        "    vid = set()\n",
        "\n",
        "    #try first US video in first mrn\n",
        "\n",
        "\n",
        "    for count,mm in enumerate(mrn):\n",
        "        \n",
        "        if mm in mrn[0]:\n",
        "            acc.add(accession[count])\n",
        "            vid.add(video_id[count])\n",
        "\n",
        "    screen_dir = '/content/gdrive/My Drive/BreastUS/SCREEN_CAPS/'\n",
        "    #video_save = os.path.join()\n",
        "\n",
        "\n",
        "    file_name =[]\n",
        "    vname=[]\n",
        "    boundary_box = []\n",
        "    for cc,video_temp in enumerate(video_id):\n",
        "        [_,fpath] =image_path[cc].split('drive/MyDrive/Annotated data/')\n",
        "        full_file = os.path.join(annotated_dir,fpath)\n",
        "        file_name.append(full_file)\n",
        "        vname.append(video_temp)\n",
        "        boundary_box.append(bounding_box[cc])\n",
        "\n",
        "    print(file_name[0])\n",
        "    print(vname[0])\n",
        "    print(len(file_name))\n",
        "\n",
        "\n",
        "    case_holder = []\n",
        "    failed_list = [] #record of images that failed cropping\n",
        "\n",
        "    skip_to = 8002 #-1\n",
        "    for count,ii in enumerate(file_name):\n",
        "\n",
        "        if (count < skip_to):\n",
        "            continue #skip these until we get back to the proper file\n",
        "\n",
        "        #\n",
        "        # Save folder information. Get the video name and make a folder with that \n",
        "        # long name in the screen caps folder\n",
        "        #\n",
        "        video_save = os.path.join(screen_dir,vname[count])\n",
        "        if(os.path.exists(video_save)):\n",
        "            print('folder found')\n",
        "        else:\n",
        "            os.mkdir(video_save)\n",
        "\n",
        "        img_data = image.imread(ii)\n",
        "        #print(np.shape(img_data))\n",
        "        #print('Processing: ', ii)\n",
        "        print(count)\n",
        "\n",
        "\n",
        "        corners=literal_eval(boundary_box[count]) #bounding_box[count])\n",
        "        #bounding box values are in (x,y) formats from xml\n",
        "        #print('corners, len = ',corners, len(corners))\n",
        "        #print('bounding box = ',boundary_box[count]) #bounding_box[count])\n",
        "\n",
        "        '''\n",
        "        Bounding box ordering is \n",
        "        point 1        point 2\n",
        "        ----------------------\n",
        "        point 4        point 3\n",
        "        '''\n",
        "        if (len(corners)>0):\n",
        "            #print('corners ',pos)\n",
        "            pos = np.uint(corners)\n",
        "            xmin = pos[0][0]\n",
        "            xmax = pos[1][0]\n",
        "            ymin = pos[0][1]\n",
        "            ymax = pos[2][1]\n",
        "            #boxes.append([xmin, ymin, xmax, ymax])\n",
        "            h=ymax-ymin\n",
        "            w=xmax-xmin\n",
        "        else:\n",
        "            #print('EMPTY bounding box')\n",
        "            xmin=np.uint(1)\n",
        "            xmax=np.uint(2)\n",
        "            ymin=np.uint(1)\n",
        "            ymax=np.uint(2)\n",
        "            h=0\n",
        "            w=0\n",
        "            #boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "\n",
        "\n",
        "        #skip points =[low_row, high_row,start_column, final_column]\n",
        "        ## Convert the RGB input into grayscale\n",
        "        R, G, B = img_data[:,:,0], img_data[:,:,1], img_data[:,:,2]\n",
        "        imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "\n",
        "\n",
        "        #\n",
        "        # Crop the image to the Ultrasound borders. If it fails during this \n",
        "        # process, mark it and continue to next image. That failed index will be\n",
        "        # stored to remove any failed images later\n",
        "        #\n",
        "        cropped_image, skip_points,fail_flag = crop_us_image(imgGray,0)\n",
        "\n",
        "\n",
        "        show_plots = 0\n",
        "        if (fail_flag == 1): #if cropping worked, process annotations\n",
        "\n",
        "            #print('skip points = ',skip_points)\n",
        "            #print('h = ymax-ymin',ymax,ymin,h)\n",
        "            offset_row = ymin-skip_points[0]\n",
        "            offset_col = xmin-skip_points[2]\n",
        "\n",
        "            if (len(corners)>0):\n",
        "                width = xmax-xmin\n",
        "                height = ymax-ymin\n",
        "                rect = patches.Rectangle((offset_col,offset_row),width,height,linewidth=1,edgecolor='r',facecolor='none')\n",
        "                #print('adding rectangle ',offset_row, offset_col, height, width)\n",
        "                #print(ymin, ymax, xmin, xmax)\n",
        "            else:\n",
        "                rect = patches.Rectangle((0,0),0,0,linewidth=1,edgecolor='r',facecolor='none')\n",
        "\n",
        "            if (show_plots == 1):\n",
        "                show_crop = 0\n",
        "                if (show_crop == 0):\n",
        "\n",
        "                    fig = plt.figure(figsize=(8, 6), dpi=80)\n",
        "                    #plt.figure(figsize=(8, 6), dpi=80)\n",
        "                    #plt.imshow(cropped_image, cmap='gray')\n",
        "                    plt.imshow(cropped_image,cmap='gray')\n",
        "                    # Get the current reference\n",
        "                    ax = plt.gca()\n",
        "                    # Add the patch to the Axes\n",
        "                    ax.add_patch(rect)\n",
        "\n",
        "                    fname = os.path.join(video_save,os.path.basename(ii))\n",
        "                    #fname = os.path.basename(ii)\n",
        "                    title_text = 'Frame ' + str(fname)\n",
        "                    plt.title(title_text)\n",
        "                    \n",
        "                    plt.savefig(os.path.join(screen_dir,str(fname)))\n",
        "                    #time.sleep(1)\n",
        "                    #plt.show()\n",
        "                    plt.close()\n",
        "\n",
        "\n",
        "                if (show_crop == 1):  \n",
        "                    if (len(corners)>0):\n",
        "                        plt.figure(figsize=(8, 6), dpi=80)\n",
        "                        plt.imshow(img_data,cmap='gray')\n",
        "                \n",
        "                    # Get the current reference\n",
        "                        \n",
        "                        rect = patches.Rectangle((corners[0][0],corners[0][1]),w,h,linewidth=1,edgecolor='r',facecolor='none')\n",
        "                        bx = plt.gca()\n",
        "                        # Add the patch to the Axes\n",
        "                        bx.add_patch(rect)\n",
        "                        plt.show()\n",
        "                        plt.close()\n",
        "\n",
        "        else:\n",
        "            print('Failed crop for : ',file_name)\n",
        "            failed_list.append(count)\n",
        "        #save the fail list and file name list to remove fails later\n",
        "        if (count%1000 == 0):\n",
        "            status_file = os.path.join(local_dir,'us_status_index2nd.pickle')\n",
        "\n",
        "            pickle.dump([failed_list],open( status_file, \"wb\" ),protocol=5 )\n",
        "\n",
        "\n",
        "    print('finished plotting all images')\n",
        "\n",
        "\n",
        "    status_file = os.path.join(local_dir,'us_status_index2nd.pickle')\n",
        "    pickle.dump([failed_list],open( status_file, \"wb\" ),protocol=5 )\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    #print('filename exists: ',os.path.exists(full_file))\n",
        "\n",
        "    #image = image.imread(full_file)\n",
        "    #print(np.shape(image))\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knTF2v1bBNao"
      },
      "source": [
        "if(0):\n",
        "    status_file = os.path.join(local_dir,'Copy of us_status_index.pickle')\n",
        "    f1 = pickle.load( open( status_file, \"rb\" ) )\n",
        "\n",
        "    print(len(f1[0]))\n",
        "\n",
        "\n",
        "\n",
        "    status_file = os.path.join(local_dir,'us_status_index2nd.pickle')\n",
        "    f2 = pickle.load( open( status_file, \"rb\" ) )\n",
        "\n",
        "    print(len(f2[0]))\n",
        "\n",
        "    bad_count = f1[0]+f2[0]\n",
        "    print(len(bad_count))\n",
        "\n",
        "    bad_files = os.path.join(local_dir,'counters_to_remove.pickle')\n",
        "    pickle.dump([bad_count],open( bad_files, \"wb\" ),protocol=5 )"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu6rJqoou_3n"
      },
      "source": [
        "if(0):\n",
        "    #Test out problem images\n",
        "    #idata = '/content/gdrive/Shareddrives/BreastUS/Annotated data/unzipped_updated/1_srh8gfhj_a_0629s3fh_0.mp4-2021_07_15_17_21_15-labelme 3.0.zip/default/frame_000036.PNG'\n",
        "    idata = file_name[8002]\n",
        "    img_data = image.imread(idata)\n",
        "    print(np.shape(img_data))\n",
        "\n",
        "    %pdb off\n",
        "\n",
        "\n",
        "    ## Convert the RGB input into grayscale\n",
        "    R, G, B = img_data[:,:,0], img_data[:,:,1], img_data[:,:,2]\n",
        "    imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.imshow(imgGray,cmap='gray')\n",
        "\n",
        "    cropped_image, skip_points,fail_flag = crop_us_image(imgGray,1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMS1eAmkyjYr"
      },
      "source": [
        "if(0):\n",
        "    idata = '/content/gdrive/Shareddrives/BreastUS/Annotated data/unzipped_updated/1_2w948u10_a_06g91d42_4.mp4-2021_08_20_17_24_25-labelme 3.0.zip/default/frame_000159.PNG'\n",
        "    idata = '/content/gdrive/Shareddrives/BreastUS/Annotated data/unzipped_updated/1_srh8gfhj_a_0629s3fh_0.mp4-2021_07_15_17_21_15-labelme 3.0.zip/default/frame_000036.PNG'\n",
        "\n",
        "    img_data = image.imread(idata)\n",
        "    print(np.shape(img_data))\n",
        "\n",
        "\n",
        "    ## Convert the RGB input into grayscale\n",
        "    R, G, B = img_data[:,:,0], img_data[:,:,1], img_data[:,:,2]\n",
        "    imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.imshow(imgGray,cmap='gray')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    nrows, ncols = np.shape(imgGray)\n",
        "    middle_column = np.uint(ncols/2)\n",
        "    middle_row = np.uint(nrows/2)\n",
        "\n",
        "    #find the 0 gaps before and after the image on the column\n",
        "    data_strip = imgGray[:,middle_column]\n",
        "    index = np.where(data_strip == 0) #find first section of 0 values\n",
        "    print('shape of index= ',np.shape(index))\n",
        "    print('shape of data strip = ',np.shape(data_strip))\n",
        "    print('data_strip 0:20 ',data_strip[0:20])\n",
        "    print('index 0:20 ', index[0][:])\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.plot(data_strip)\n",
        "    plt.ylabel('intensity')\n",
        "    plt.xlabel('rows')\n",
        "    ttext = 'Strip Slice at column ' +  str(middle_column)\n",
        "    plt.title(ttext)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(data_strip[400:500])\n",
        "    plt.title('zoomed')\n",
        "\n",
        "\n",
        "\n",
        "    first_row_gap = index[0][0]\n",
        "    diff_row = np.diff(index[0])\n",
        "\n",
        "    print('diff ' , diff_row)\n",
        "\n",
        "    for counter,ii in enumerate(diff_row):\n",
        "        if (counter > 2):\n",
        "            #find a set of diff 0s that skip a distance (edge)\n",
        "            if ( (diff_row[counter-3] <5 ) and\n",
        "                (diff_row[counter-2] <5) and\n",
        "                (diff_row[counter-1] <5) and\n",
        "                (ii> 50)):\n",
        "                print('found an edge at ', counter,ii, index[0][counter])\n",
        "                second_row_gap = index[0][counter]\n",
        "                break #only need first point\n",
        "\n",
        "    #find the large jump in diff, that will likely be US image then down to background 0\n",
        "    #second_row_gap = np.where(diff_row >= 150)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('Second Row Gap ',second_row_gap)\n",
        "    plt.figure()\n",
        "    plt.plot(data_strip,'r.-')\n",
        "\n",
        "\n",
        "\n",
        "    cropped_image, skip_points = crop_us_image(imgGray,1)\n",
        "    plt.figure()\n",
        "    plt.imshow(cropped_image,cmap='gray')\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjiPjns1ekPQ"
      },
      "source": [
        "### test code\n",
        "if(0):\n",
        "    '''\n",
        "    DOES NOT WORK!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    '''\n",
        "    import cv2\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    idata = '/content/gdrive/Shareddrives/BreastUS/Annotated data/unzipped_updated/1_2w948u10_a_06g91d42_4.mp4-2021_08_20_17_24_25-labelme 3.0.zip/default/frame_000159.PNG'\n",
        "\n",
        "    import cv2\n",
        "\n",
        "    # Load the image\n",
        "    img = cv2.imread(idata)\n",
        "\n",
        "    # convert to grayscale\n",
        "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    edged = cv2.Canny(img, 170, 490)\n",
        "    # Apply adaptive threshold\n",
        "    thresh = cv2.adaptiveThreshold(edged, 1, 1, 1, 11, 2)\n",
        "    thresh_color = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # apply some dilation and erosion to join the gaps - change iteration to detect more or less area's\n",
        "    thresh = cv2.dilate(thresh,None,iterations = 15)\n",
        "    thresh = cv2.erode(thresh,None,iterations = 15)\n",
        "\n",
        "    # Find the contours\n",
        "    contours,hierarchy = cv2.findContours(thresh,\n",
        "                                            cv2.RETR_TREE,\n",
        "                                            cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # For each contour, find the bounding rectangle and draw it\n",
        "    for cnt in contours:\n",
        "        x,y,w,h = cv2.boundingRect(cnt)\n",
        "        cv2.rectangle(img,\n",
        "                        (x,y),(x+w,y+h),\n",
        "                        (0,255,0),\n",
        "                        2)\n",
        "\n",
        "    cv2_imshow(img)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N19fiKU3fE7t"
      },
      "source": [
        "if(0):\n",
        "    #\n",
        "    # Test images as 3d volumes\n",
        "    #\n",
        "    for counter, ii in enumerate(file_name):\n",
        "        img_data = image.imread(ii)\n",
        "        print(np.shape(img_data))\n",
        "        if (counter >0):\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "        ## Convert the RGB input into grayscale\n",
        "        R, G, B = img_data[:,:,0], img_data[:,:,1], img_data[:,:,2]\n",
        "        imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "        cropped_image = crop_us_image(imgGray)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        #img = cv2.imread('your_image.jpg')\n",
        "        cropped_image = cv2.resize(cropped_image, dsize=(600, 480), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        nr,nc = np.shape(cropped_image)\n",
        "        img_volume = np.zeros((3,480,600)) #nr,nc))\n",
        "        img_volume[0,:,:] = cropped_image\n",
        "        img_volume[1,:,:] = cropped_image\n",
        "        img_volume[2,:,:] = cropped_image\n",
        "        \n",
        "        \n",
        "        plt.figure(figsize=(8, 6), dpi=80)\n",
        "        plt.imshow(img_volume[0,:,:], cmap='gray')\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrkfWeZJB7Iv"
      },
      "source": [
        "# Bounding Box information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB8180Q-rGe7",
        "outputId": "0866979f-c022-4a4a-d7ba-6a46f6cf6db1"
      },
      "source": [
        "#\n",
        "# Bounding box info\n",
        "#\n",
        "import torch\n",
        "print('-------')\n",
        "uvids = set(video_id)\n",
        "\n",
        "for v in uvids:\n",
        "    for count,ii in enumerate(video_id):\n",
        "        if (v in ii):\n",
        "            #get bb info\n",
        "            box_info = bounding_box[count]\n",
        "            #box_info = box_info.strip('][') #.split(', ')\n",
        "\n",
        "            if (not (box_info =='[]')):\n",
        "                \n",
        "                corners=literal_eval(box_info)\n",
        "                bbox = torch.FloatTensor(corners)\n",
        "                #print(corners)\n",
        "            else:\n",
        "                continue\n",
        "            "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNbFf0FAskGM"
      },
      "source": [
        "def get_bb_stats(bounding_box):\n",
        "    total_area=[]\n",
        "    num_box=0\n",
        "    num_h=[]\n",
        "    num_w=[]\n",
        "    for index in range(0,len(bounding_box)):\n",
        "\n",
        "        corners=literal_eval(bounding_box[index])\n",
        "\n",
        "        boxes = []\n",
        "        area = 0\n",
        "        pos=literal_eval(bounding_box[index])\n",
        "\n",
        "        #pos = np.double(pos)\n",
        "        if (len(pos) !=0): #(pos):\n",
        "            #print('corners ',pos)\n",
        "            pos = np.int32(pos)\n",
        "            xmin = pos[0][1]\n",
        "            xmax = pos[2][1]\n",
        "            ymin = pos[0][0]\n",
        "            ymax = pos[1][0]\n",
        "            w=xmax-xmin\n",
        "            h = ymax-ymin\n",
        "            num_h.append(h)\n",
        "            num_w.append(w)\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            area += (xmax-xmin)*(ymax-ymin)\n",
        "            total_area.append(area)\n",
        "            num_box+=1\n",
        "    return total_area, num_box,num_h, num_w"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EoFrtsmB_PNG",
        "outputId": "a714bdc4-0395-4b51-fdf1-1ac87b88852b"
      },
      "source": [
        "#bounding_box\n",
        "ta,box_num,num_h, num_w=get_bb_stats(bounding_box)\n",
        "\n",
        "avg_area = sum(ta)/len(ta)\n",
        "print(avg_area)\n",
        "\n",
        "#avg width\n",
        "print(np.average(num_w))\n",
        "#get smallest box\n",
        "\n",
        "print('smallest width is ', np.min(num_w))\n",
        "print('largest width is  ', np.max(num_w))\n",
        "\n",
        "#avg height\n",
        "print(np.average(num_h))\n",
        "print('smallest height is ', np.min(num_h))\n",
        "print('largest height is  ',np.max(num_h))\n",
        "\n",
        "vv,_,_=plt.hist(num_h, bins = 50)\n",
        "plt.title('h')\n",
        "print('-----',vv)\n",
        "\n",
        "for ii in range(0,5):\n",
        "    bin_max = np.max(vv[ii*10:ii*10+9])\n",
        "    print('max Height bin is ', ii,bin_max)\n",
        "\n",
        "vv,_,_=plt.hist(num_w, bins = 50)\n",
        "plt.title('w')\n",
        "print('-----',vv)\n",
        "\n",
        "for ii in range(0,5):\n",
        "    bin_max = np.max(vv[ii*10:ii*10+9])\n",
        "    print('max Width bin is ', ii,bin_max)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(num_h, bins=50,alpha=0.5, label=\"Heights\")\n",
        "plt.hist(num_w,bins=50,alpha=0.5, label=\"Widths\")\n",
        "plt.title('Histogram of Annotation Heights')\n",
        "plt.ylabel('Number of Occurences')\n",
        "plt.xlabel('Annotation Length (pixels)')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(num_w,bins=50,alpha=0.5, label=\"Widths\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57268.8385847562\n",
            "171.43091005104736\n",
            "smallest width is  55\n",
            "largest width is   359\n",
            "276.0306284104911\n",
            "smallest height is  47\n",
            "largest height is   574\n",
            "----- [124.   0.   0.  66.  26. 120. 143. 317. 183. 288. 186. 369. 109. 232.\n",
            " 268. 232. 161. 180.   0.  48.  29. 207.  89. 115.   6. 280.  92.  29.\n",
            "  81.   1.   2.  87. 232. 168.   9. 164.  56.  82.  50.   0. 125.  85.\n",
            " 104.   0. 178.   0.  68. 161.  55.  74.]\n",
            "max Height bin is  0 317.0\n",
            "max Height bin is  1 369.0\n",
            "max Height bin is  2 280.0\n",
            "max Height bin is  3 232.0\n",
            "max Height bin is  4 178.0\n",
            "----- [176.   0. 212.  98.  39. 245. 178. 116. 281. 137. 250. 157. 102. 180.\n",
            " 347. 296. 432. 309.  91.   4.   0.  88. 169.  81.   0. 220. 202. 163.\n",
            "   0.  88.  87.  60.   0.   0.   0.   0.   0.   0.   0.   0. 125. 233.\n",
            "  85.  91.   0.  83.   0. 178.   0.  78.]\n",
            "max Width bin is  0 281.0\n",
            "max Width bin is  1 432.0\n",
            "max Width bin is  2 220.0\n",
            "max Width bin is  3 87.0\n",
            "max Width bin is  4 233.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([176.,   0., 212.,  98.,  39., 245., 178., 116., 281., 137., 250.,\n",
              "        157., 102., 180., 347., 296., 432., 309.,  91.,   4.,   0.,  88.,\n",
              "        169.,  81.,   0., 220., 202., 163.,   0.,  88.,  87.,  60.,   0.,\n",
              "          0.,   0.,   0.,   0.,   0.,   0.,   0., 125., 233.,  85.,  91.,\n",
              "          0.,  83.,   0., 178.,   0.,  78.]),\n",
              " array([ 55.  ,  61.08,  67.16,  73.24,  79.32,  85.4 ,  91.48,  97.56,\n",
              "        103.64, 109.72, 115.8 , 121.88, 127.96, 134.04, 140.12, 146.2 ,\n",
              "        152.28, 158.36, 164.44, 170.52, 176.6 , 182.68, 188.76, 194.84,\n",
              "        200.92, 207.  , 213.08, 219.16, 225.24, 231.32, 237.4 , 243.48,\n",
              "        249.56, 255.64, 261.72, 267.8 , 273.88, 279.96, 286.04, 292.12,\n",
              "        298.2 , 304.28, 310.36, 316.44, 322.52, 328.6 , 334.68, 340.76,\n",
              "        346.84, 352.92, 359.  ]),\n",
              " <a list of 50 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM1oXD-itHex",
        "outputId": "0fc8dcfb-8875-4e83-92f3-20a7c9aac9a7"
      },
      "source": [
        "#generate stats on bounding boxes\n",
        "uvids = set(video_id)\n",
        "\n",
        "box_collect=[]\n",
        "frames_collect=[]\n",
        "empty_annotations=[]\n",
        "for count, v in enumerate(uvids):\n",
        "    number_frames = 0\n",
        "    number_boxes = 0\n",
        "    \n",
        "    #print('---- loading: ', v)\n",
        "    for icount,ii in enumerate(video_id):\n",
        "        if (ii == v):\n",
        "            box_info = bounding_box[icount]\n",
        "            number_frames+=1\n",
        "            if (not (box_info =='[]')):\n",
        "                number_boxes+=1\n",
        "            else:\n",
        "                pass\n",
        "                #print('no annotation', frame_id[icount])\n",
        "    frames_collect.append(number_frames)\n",
        "    box_collect.append(number_boxes)\n",
        "    #print(v,number_frames,number_boxes)\n",
        "    if (number_boxes == 0):\n",
        "        empty_annotations.append(v)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Number of videos: ',np.size(frames_collect[:]))\n",
        "print('Number of annotated cases ',np.size(box_collect))\n",
        "print('Videos with no Annotations:', empty_annotations)\n",
        "    #get_bb_stats(v, video_id,bounding_box, image_path)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of videos:  102\n",
            "Number of annotated cases  102\n",
            "Videos with no Annotations: ['1_jq3lf35n_a_12h3r1u8_2.mp4-2021_07_15_17_21_26-labelme 3.0.zip', '1_jq3lf35n_a_12h3r1u8_0.mp4-2021_07_15_17_21_32-labelme 3.0.zip', '1_en9w0q3l_a_e128787p_2.mp4-2021_09_22_21_42_11-labelme 3.0.zip', '1_srh8gfhj_a_0629s3fh_0.mp4-2021_07_15_17_21_15-labelme 3.0.zip', '1_jq3lf35n_a_12h3r1u8_4.mp4-2021_07_15_17_21_22-labelme 3.0.zip', '1_jq3lf35n_a_12h3r1u8_1.mp4-2021_07_15_17_21_37-labelme 3.0.zip', '1_jq3lf35n_a_12h3r1u8_5.mp4-2021_07_15_17_21_17-labelme 3.0.zip', '1_2w948u10_a_06g91d42_5.mp4-2021_07_15_17_21_37-labelme 3.0.zip', '1_2w948u10_a_06g91d42_2.mp4-2021_07_15_17_21_25-labelme 3.0.zip', '1_srh8gfhj_a_0629s3fh_3.mp4-2021_07_15_17_21_17-labelme 3.0.zip', '1_2w948u10_a_06g91d42_0.mp4-2021_07_15_17_21_18-labelme 3.0.zip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxsXie0YGRUg"
      },
      "source": [
        "#GET COORDINATES FROM CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c-aUiEvo7RW"
      },
      "source": [
        "def get_coordinates(video_id,video_series):\n",
        "#video_series is the video_id for a series of PNG images taken from a movie.\n",
        "#Anything with this id will be a frame set that should be kept together\n",
        "\n",
        "#uvids = set(video_id)\n",
        "\n",
        "    bbox = []\n",
        "    for count,v in enumerate(video_id): #video_series:\n",
        "        if (v in video_series):\n",
        "            box_info = bounding_box[count]\n",
        "            #box_info = box_info.strip('][') #.split(', ')\n",
        "\n",
        "            if (not (box_info =='[]')):\n",
        "                \n",
        "                corners=literal_eval(box_info)\n",
        "                bbox.append(torch.FloatTensor(corners))\n",
        "                print(corners)\n",
        "            else:\n",
        "                bbox.append(torch.FloatTensor(0))\n",
        "\n",
        "    return bbox\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV73Fx3_s32K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f11909-5fa1-4717-a0f7-0856f8585a06"
      },
      "source": [
        "video_id[0]\n",
        "boxes = get_coordinates(video_id,video_id[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWp6S054HoNv"
      },
      "source": [
        "#                         **** Data Loaders ****\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxErahOxJmLb"
      },
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 img_dir,\n",
        "                 label_data,\n",
        "                 category=[],\n",
        "                 file_count=1,\n",
        "                 file_list =[],\n",
        "                 transform=None,\n",
        "                 target_transform=None):\n",
        "        #self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.category = category\n",
        "        self.file_count = file_count\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.category_name =''\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "        self.imgs = file_list\n",
        "\n",
        "\n",
        "        '''\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "        '''\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image_dir = self.img_dir\n",
        "\n",
        "        file_name = self.file_list[index]\n",
        "\n",
        "\n",
        "        img_data = image.imread(file_name)\n",
        "        print('image shape read in is ',np.shape(img_data))\n",
        "\n",
        "\n",
        "        ## Convert the RGB input into grayscale\n",
        "        R, G, B = img_data[:,:,0], img_data[:,:,1], img_data[:,:,2]\n",
        "        imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "        cropped_image, skip_points,ff = crop_us_image(imgGray,0)\n",
        "        if (np.size(cropped_image) <1):\n",
        "            print('!!!!!!!!!!!!!!!!!!!Failure during cropping for ', file_name)\n",
        "            #we should exit out, but put a fake image for now\n",
        "            cropped_image = np.zeros((400,600))\n",
        "            print('!!!!!!!!!!!!!!!!!!! ZERO IMAGE PASSED IN')\n",
        "        print('about to resize image. shape going in is @',np.shape(cropped_image), index)\n",
        "        cropped_image = np.array(cropped_image)\n",
        "        print('image after crop is ', np.shape(cropped_image))\n",
        "        #if(type(image) == type(None)):\n",
        "        #    pass\n",
        "        #else:\n",
        "        #    image = cv2.resize(image, (h, w), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        cropped_image = cv2.resize(cropped_image,\n",
        "                                   dsize=(800,600), #(480,600)\n",
        "                                   interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        nr,nc = np.shape(cropped_image)\n",
        "        print('resized shapes are ',nr,nc)\n",
        "        '''\n",
        "        img = np.zeros((3,600,480), dtype=torch.DoubleTensor) #nr,nc))\n",
        "        img[0,:,:] = cropped_image.astype(torch.DoubleTensor)\n",
        "        img[1,:,:] = cropped_image.astype(torch.DoubleTensor)\n",
        "        img[2,:,:] = cropped_image.astype(torch.DoubleTensor)\n",
        "        '''\n",
        "\n",
        "\n",
        "        img = np.zeros((3,600,800), dtype=np.double)  #600,480), dtype=np.double) #nr,nc))\n",
        "        img[0,:,:] = cropped_image#.astype(np.double)\n",
        "        img[1,:,:] = cropped_image#.astype(np.double)\n",
        "        img[2,:,:] = cropped_image#.astype(np.double)\n",
        "        img=img.astype(np.double)\n",
        "\n",
        "        #img=torch.from_numpy(img)\n",
        "\n",
        "        #get label and pull category \n",
        "        #if (first50[index] == 'M'):\n",
        "        #    label = torch.as_tensor(1, dtype=torch.int64)\n",
        "        #else:\n",
        "        #    label = torch.as_tensor(0, dtype=torch.int64)\n",
        "            #label = torch.zeros([0], dtype=torch.int64)\n",
        "            \n",
        "        corners=literal_eval(bounding_box[index])\n",
        "                #bbox = torch.FloatTensor(corners)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(self.file_list) #bounding_box)\n",
        "        boxes = []\n",
        "        #area = 0\n",
        "        pos=literal_eval(bounding_box[index])\n",
        "\n",
        "\n",
        "        #if (len(corners)>0):\n",
        "        #    width = xmax-xmin\n",
        "        #    height = ymax-ymin\n",
        "        #    rect = patches.Rectangle((offset_col,offset_row),width,height,linewidth=1,edgecolor='r',facecolor='none')\n",
        "        #skip_points = [low_row, high_row,start_column, final_column]\n",
        "\n",
        "        #pos = np.double(pos)\n",
        "        if (len(pos) !=0): #(pos):\n",
        "            print('corners ',pos)\n",
        "            pos = np.int32(pos)\n",
        "            xmin = pos[0][0]\n",
        "            xmax = pos[1][0]\n",
        "            ymin = pos[0][1]\n",
        "            ymax = pos[2][1]\n",
        "            ### Correct for the cropped image as annotation points are for the \n",
        "            ### main image with subset\n",
        "            offset_row = ymin-skip_points[0]\n",
        "            offset_col = xmin-skip_points[2]\n",
        "\n",
        "            boxes.append([offset_col, offset_row, \n",
        "                          xmax-skip_points[2], \n",
        "                          ymax-skip_points[0]])\n",
        "            if (((xmax-skip_points[0]) < offset_col) or ((ymax-skip_points[2]) < offset_row)):\n",
        "                print('found a negative box!! ',  boxes)\n",
        "                print('skip points: ',skip_points)\n",
        "                print('offset col, row ',offset_col, offset_row)\n",
        "            #boxes.append([xmin, ymin, xmax, ymax])\n",
        "            area = (xmax-skip_points[0] - offset_col) * (ymax-skip_points[2] - offset_row) \n",
        "            #area += (xmax-xmin)*(ymax-ymin)#!! alter this\n",
        "        '''\n",
        "        else:\n",
        "            #empty_box = torch.zeros([0, 4)], dtype=torch.double)\n",
        "            #print('EMPTY bounding box')\n",
        "            xmin=np.int32(0) #torch.DoubleTensor(3)\n",
        "            xmax=np.int32(1) #torch.DoubleTensor(5)\n",
        "            ymin=np.int32(0) #torch.DoubleTensor(3)\n",
        "            ymax=np.int32(1) #torch.DoubleTensor(5)\n",
        "            boxes.append([0,0,0,0]) #[xmin, ymin, xmax, ymax])\n",
        "        '''\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        # Handle empty bounding boxes\n",
        "        if (len(pos) ==0): #num_objs == 0:\n",
        "            #boxes.append([0,0,0,0])\n",
        "            boxes.append([0,0,5,5]) #testing out something with an area\n",
        "\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.double)\n",
        "            #boxes = torch.zeros((0, 4), dtype=torch.double)\n",
        "            #area = 0 #10*10\n",
        "            area = torch.as_tensor(25, dtype=torch.double)\n",
        "\n",
        "            #label = torch.as_tensor(0, dtype=torch.int64) #put as background if no boxes\n",
        "            label = torch.zeros((1,), dtype=torch.int64)\n",
        "\n",
        "        else:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.double) \n",
        "            area = torch.as_tensor(area, dtype=torch.double)\n",
        "        \n",
        "            #label = torch.as_tensor(1, dtype=torch.int64)\n",
        "            label = torch.ones((1,), dtype=torch.int64)\n",
        "\n",
        "\n",
        "\n",
        "        print('shape of boxes is ', np.shape(boxes))\n",
        "\n",
        "\n",
        "        # there is only one class\n",
        "        #labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        #masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        #image_id = torch.tensor([idx])\n",
        "        #area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((len(self.imgs),), dtype=torch.double)\n",
        "\n",
        "        print('setting target')\n",
        "        print('label is ', label, type(label))\n",
        "        print('area is ', area)\n",
        "        \n",
        "\n",
        "\n",
        "        target = []\n",
        "        d = {}\n",
        "        d['boxes'] = boxes  #np.squeeze(boxes,0)\n",
        "        d['labels'] = label\n",
        "        d['image_id'] = torch.as_tensor(index, dtype=torch.double) \n",
        "        d['area'] = area \n",
        "        d['iscrowd'] = iscrowd \n",
        "        target.append(d)\n",
        "\n",
        "\n",
        "        '''\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = label\n",
        "        #target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = index #image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        '''\n",
        "        #self.transform(self.x_data[index]), self.transform(self.y_data[index])\n",
        "        #return {'image': torch.from_numpy(image),\n",
        "#                'landmarks': torch.from_numpy(landmarks)}\n",
        "        #return self.transform(img), self.transform(target)\n",
        "        \n",
        "        #print('image type before is ', type(img))\n",
        "        img = torch.as_tensor(img, dtype=torch.float32) #model has float32 \n",
        "        #img= torch.from_numpy(img)\n",
        "        #print('image type after is ', type(img))\n",
        "        #img = torchvision.transforms.ToTensor()(image)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI_gE6lyWv_y"
      },
      "source": [
        "#GENERATE ALL FILE PATHS "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "211LJzRiN9a1"
      },
      "source": [
        "full_file_list=[]\n",
        "for file_path in image_path:\n",
        "    #print(file_path)\n",
        "\n",
        "    filename = os.path.basename(file_path) \n",
        "    [_,fpath] =file_path.split('drive/MyDrive/Annotated data/')\n",
        "    full_file = os.path.join(annotated_dir,fpath)\n",
        "    full_file_list.append(full_file)\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44VxBT_tRpQ_"
      },
      "source": [
        "#REMOVE PRE-DETERMINED LIST OF UNCROPPABLE IMAGES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ7b9QluP_Qg",
        "outputId": "e1d0c63c-3ff7-42ba-dc98-959d8d946d0b"
      },
      "source": [
        "#Remove the bad counters \n",
        "bad_files = os.path.join(local_dir,'counters_to_remove.pickle')\n",
        "bad_count = pickle.load( open( bad_files, \"rb\" ) )\n",
        "print(len(bad_count[0]))\n",
        "\n",
        "new_flist =[]\n",
        "new_bb = []\n",
        "new_first50=[]\n",
        "for counter, fname in enumerate(full_file_list):\n",
        "    if counter in bad_count[0]:\n",
        "        print('found bad counter ',counter)\n",
        "    else:\n",
        "        new_flist.append(fname)\n",
        "        new_bb.append(bounding_box[counter])\n",
        "        new_first50.append(first50[counter])\n",
        "\n",
        "full_file_list = new_flist\n",
        "bounding_box = new_bb\n",
        "first50=new_first50\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1105\n",
            "found bad counter  637\n",
            "found bad counter  638\n",
            "found bad counter  748\n",
            "found bad counter  833\n",
            "found bad counter  834\n",
            "found bad counter  931\n",
            "found bad counter  932\n",
            "found bad counter  933\n",
            "found bad counter  934\n",
            "found bad counter  935\n",
            "found bad counter  936\n",
            "found bad counter  937\n",
            "found bad counter  938\n",
            "found bad counter  939\n",
            "found bad counter  940\n",
            "found bad counter  941\n",
            "found bad counter  942\n",
            "found bad counter  943\n",
            "found bad counter  944\n",
            "found bad counter  945\n",
            "found bad counter  946\n",
            "found bad counter  947\n",
            "found bad counter  948\n",
            "found bad counter  949\n",
            "found bad counter  950\n",
            "found bad counter  951\n",
            "found bad counter  952\n",
            "found bad counter  953\n",
            "found bad counter  954\n",
            "found bad counter  955\n",
            "found bad counter  956\n",
            "found bad counter  957\n",
            "found bad counter  958\n",
            "found bad counter  959\n",
            "found bad counter  960\n",
            "found bad counter  961\n",
            "found bad counter  962\n",
            "found bad counter  963\n",
            "found bad counter  964\n",
            "found bad counter  965\n",
            "found bad counter  966\n",
            "found bad counter  1106\n",
            "found bad counter  1107\n",
            "found bad counter  1108\n",
            "found bad counter  1109\n",
            "found bad counter  1110\n",
            "found bad counter  1111\n",
            "found bad counter  1112\n",
            "found bad counter  1113\n",
            "found bad counter  1114\n",
            "found bad counter  1115\n",
            "found bad counter  1116\n",
            "found bad counter  1117\n",
            "found bad counter  1118\n",
            "found bad counter  1119\n",
            "found bad counter  1120\n",
            "found bad counter  1121\n",
            "found bad counter  1122\n",
            "found bad counter  1123\n",
            "found bad counter  1124\n",
            "found bad counter  1125\n",
            "found bad counter  1126\n",
            "found bad counter  1127\n",
            "found bad counter  1128\n",
            "found bad counter  1129\n",
            "found bad counter  1130\n",
            "found bad counter  1131\n",
            "found bad counter  1132\n",
            "found bad counter  1133\n",
            "found bad counter  1134\n",
            "found bad counter  1135\n",
            "found bad counter  1136\n",
            "found bad counter  1137\n",
            "found bad counter  1138\n",
            "found bad counter  1139\n",
            "found bad counter  1140\n",
            "found bad counter  1141\n",
            "found bad counter  1167\n",
            "found bad counter  1168\n",
            "found bad counter  1169\n",
            "found bad counter  1170\n",
            "found bad counter  1171\n",
            "found bad counter  1172\n",
            "found bad counter  1173\n",
            "found bad counter  1174\n",
            "found bad counter  1175\n",
            "found bad counter  1176\n",
            "found bad counter  1177\n",
            "found bad counter  1187\n",
            "found bad counter  1188\n",
            "found bad counter  1189\n",
            "found bad counter  1190\n",
            "found bad counter  1191\n",
            "found bad counter  1192\n",
            "found bad counter  1193\n",
            "found bad counter  1194\n",
            "found bad counter  1195\n",
            "found bad counter  1196\n",
            "found bad counter  1197\n",
            "found bad counter  1198\n",
            "found bad counter  1199\n",
            "found bad counter  1200\n",
            "found bad counter  1201\n",
            "found bad counter  1202\n",
            "found bad counter  1203\n",
            "found bad counter  1204\n",
            "found bad counter  1205\n",
            "found bad counter  1206\n",
            "found bad counter  1207\n",
            "found bad counter  1208\n",
            "found bad counter  1209\n",
            "found bad counter  1210\n",
            "found bad counter  1211\n",
            "found bad counter  1212\n",
            "found bad counter  1213\n",
            "found bad counter  1214\n",
            "found bad counter  1215\n",
            "found bad counter  1216\n",
            "found bad counter  1217\n",
            "found bad counter  1218\n",
            "found bad counter  1219\n",
            "found bad counter  1220\n",
            "found bad counter  1221\n",
            "found bad counter  1222\n",
            "found bad counter  1313\n",
            "found bad counter  1314\n",
            "found bad counter  1315\n",
            "found bad counter  1316\n",
            "found bad counter  1317\n",
            "found bad counter  1318\n",
            "found bad counter  1319\n",
            "found bad counter  1320\n",
            "found bad counter  1321\n",
            "found bad counter  1322\n",
            "found bad counter  1323\n",
            "found bad counter  1324\n",
            "found bad counter  1325\n",
            "found bad counter  1326\n",
            "found bad counter  1327\n",
            "found bad counter  1328\n",
            "found bad counter  1329\n",
            "found bad counter  1330\n",
            "found bad counter  1331\n",
            "found bad counter  1332\n",
            "found bad counter  1333\n",
            "found bad counter  1334\n",
            "found bad counter  1335\n",
            "found bad counter  1336\n",
            "found bad counter  1337\n",
            "found bad counter  1338\n",
            "found bad counter  1507\n",
            "found bad counter  1508\n",
            "found bad counter  1509\n",
            "found bad counter  1510\n",
            "found bad counter  1511\n",
            "found bad counter  1512\n",
            "found bad counter  1513\n",
            "found bad counter  1514\n",
            "found bad counter  1515\n",
            "found bad counter  1516\n",
            "found bad counter  1517\n",
            "found bad counter  1518\n",
            "found bad counter  1519\n",
            "found bad counter  1520\n",
            "found bad counter  1521\n",
            "found bad counter  1522\n",
            "found bad counter  1523\n",
            "found bad counter  1524\n",
            "found bad counter  1525\n",
            "found bad counter  1526\n",
            "found bad counter  1527\n",
            "found bad counter  1528\n",
            "found bad counter  1529\n",
            "found bad counter  1530\n",
            "found bad counter  1531\n",
            "found bad counter  1532\n",
            "found bad counter  1533\n",
            "found bad counter  1534\n",
            "found bad counter  1535\n",
            "found bad counter  1536\n",
            "found bad counter  1537\n",
            "found bad counter  1538\n",
            "found bad counter  1539\n",
            "found bad counter  1540\n",
            "found bad counter  1541\n",
            "found bad counter  1542\n",
            "found bad counter  1590\n",
            "found bad counter  1591\n",
            "found bad counter  1592\n",
            "found bad counter  1593\n",
            "found bad counter  1594\n",
            "found bad counter  2397\n",
            "found bad counter  2398\n",
            "found bad counter  2399\n",
            "found bad counter  2400\n",
            "found bad counter  2401\n",
            "found bad counter  2402\n",
            "found bad counter  2403\n",
            "found bad counter  2404\n",
            "found bad counter  2405\n",
            "found bad counter  2406\n",
            "found bad counter  2407\n",
            "found bad counter  2408\n",
            "found bad counter  2409\n",
            "found bad counter  2410\n",
            "found bad counter  2411\n",
            "found bad counter  2412\n",
            "found bad counter  2413\n",
            "found bad counter  2414\n",
            "found bad counter  2415\n",
            "found bad counter  2416\n",
            "found bad counter  2417\n",
            "found bad counter  2418\n",
            "found bad counter  2419\n",
            "found bad counter  2420\n",
            "found bad counter  2421\n",
            "found bad counter  2422\n",
            "found bad counter  2423\n",
            "found bad counter  2424\n",
            "found bad counter  2425\n",
            "found bad counter  2426\n",
            "found bad counter  2429\n",
            "found bad counter  2684\n",
            "found bad counter  2858\n",
            "found bad counter  2859\n",
            "found bad counter  2860\n",
            "found bad counter  2867\n",
            "found bad counter  2870\n",
            "found bad counter  2871\n",
            "found bad counter  3110\n",
            "found bad counter  3111\n",
            "found bad counter  3112\n",
            "found bad counter  3458\n",
            "found bad counter  3459\n",
            "found bad counter  3461\n",
            "found bad counter  3469\n",
            "found bad counter  3475\n",
            "found bad counter  3477\n",
            "found bad counter  3618\n",
            "found bad counter  3627\n",
            "found bad counter  3646\n",
            "found bad counter  3649\n",
            "found bad counter  3669\n",
            "found bad counter  3703\n",
            "found bad counter  3739\n",
            "found bad counter  3742\n",
            "found bad counter  3760\n",
            "found bad counter  3763\n",
            "found bad counter  3766\n",
            "found bad counter  3847\n",
            "found bad counter  3869\n",
            "found bad counter  3871\n",
            "found bad counter  3888\n",
            "found bad counter  4029\n",
            "found bad counter  4133\n",
            "found bad counter  4134\n",
            "found bad counter  4135\n",
            "found bad counter  4136\n",
            "found bad counter  4137\n",
            "found bad counter  4138\n",
            "found bad counter  4139\n",
            "found bad counter  4140\n",
            "found bad counter  4141\n",
            "found bad counter  4142\n",
            "found bad counter  4143\n",
            "found bad counter  4144\n",
            "found bad counter  4145\n",
            "found bad counter  4146\n",
            "found bad counter  4147\n",
            "found bad counter  4148\n",
            "found bad counter  4149\n",
            "found bad counter  4150\n",
            "found bad counter  4151\n",
            "found bad counter  4152\n",
            "found bad counter  4153\n",
            "found bad counter  4154\n",
            "found bad counter  4155\n",
            "found bad counter  4156\n",
            "found bad counter  4157\n",
            "found bad counter  4158\n",
            "found bad counter  4159\n",
            "found bad counter  4160\n",
            "found bad counter  4161\n",
            "found bad counter  4162\n",
            "found bad counter  4163\n",
            "found bad counter  4164\n",
            "found bad counter  4165\n",
            "found bad counter  4166\n",
            "found bad counter  4167\n",
            "found bad counter  4168\n",
            "found bad counter  4169\n",
            "found bad counter  4170\n",
            "found bad counter  4171\n",
            "found bad counter  4172\n",
            "found bad counter  4173\n",
            "found bad counter  4174\n",
            "found bad counter  4175\n",
            "found bad counter  4176\n",
            "found bad counter  4177\n",
            "found bad counter  4178\n",
            "found bad counter  4179\n",
            "found bad counter  4180\n",
            "found bad counter  4181\n",
            "found bad counter  4182\n",
            "found bad counter  4183\n",
            "found bad counter  4184\n",
            "found bad counter  4185\n",
            "found bad counter  4186\n",
            "found bad counter  4187\n",
            "found bad counter  4188\n",
            "found bad counter  4189\n",
            "found bad counter  4190\n",
            "found bad counter  4191\n",
            "found bad counter  4192\n",
            "found bad counter  4193\n",
            "found bad counter  4194\n",
            "found bad counter  4195\n",
            "found bad counter  4196\n",
            "found bad counter  4197\n",
            "found bad counter  4198\n",
            "found bad counter  4199\n",
            "found bad counter  4200\n",
            "found bad counter  4201\n",
            "found bad counter  4202\n",
            "found bad counter  4203\n",
            "found bad counter  4204\n",
            "found bad counter  4205\n",
            "found bad counter  4206\n",
            "found bad counter  4207\n",
            "found bad counter  4208\n",
            "found bad counter  4209\n",
            "found bad counter  4210\n",
            "found bad counter  4211\n",
            "found bad counter  4212\n",
            "found bad counter  4213\n",
            "found bad counter  4214\n",
            "found bad counter  4215\n",
            "found bad counter  4216\n",
            "found bad counter  4217\n",
            "found bad counter  4218\n",
            "found bad counter  4219\n",
            "found bad counter  4220\n",
            "found bad counter  4221\n",
            "found bad counter  4222\n",
            "found bad counter  4223\n",
            "found bad counter  4224\n",
            "found bad counter  4225\n",
            "found bad counter  4226\n",
            "found bad counter  4227\n",
            "found bad counter  4228\n",
            "found bad counter  4229\n",
            "found bad counter  4230\n",
            "found bad counter  4231\n",
            "found bad counter  4232\n",
            "found bad counter  4233\n",
            "found bad counter  4234\n",
            "found bad counter  4235\n",
            "found bad counter  4236\n",
            "found bad counter  4237\n",
            "found bad counter  4238\n",
            "found bad counter  4239\n",
            "found bad counter  4240\n",
            "found bad counter  4241\n",
            "found bad counter  4242\n",
            "found bad counter  4243\n",
            "found bad counter  4244\n",
            "found bad counter  4245\n",
            "found bad counter  4246\n",
            "found bad counter  4247\n",
            "found bad counter  4248\n",
            "found bad counter  4249\n",
            "found bad counter  4250\n",
            "found bad counter  4251\n",
            "found bad counter  4252\n",
            "found bad counter  4253\n",
            "found bad counter  4254\n",
            "found bad counter  4255\n",
            "found bad counter  4256\n",
            "found bad counter  4257\n",
            "found bad counter  4258\n",
            "found bad counter  4259\n",
            "found bad counter  4260\n",
            "found bad counter  4261\n",
            "found bad counter  4262\n",
            "found bad counter  4263\n",
            "found bad counter  4264\n",
            "found bad counter  4265\n",
            "found bad counter  4266\n",
            "found bad counter  4267\n",
            "found bad counter  4268\n",
            "found bad counter  4269\n",
            "found bad counter  4270\n",
            "found bad counter  4271\n",
            "found bad counter  4272\n",
            "found bad counter  4273\n",
            "found bad counter  4274\n",
            "found bad counter  4275\n",
            "found bad counter  4276\n",
            "found bad counter  4277\n",
            "found bad counter  4278\n",
            "found bad counter  4279\n",
            "found bad counter  4403\n",
            "found bad counter  4404\n",
            "found bad counter  4405\n",
            "found bad counter  4406\n",
            "found bad counter  4407\n",
            "found bad counter  4408\n",
            "found bad counter  4409\n",
            "found bad counter  4410\n",
            "found bad counter  4411\n",
            "found bad counter  4412\n",
            "found bad counter  4413\n",
            "found bad counter  4414\n",
            "found bad counter  4415\n",
            "found bad counter  4416\n",
            "found bad counter  4417\n",
            "found bad counter  4418\n",
            "found bad counter  4419\n",
            "found bad counter  4420\n",
            "found bad counter  4421\n",
            "found bad counter  4422\n",
            "found bad counter  4423\n",
            "found bad counter  4424\n",
            "found bad counter  4425\n",
            "found bad counter  4426\n",
            "found bad counter  4427\n",
            "found bad counter  4428\n",
            "found bad counter  4429\n",
            "found bad counter  4430\n",
            "found bad counter  4431\n",
            "found bad counter  4432\n",
            "found bad counter  4433\n",
            "found bad counter  4434\n",
            "found bad counter  4435\n",
            "found bad counter  4436\n",
            "found bad counter  4437\n",
            "found bad counter  4444\n",
            "found bad counter  4452\n",
            "found bad counter  4453\n",
            "found bad counter  4454\n",
            "found bad counter  4464\n",
            "found bad counter  4467\n",
            "found bad counter  4474\n",
            "found bad counter  4477\n",
            "found bad counter  4484\n",
            "found bad counter  4486\n",
            "found bad counter  4488\n",
            "found bad counter  4494\n",
            "found bad counter  4504\n",
            "found bad counter  4506\n",
            "found bad counter  4513\n",
            "found bad counter  4514\n",
            "found bad counter  4531\n",
            "found bad counter  4626\n",
            "found bad counter  4627\n",
            "found bad counter  4628\n",
            "found bad counter  4629\n",
            "found bad counter  4630\n",
            "found bad counter  4631\n",
            "found bad counter  4632\n",
            "found bad counter  4633\n",
            "found bad counter  4634\n",
            "found bad counter  4635\n",
            "found bad counter  4636\n",
            "found bad counter  4637\n",
            "found bad counter  4638\n",
            "found bad counter  4639\n",
            "found bad counter  4660\n",
            "found bad counter  4661\n",
            "found bad counter  4662\n",
            "found bad counter  4860\n",
            "found bad counter  4861\n",
            "found bad counter  4862\n",
            "found bad counter  4863\n",
            "found bad counter  4864\n",
            "found bad counter  4865\n",
            "found bad counter  4866\n",
            "found bad counter  4867\n",
            "found bad counter  4868\n",
            "found bad counter  4869\n",
            "found bad counter  4870\n",
            "found bad counter  4871\n",
            "found bad counter  4872\n",
            "found bad counter  4873\n",
            "found bad counter  4874\n",
            "found bad counter  4921\n",
            "found bad counter  4922\n",
            "found bad counter  4923\n",
            "found bad counter  4924\n",
            "found bad counter  4925\n",
            "found bad counter  4954\n",
            "found bad counter  4964\n",
            "found bad counter  4970\n",
            "found bad counter  4975\n",
            "found bad counter  4980\n",
            "found bad counter  4994\n",
            "found bad counter  5259\n",
            "found bad counter  5261\n",
            "found bad counter  5262\n",
            "found bad counter  5263\n",
            "found bad counter  5264\n",
            "found bad counter  5265\n",
            "found bad counter  5266\n",
            "found bad counter  5267\n",
            "found bad counter  5268\n",
            "found bad counter  5276\n",
            "found bad counter  5277\n",
            "found bad counter  5278\n",
            "found bad counter  5279\n",
            "found bad counter  5280\n",
            "found bad counter  5281\n",
            "found bad counter  5282\n",
            "found bad counter  5283\n",
            "found bad counter  5313\n",
            "found bad counter  5314\n",
            "found bad counter  5315\n",
            "found bad counter  5316\n",
            "found bad counter  5317\n",
            "found bad counter  5318\n",
            "found bad counter  5319\n",
            "found bad counter  5320\n",
            "found bad counter  5321\n",
            "found bad counter  5322\n",
            "found bad counter  5323\n",
            "found bad counter  5324\n",
            "found bad counter  5325\n",
            "found bad counter  5326\n",
            "found bad counter  5327\n",
            "found bad counter  5328\n",
            "found bad counter  5329\n",
            "found bad counter  5330\n",
            "found bad counter  5331\n",
            "found bad counter  5332\n",
            "found bad counter  5333\n",
            "found bad counter  5334\n",
            "found bad counter  5335\n",
            "found bad counter  5336\n",
            "found bad counter  5337\n",
            "found bad counter  5338\n",
            "found bad counter  5339\n",
            "found bad counter  5340\n",
            "found bad counter  5341\n",
            "found bad counter  5342\n",
            "found bad counter  5343\n",
            "found bad counter  5344\n",
            "found bad counter  5345\n",
            "found bad counter  5363\n",
            "found bad counter  5364\n",
            "found bad counter  5365\n",
            "found bad counter  5366\n",
            "found bad counter  5377\n",
            "found bad counter  5378\n",
            "found bad counter  5411\n",
            "found bad counter  5414\n",
            "found bad counter  5415\n",
            "found bad counter  5416\n",
            "found bad counter  5417\n",
            "found bad counter  5418\n",
            "found bad counter  5419\n",
            "found bad counter  5420\n",
            "found bad counter  5421\n",
            "found bad counter  5422\n",
            "found bad counter  5423\n",
            "found bad counter  5424\n",
            "found bad counter  5425\n",
            "found bad counter  5426\n",
            "found bad counter  5427\n",
            "found bad counter  5428\n",
            "found bad counter  5429\n",
            "found bad counter  5435\n",
            "found bad counter  5436\n",
            "found bad counter  5437\n",
            "found bad counter  5438\n",
            "found bad counter  5439\n",
            "found bad counter  5443\n",
            "found bad counter  5444\n",
            "found bad counter  5445\n",
            "found bad counter  5446\n",
            "found bad counter  5447\n",
            "found bad counter  5448\n",
            "found bad counter  5456\n",
            "found bad counter  5457\n",
            "found bad counter  5458\n",
            "found bad counter  5459\n",
            "found bad counter  5460\n",
            "found bad counter  5461\n",
            "found bad counter  5462\n",
            "found bad counter  5463\n",
            "found bad counter  5464\n",
            "found bad counter  5465\n",
            "found bad counter  5467\n",
            "found bad counter  5468\n",
            "found bad counter  5469\n",
            "found bad counter  5470\n",
            "found bad counter  5471\n",
            "found bad counter  5543\n",
            "found bad counter  5544\n",
            "found bad counter  5545\n",
            "found bad counter  5546\n",
            "found bad counter  5547\n",
            "found bad counter  5548\n",
            "found bad counter  5549\n",
            "found bad counter  5550\n",
            "found bad counter  5551\n",
            "found bad counter  5552\n",
            "found bad counter  5577\n",
            "found bad counter  5688\n",
            "found bad counter  5708\n",
            "found bad counter  5710\n",
            "found bad counter  5714\n",
            "found bad counter  5723\n",
            "found bad counter  5724\n",
            "found bad counter  5725\n",
            "found bad counter  5726\n",
            "found bad counter  5727\n",
            "found bad counter  5728\n",
            "found bad counter  5733\n",
            "found bad counter  5755\n",
            "found bad counter  5756\n",
            "found bad counter  5757\n",
            "found bad counter  5758\n",
            "found bad counter  5775\n",
            "found bad counter  5777\n",
            "found bad counter  5778\n",
            "found bad counter  5779\n",
            "found bad counter  5780\n",
            "found bad counter  5781\n",
            "found bad counter  5782\n",
            "found bad counter  5783\n",
            "found bad counter  5784\n",
            "found bad counter  5786\n",
            "found bad counter  5787\n",
            "found bad counter  5788\n",
            "found bad counter  5789\n",
            "found bad counter  5790\n",
            "found bad counter  5791\n",
            "found bad counter  5792\n",
            "found bad counter  5793\n",
            "found bad counter  5795\n",
            "found bad counter  5796\n",
            "found bad counter  5797\n",
            "found bad counter  5798\n",
            "found bad counter  5799\n",
            "found bad counter  5802\n",
            "found bad counter  5803\n",
            "found bad counter  5804\n",
            "found bad counter  5805\n",
            "found bad counter  5806\n",
            "found bad counter  5807\n",
            "found bad counter  5808\n",
            "found bad counter  5809\n",
            "found bad counter  5833\n",
            "found bad counter  5836\n",
            "found bad counter  5837\n",
            "found bad counter  5848\n",
            "found bad counter  5849\n",
            "found bad counter  5850\n",
            "found bad counter  5851\n",
            "found bad counter  5852\n",
            "found bad counter  5858\n",
            "found bad counter  5861\n",
            "found bad counter  5865\n",
            "found bad counter  5866\n",
            "found bad counter  5867\n",
            "found bad counter  5868\n",
            "found bad counter  5894\n",
            "found bad counter  5895\n",
            "found bad counter  5896\n",
            "found bad counter  5897\n",
            "found bad counter  5898\n",
            "found bad counter  5899\n",
            "found bad counter  5900\n",
            "found bad counter  5992\n",
            "found bad counter  6005\n",
            "found bad counter  6006\n",
            "found bad counter  6007\n",
            "found bad counter  6008\n",
            "found bad counter  6009\n",
            "found bad counter  6337\n",
            "found bad counter  6338\n",
            "found bad counter  6339\n",
            "found bad counter  6340\n",
            "found bad counter  6341\n",
            "found bad counter  6342\n",
            "found bad counter  6343\n",
            "found bad counter  6344\n",
            "found bad counter  6345\n",
            "found bad counter  6346\n",
            "found bad counter  6347\n",
            "found bad counter  6348\n",
            "found bad counter  6349\n",
            "found bad counter  6350\n",
            "found bad counter  6351\n",
            "found bad counter  6352\n",
            "found bad counter  6353\n",
            "found bad counter  6354\n",
            "found bad counter  6355\n",
            "found bad counter  6356\n",
            "found bad counter  6357\n",
            "found bad counter  6358\n",
            "found bad counter  6359\n",
            "found bad counter  6360\n",
            "found bad counter  6361\n",
            "found bad counter  6362\n",
            "found bad counter  6363\n",
            "found bad counter  6364\n",
            "found bad counter  6365\n",
            "found bad counter  6366\n",
            "found bad counter  6367\n",
            "found bad counter  6368\n",
            "found bad counter  6369\n",
            "found bad counter  6370\n",
            "found bad counter  6371\n",
            "found bad counter  6372\n",
            "found bad counter  6374\n",
            "found bad counter  6375\n",
            "found bad counter  6634\n",
            "found bad counter  6693\n",
            "found bad counter  6694\n",
            "found bad counter  6695\n",
            "found bad counter  6696\n",
            "found bad counter  6697\n",
            "found bad counter  6698\n",
            "found bad counter  6699\n",
            "found bad counter  6700\n",
            "found bad counter  6701\n",
            "found bad counter  6702\n",
            "found bad counter  6703\n",
            "found bad counter  6704\n",
            "found bad counter  6705\n",
            "found bad counter  6706\n",
            "found bad counter  6707\n",
            "found bad counter  6708\n",
            "found bad counter  6709\n",
            "found bad counter  6710\n",
            "found bad counter  6711\n",
            "found bad counter  6712\n",
            "found bad counter  6713\n",
            "found bad counter  6714\n",
            "found bad counter  6715\n",
            "found bad counter  6716\n",
            "found bad counter  6717\n",
            "found bad counter  6718\n",
            "found bad counter  6719\n",
            "found bad counter  6720\n",
            "found bad counter  6721\n",
            "found bad counter  6722\n",
            "found bad counter  6723\n",
            "found bad counter  6724\n",
            "found bad counter  6725\n",
            "found bad counter  6726\n",
            "found bad counter  6727\n",
            "found bad counter  6728\n",
            "found bad counter  6730\n",
            "found bad counter  6731\n",
            "found bad counter  6732\n",
            "found bad counter  6733\n",
            "found bad counter  6734\n",
            "found bad counter  6735\n",
            "found bad counter  6736\n",
            "found bad counter  6737\n",
            "found bad counter  6738\n",
            "found bad counter  6739\n",
            "found bad counter  6740\n",
            "found bad counter  6741\n",
            "found bad counter  6742\n",
            "found bad counter  6743\n",
            "found bad counter  6744\n",
            "found bad counter  6745\n",
            "found bad counter  6746\n",
            "found bad counter  6747\n",
            "found bad counter  6748\n",
            "found bad counter  6749\n",
            "found bad counter  6750\n",
            "found bad counter  6751\n",
            "found bad counter  6752\n",
            "found bad counter  6753\n",
            "found bad counter  6754\n",
            "found bad counter  6755\n",
            "found bad counter  6756\n",
            "found bad counter  6757\n",
            "found bad counter  6820\n",
            "found bad counter  6821\n",
            "found bad counter  6822\n",
            "found bad counter  6823\n",
            "found bad counter  6824\n",
            "found bad counter  6825\n",
            "found bad counter  6999\n",
            "found bad counter  7000\n",
            "found bad counter  7001\n",
            "found bad counter  7002\n",
            "found bad counter  7003\n",
            "found bad counter  7004\n",
            "found bad counter  7005\n",
            "found bad counter  7006\n",
            "found bad counter  7007\n",
            "found bad counter  7008\n",
            "found bad counter  7009\n",
            "found bad counter  7010\n",
            "found bad counter  7011\n",
            "found bad counter  7012\n",
            "found bad counter  7013\n",
            "found bad counter  7014\n",
            "found bad counter  7015\n",
            "found bad counter  7016\n",
            "found bad counter  7017\n",
            "found bad counter  7018\n",
            "found bad counter  7019\n",
            "found bad counter  7020\n",
            "found bad counter  7021\n",
            "found bad counter  7022\n",
            "found bad counter  7023\n",
            "found bad counter  7024\n",
            "found bad counter  7025\n",
            "found bad counter  7026\n",
            "found bad counter  7027\n",
            "found bad counter  7028\n",
            "found bad counter  7029\n",
            "found bad counter  7030\n",
            "found bad counter  7031\n",
            "found bad counter  7032\n",
            "found bad counter  7033\n",
            "found bad counter  7034\n",
            "found bad counter  7071\n",
            "found bad counter  7072\n",
            "found bad counter  7073\n",
            "found bad counter  7222\n",
            "found bad counter  7223\n",
            "found bad counter  7224\n",
            "found bad counter  7356\n",
            "found bad counter  7357\n",
            "found bad counter  7358\n",
            "found bad counter  7359\n",
            "found bad counter  7360\n",
            "found bad counter  7361\n",
            "found bad counter  7362\n",
            "found bad counter  7363\n",
            "found bad counter  7364\n",
            "found bad counter  7365\n",
            "found bad counter  7366\n",
            "found bad counter  7367\n",
            "found bad counter  7368\n",
            "found bad counter  7369\n",
            "found bad counter  7370\n",
            "found bad counter  7371\n",
            "found bad counter  7372\n",
            "found bad counter  7373\n",
            "found bad counter  7374\n",
            "found bad counter  7375\n",
            "found bad counter  7376\n",
            "found bad counter  7377\n",
            "found bad counter  7378\n",
            "found bad counter  7379\n",
            "found bad counter  7380\n",
            "found bad counter  7381\n",
            "found bad counter  7382\n",
            "found bad counter  7383\n",
            "found bad counter  7384\n",
            "found bad counter  7385\n",
            "found bad counter  7386\n",
            "found bad counter  7387\n",
            "found bad counter  7388\n",
            "found bad counter  7389\n",
            "found bad counter  7390\n",
            "found bad counter  7391\n",
            "found bad counter  7392\n",
            "found bad counter  7393\n",
            "found bad counter  7394\n",
            "found bad counter  7395\n",
            "found bad counter  7396\n",
            "found bad counter  7397\n",
            "found bad counter  7398\n",
            "found bad counter  7399\n",
            "found bad counter  7400\n",
            "found bad counter  7401\n",
            "found bad counter  7402\n",
            "found bad counter  7403\n",
            "found bad counter  7404\n",
            "found bad counter  7405\n",
            "found bad counter  7406\n",
            "found bad counter  7407\n",
            "found bad counter  7408\n",
            "found bad counter  7409\n",
            "found bad counter  7410\n",
            "found bad counter  7411\n",
            "found bad counter  7412\n",
            "found bad counter  7413\n",
            "found bad counter  7414\n",
            "found bad counter  7415\n",
            "found bad counter  7416\n",
            "found bad counter  7417\n",
            "found bad counter  7418\n",
            "found bad counter  7419\n",
            "found bad counter  7420\n",
            "found bad counter  7421\n",
            "found bad counter  7422\n",
            "found bad counter  7423\n",
            "found bad counter  7424\n",
            "found bad counter  7425\n",
            "found bad counter  7426\n",
            "found bad counter  7427\n",
            "found bad counter  7646\n",
            "found bad counter  7647\n",
            "found bad counter  7648\n",
            "found bad counter  7649\n",
            "found bad counter  7770\n",
            "found bad counter  7771\n",
            "found bad counter  7772\n",
            "found bad counter  7773\n",
            "found bad counter  7774\n",
            "found bad counter  7775\n",
            "found bad counter  7776\n",
            "found bad counter  7777\n",
            "found bad counter  7778\n",
            "found bad counter  7779\n",
            "found bad counter  7780\n",
            "found bad counter  7781\n",
            "found bad counter  7782\n",
            "found bad counter  7783\n",
            "found bad counter  7784\n",
            "found bad counter  7785\n",
            "found bad counter  7786\n",
            "found bad counter  7787\n",
            "found bad counter  7788\n",
            "found bad counter  7789\n",
            "found bad counter  7790\n",
            "found bad counter  7791\n",
            "found bad counter  7792\n",
            "found bad counter  7793\n",
            "found bad counter  7794\n",
            "found bad counter  7795\n",
            "found bad counter  7806\n",
            "found bad counter  7807\n",
            "found bad counter  7808\n",
            "found bad counter  7809\n",
            "found bad counter  8002\n",
            "found bad counter  8003\n",
            "found bad counter  8004\n",
            "found bad counter  8005\n",
            "found bad counter  8006\n",
            "found bad counter  8007\n",
            "found bad counter  8008\n",
            "found bad counter  8009\n",
            "found bad counter  8010\n",
            "found bad counter  8011\n",
            "found bad counter  8012\n",
            "found bad counter  8013\n",
            "found bad counter  8014\n",
            "found bad counter  8015\n",
            "found bad counter  8016\n",
            "found bad counter  8017\n",
            "found bad counter  8018\n",
            "found bad counter  8019\n",
            "found bad counter  8020\n",
            "found bad counter  8021\n",
            "found bad counter  8022\n",
            "found bad counter  8023\n",
            "found bad counter  8024\n",
            "found bad counter  8025\n",
            "found bad counter  8026\n",
            "found bad counter  8027\n",
            "found bad counter  8028\n",
            "found bad counter  8029\n",
            "found bad counter  8030\n",
            "found bad counter  8031\n",
            "found bad counter  8032\n",
            "found bad counter  8033\n",
            "found bad counter  8034\n",
            "found bad counter  8035\n",
            "found bad counter  8036\n",
            "found bad counter  8037\n",
            "found bad counter  8038\n",
            "found bad counter  8039\n",
            "found bad counter  8040\n",
            "found bad counter  8041\n",
            "found bad counter  8042\n",
            "found bad counter  8043\n",
            "found bad counter  8044\n",
            "found bad counter  8045\n",
            "found bad counter  8046\n",
            "found bad counter  8047\n",
            "found bad counter  8048\n",
            "found bad counter  8049\n",
            "found bad counter  8050\n",
            "found bad counter  8051\n",
            "found bad counter  8052\n",
            "found bad counter  8053\n",
            "found bad counter  8054\n",
            "found bad counter  8055\n",
            "found bad counter  8056\n",
            "found bad counter  8057\n",
            "found bad counter  8058\n",
            "found bad counter  8059\n",
            "found bad counter  8060\n",
            "found bad counter  8061\n",
            "found bad counter  8062\n",
            "found bad counter  8063\n",
            "found bad counter  8064\n",
            "found bad counter  8065\n",
            "found bad counter  8066\n",
            "found bad counter  8067\n",
            "found bad counter  8068\n",
            "found bad counter  8069\n",
            "found bad counter  8070\n",
            "found bad counter  8071\n",
            "found bad counter  8072\n",
            "found bad counter  8073\n",
            "found bad counter  8077\n",
            "found bad counter  8078\n",
            "found bad counter  8079\n",
            "found bad counter  8080\n",
            "found bad counter  8081\n",
            "found bad counter  8082\n",
            "found bad counter  8083\n",
            "found bad counter  8084\n",
            "found bad counter  8085\n",
            "found bad counter  8086\n",
            "found bad counter  8087\n",
            "found bad counter  8088\n",
            "found bad counter  8089\n",
            "found bad counter  8090\n",
            "found bad counter  8091\n",
            "found bad counter  8092\n",
            "found bad counter  8093\n",
            "found bad counter  8094\n",
            "found bad counter  8095\n",
            "found bad counter  8096\n",
            "found bad counter  8370\n",
            "found bad counter  8371\n",
            "found bad counter  8372\n",
            "found bad counter  8373\n",
            "found bad counter  8455\n",
            "found bad counter  8456\n",
            "found bad counter  8545\n",
            "found bad counter  8771\n",
            "found bad counter  8772\n",
            "found bad counter  8773\n",
            "found bad counter  8774\n",
            "found bad counter  8775\n",
            "found bad counter  8776\n",
            "found bad counter  8777\n",
            "found bad counter  8778\n",
            "found bad counter  8779\n",
            "found bad counter  8780\n",
            "found bad counter  8781\n",
            "found bad counter  8782\n",
            "found bad counter  8783\n",
            "found bad counter  8784\n",
            "found bad counter  8785\n",
            "found bad counter  8786\n",
            "found bad counter  8787\n",
            "found bad counter  8788\n",
            "found bad counter  8789\n",
            "found bad counter  8790\n",
            "found bad counter  8791\n",
            "found bad counter  8792\n",
            "found bad counter  8793\n",
            "found bad counter  8794\n",
            "found bad counter  8795\n",
            "found bad counter  8796\n",
            "found bad counter  8797\n",
            "found bad counter  8798\n",
            "found bad counter  8799\n",
            "found bad counter  8800\n",
            "found bad counter  8801\n",
            "found bad counter  8802\n",
            "found bad counter  8803\n",
            "found bad counter  8804\n",
            "found bad counter  8805\n",
            "found bad counter  8806\n",
            "found bad counter  8807\n",
            "found bad counter  8808\n",
            "found bad counter  8809\n",
            "found bad counter  8810\n",
            "found bad counter  8811\n",
            "found bad counter  8812\n",
            "found bad counter  8813\n",
            "found bad counter  8814\n",
            "found bad counter  8815\n",
            "found bad counter  8816\n",
            "found bad counter  8817\n",
            "found bad counter  8818\n",
            "found bad counter  8819\n",
            "found bad counter  8820\n",
            "found bad counter  8821\n",
            "found bad counter  8822\n",
            "found bad counter  8823\n",
            "found bad counter  8824\n",
            "found bad counter  8825\n",
            "found bad counter  8826\n",
            "found bad counter  8827\n",
            "found bad counter  8828\n",
            "found bad counter  8829\n",
            "found bad counter  8830\n",
            "found bad counter  8831\n",
            "found bad counter  8832\n",
            "found bad counter  8833\n",
            "found bad counter  8834\n",
            "found bad counter  8835\n",
            "found bad counter  8836\n",
            "found bad counter  8837\n",
            "found bad counter  8838\n",
            "found bad counter  8839\n",
            "found bad counter  8840\n",
            "found bad counter  8841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzxqwcYqcfdE"
      },
      "source": [
        "#SET UP TRAINING DATA "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-0XJgWWHnPz"
      },
      "source": [
        "#load up with the pre-sized patch images\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "training_data = CustomDataset(img_dir=annotated_dir,\n",
        "                                label_data = first50,\n",
        "                                category = '', #full_category_name, \n",
        "                                file_count=len(full_file_list), #full_file_count,\n",
        "                                file_list = full_file_list, \n",
        "                                transform=None, \n",
        "                                target_transform=None)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx0rFlO-JRMV"
      },
      "source": [
        "if (train_on_gpu == 1):\n",
        "    m = nn.LogSoftmax(dim=1).cuda(dev)\n",
        "    nll_loss = nn.NLLLoss().cuda(dev)\n",
        "else:\n",
        "    m = nn.LogSoftmax(dim=1)\n",
        "    nll_loss = nn.NLLLoss()\n",
        "\n",
        "L1loss = nn.L1Loss()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snJpkHyrCBz1"
      },
      "source": [
        "# Model Import/Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szM2cIAhiXVc"
      },
      "source": [
        "#RESNET 50 FPN PRE-TRAINED  w/ CUSTOM BACKBONE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_W26LydiN3v",
        "outputId": "ca1cbf38-c158-4675-bccf-26e486585d63"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "loss_func = nn.NLLLoss()\n",
        "\n",
        "#!pip3 -q install engine\n",
        "#from engine import train_one_epoch, evaluate\n",
        "#import utils\n",
        "\n",
        "anchor_generator = AnchorGenerator(sizes=((64, 128,192 , 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 0.75, 1.0),))\n",
        "num_classes = 2\n",
        "resnet50_fpn = torchvision.models.detection.maskrcnn_resnet50_fpn(num_classes=2)\n",
        "'''\n",
        "backbone_fpn = nn.Sequential(\n",
        "\tresnet50_fpn.backbone.body.conv1,\n",
        "\tresnet50_fpn.backbone.body.bn1,\n",
        "\tresnet50_fpn.backbone.body.relu,\n",
        "\tresnet50_fpn.backbone.body.maxpool,\n",
        "\tresnet50_fpn.backbone.body.layer1,\n",
        "\tresnet50_fpn.backbone.body.layer2,\n",
        "\tresnet50_fpn.backbone.body.layer3,\n",
        "\tresnet50_fpn.backbone.body.layer4\n",
        "\t)\n",
        "backbone_fpn.out_channels = 2048\n",
        "modelr = FasterRCNN(backbone_fpn,num_classes=2,rpn_anchor_generator=anchor_generator)\n",
        "'''\n",
        "\n",
        "resnet_net = torchvision.models.resnet50(pretrained=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "modules = list(resnet_net.children())[:-1]\n",
        "backbone = nn.Sequential(*modules)\n",
        "backbone.out_channels = 2048\n",
        "#backbone = resnet_fpn_backbone('resnet50', pretrained_backbone)\n",
        "modelr = FasterRCNN(backbone,num_classes=2,rpn_anchor_generator=anchor_generator)\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "# get the number of input features \n",
        "in_features = modelr.roi_heads.box_predictor.cls_score.in_features\n",
        "# define a new head for the detector with required number of classes\n",
        "modelr.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "\n",
        "#modelr.train() #eval()\n",
        "\n",
        "#kaggle setup\n",
        "# freeze the backbone (it will freeze the body and fpn params). We don't need to\n",
        "#updated any of the pre-trained weight parameters\n",
        "for param in modelr.parameters():\n",
        "    param.requires_grad = False \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# freeze the fc6 layer in roi_heads\n",
        "for p in modelr.roi_heads.box_head.fc6.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Add a fully connected layer at the end with two outputs for our classes\n",
        "#modelr.fc = nn.Sequential(\n",
        "#               nn.Linear(2048, 128),\n",
        "#               nn.ReLU(inplace=True),\n",
        "#               nn.Linear(128, 2)).to(dev)\n",
        "\n",
        "\n",
        "#fc_inputs = modelr.fc.in_features\n",
        "modelr.fc = nn.Sequential(\n",
        "    nn.Linear(2048, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, 2),\n",
        "    nn.LogSoftmax(dim=1) # For using NLLLoss()\n",
        ")\n",
        "for param in modelr.fc:\n",
        "    param.requires_grad = True \n",
        "\n",
        "\n",
        "for p in modelr.rpn.parameters():\n",
        "    print(p)\n",
        "    p.requires_grad = True\n",
        "\n",
        "print('Model Params with Grad=True')\n",
        "for n, param in modelr.named_parameters():\n",
        "  if param.requires_grad==True:\n",
        "    print(n)\n",
        "\n",
        "#for param in modelr.fc():\n",
        "#    param.requires_grad==True\n",
        "\n",
        "optimizer = optim.Adam(modelr.fc.parameters())\n",
        "\n",
        "\n",
        "if (train_on_gpu):\n",
        "    modelr = modelr.to(dev)\n",
        "\n",
        "\n",
        "batch_size = 5\n",
        "data_loader = torch.utils.data.DataLoader(training_data,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=True,\n",
        "                                            num_workers=2)\n",
        "\n",
        "# define list to store the smooth loss and learning rate\n",
        "smooth_loss_list  = []\n",
        "lr_list = []\n",
        "batch_num = 0\n",
        "avg_loss = 0\n",
        "best_loss = 0\n",
        "\n",
        "for epoch in range(0,1):\n",
        "    epoch_loss = []\n",
        "    #with torch.no_grad(): #trying enabled\n",
        "    with torch.set_grad_enabled(True):\n",
        "        for i, data in enumerate(data_loader, 0):\n",
        "            train_loss =0\n",
        "            print('------------------------------------dataload loop is ',i)\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            #inputs, labels = data\n",
        "            \n",
        "            \n",
        "            modelr.train() #eval()\n",
        "\n",
        "            images = data[0]\n",
        "            targets = data[1]\n",
        "            #targets = targets.to(dev)\n",
        "            #np.squeeze(targets[0]['boxes'], 1)\n",
        "\n",
        "            \n",
        "\n",
        "            imagelist=[]\n",
        "            for ii in range(0,len(images)):\n",
        "                #imagelist.append(images[ii])\n",
        "                if (train_on_gpu):\n",
        "                    imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32).to(dev))\n",
        "                else:\n",
        "                    imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32))\n",
        "            print(np.shape(images), len(images))\n",
        "            print(np.shape(imagelist[0]))\n",
        "\n",
        "\n",
        "            #\n",
        "            # Reformat the target dictionary\n",
        "            #\n",
        "            target_new = []\n",
        "            for ii in range(0,batch_size):\n",
        "                d={}\n",
        "                #d['boxes'] = targets[0]['boxes'][ii]\n",
        "                if (train_on_gpu == 1):\n",
        "                    d['boxes'] = torch.as_tensor(targets[0]['boxes'][ii]).to(dev)\n",
        "                    d['labels'] = torch.as_tensor(targets[0]['labels'][ii]).to(dev)\n",
        "                    d['image_id'] = torch.as_tensor(targets[0]['image_id'][ii]).to(dev)\n",
        "                    d['area'] = torch.as_tensor(targets[0]['area'][ii]).to(dev)\n",
        "                else:\n",
        "                    d['boxes'] = targets[0]['boxes'][ii]\n",
        "                    d['labels'] = targets[0]['labels'][ii]\n",
        "                    d['image_id'] = targets[0]['image_id'][ii]\n",
        "                    d['area'] = targets[0]['area'][ii]                  \n",
        "                target_new.append(d)\n",
        "\n",
        "            #Loss setup \n",
        "            #criterion = nn.CrossEntropyLoss()\n",
        "            criterion = nn.NLLLoss()\n",
        "                        #nn.L1Loss()\n",
        "            if (train_on_gpu ==1):\n",
        "                criterion.cuda(dev)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #forward pass\n",
        "            loss_dict = modelr(imagelist, target_new) #targets)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            \n",
        "            print('*** SUMMED losses is ', losses)\n",
        "            print(loss_dict)\n",
        "            train_loss += losses\n",
        "#            _, preds = torch.max(loss_dict, 1)\n",
        "\n",
        "\n",
        "            #print(nn.Softmax2d(out_custom['loss_classifier']))\n",
        "            #criterion(out_custom, targets)\n",
        "\n",
        "\n",
        "            #loss = criterion(out_custom, targets[0]['labels'])\n",
        "\n",
        "                # backward pass\n",
        "            losses.backward()\n",
        "            # update optimizer\n",
        "            optimizer.step()\n",
        "            #nn.loss1.backward()\n",
        "    epoch_loss.append(train_loss/batch_size)\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (428, 603) 2264\n",
            "image after crop is  (428, 603)\n",
            "resized shapes are  600 800\n",
            "corners  [(377, 127), (675, 127), (675, 317), (377, 317)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(57036., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (301, 526) 1236\n",
            "image after crop is  (301, 526)\n",
            "resized shapes are  600 800\n",
            "corners  [(303, 199), (690, 199), (690, 355), (303, 355)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(70850., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (454, 580) 6637\n",
            "image after crop is  (454, 580)\n",
            "resized shapes are  600 800\n",
            "corners  [(280, 169), (470, 169), (470, 298), (280, 298)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(22018., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  120\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5612, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6959, device='cuda:0'), 'loss_box_reg': tensor(0.0026, device='cuda:0'), 'loss_objectness': tensor(0.6898, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1728, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  121\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (303, 582) 1139\n",
            "image after crop is  (303, 582)\n",
            "resized shapes are  600 800\n",
            "corners  [(221, 204), (628, 204), (628, 355), (221, 355)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(69741., dtype=torch.float64)\n",
            "*** SUMMED losses is  tensor(1.5294, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6955, device='cuda:0'), 'loss_box_reg': tensor(0.0026, device='cuda:0'), 'loss_objectness': tensor(0.6899, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1414, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 612) 422\n",
            "image after crop is  (362, 612)\n",
            "resized shapes are  600 800\n",
            "corners  [(404, 175), (451, 175), (451, 230), (404, 230)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(1080., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (283, 607) 1291\n",
            "image after crop is  (283, 607)\n",
            "resized shapes are  600 800\n",
            "corners  [(303, 208), (690, 208), (690, 364), (303, 364)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(71060., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (457, 581) 3302\n",
            "image after crop is  (457, 581)\n",
            "resized shapes are  600 800\n",
            "corners  [(330, 170), (498, 170), (498, 307), (330, 307)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(21276., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (363, 608) 1680\n",
            "image after crop is  (363, 608)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "image shape read in is  (600, 800, 3)\n",
            "setting target\n",
            "about to resize image. shape going in is @ (459, 516) 7203\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image after crop is  (459, 516)\n",
            "resized shapes are  600 800\n",
            "corners  [(184, 129), (654, 129), (654, 429), (184, 429)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(126909., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 609) 610\n",
            "image after crop is  (362, 609)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (420, 612) 6520\n",
            "image after crop is  (420, 612)\n",
            "resized shapes are  600 800\n",
            "corners  [(233, 211), (652, 211), (652, 371), (233, 371)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(68310., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (422, 585) 6050\n",
            "image after crop is  (422, 585)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (363, 601) 2885\n",
            "image after crop is  (363, 601)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  122\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.5374, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6956, device='cuda:0'), 'loss_box_reg': tensor(0.0017, device='cuda:0'), 'loss_objectness': tensor(0.6916, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.1484, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  123\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5733, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6937, device='cuda:0'), 'loss_box_reg': tensor(0.0019, device='cuda:0'), 'loss_objectness': tensor(0.6891, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1885, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (459, 512) 7119\n",
            "image after crop is  (459, 512)\n",
            "resized shapes are  600 800\n",
            "corners  [(203, 129), (683, 129), (683, 445), (203, 445)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(137379., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (459, 578) 7\n",
            "image after crop is  (459, 578)\n",
            "resized shapes are  600 800\n",
            "corners  [(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(62328., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (313, 608) 5493\n",
            "image after crop is  (313, 608)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (429, 608) 4417\n",
            "image after crop is  (429, 608)\n",
            "resized shapes are  600 800\n",
            "corners  [(217, 153), (644, 153), (644, 349), (217, 349)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(83692., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (459, 513) 7166\n",
            "image after crop is  (459, 513)\n",
            "resized shapes are  600 800\n",
            "corners  [(184, 129), (654, 129), (654, 429), (184, 429)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(126024., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (422, 608) 4328\n",
            "image after crop is  (422, 608)\n",
            "resized shapes are  600 800\n",
            "corners  [(160, 152), (672, 152), (672, 364), (160, 364)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(109728., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (309, 710) 7523\n",
            "image after crop is  (309, 710)\n",
            "resized shapes are  600 800\n",
            "corners  [(400, 239), (529, 239), (529, 326), (400, 326)]\n",
            "found a negative box!!  [[384, 58, 513, 145]]\n",
            "skip points:  [181, 490, 16, 726]\n",
            "offset col, row  384 58\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(-9072., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 698) 4651\n",
            "image after crop is  (362, 698)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (301, 609) 3219\n",
            "image after crop is  (301, 609)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (427, 606) 3468\n",
            "image after crop is  (427, 606)\n",
            "resized shapes are  600 800\n",
            "corners  [(366, 189), (529, 189), (529, 330), (366, 330)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(23004., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  124\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5497, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6968, device='cuda:0'), 'loss_box_reg': tensor(0.0022, device='cuda:0'), 'loss_objectness': tensor(0.6909, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1599, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  125\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.6019, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6957, device='cuda:0'), 'loss_box_reg': tensor(0.0017, device='cuda:0'), 'loss_objectness': tensor(0.6901, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2144, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (302, 711) 7404\n",
            "image after crop is  (302, 711)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (411, 507) 5644\n",
            "image after crop is  (411, 507)\n",
            "resized shapes are  600 800\n",
            "corners  [(272, 166), (590, 166), (590, 411), (272, 411)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(72600., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (425, 608) 2217\n",
            "image after crop is  (425, 608)\n",
            "resized shapes are  600 800\n",
            "corners  [(377, 127), (675, 127), (675, 317), (377, 317)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(56620., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (244, 596) 2764\n",
            "image after crop is  (244, 596)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (457, 514) 7124\n",
            "image after crop is  (457, 514)\n",
            "resized shapes are  600 800\n",
            "corners  [(203, 129), (683, 129), (683, 445), (203, 445)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(137088., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (459, 516) 7172\n",
            "image after crop is  (459, 516)\n",
            "resized shapes are  600 800\n",
            "corners  [(184, 129), (654, 129), (654, 429), (184, 429)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(126024., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (305, 611) 3358\n",
            "image after crop is  (305, 611)\n",
            "resized shapes are  600 800\n",
            "corners  [(456, 249), (586, 249), (586, 341), (456, 341)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(10472., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (456, 582) 828\n",
            "image after crop is  (456, 582)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (419, 604) 5890\n",
            "image after crop is  (419, 604)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (456, 507) 2196\n",
            "image after crop is  (456, 507)\n",
            "resized shapes are  600 800\n",
            "corners  [(265, 207), (641, 207), (641, 449), (265, 449)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(79605., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  126\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5665, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (422, 613) 3407\n",
            "image after crop is  (422, 613)\n",
            "resized shapes are  600 800\n",
            "corners  [(368, 200), (532, 200), (532, 325), (368, 325)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(20640., dtype=torch.float64)\n",
            "{'loss_classifier': tensor(0.6945, device='cuda:0'), 'loss_box_reg': tensor(0.0018, device='cuda:0'), 'loss_objectness': tensor(0.6895, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1807, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  127\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.6823, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6951, device='cuda:0'), 'loss_box_reg': tensor(0.0018, device='cuda:0'), 'loss_objectness': tensor(0.6916, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2938, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (301, 602) 2513\n",
            "image after crop is  (301, 602)\n",
            "resized shapes are  600 800\n",
            "corners  [(314, 226), (489, 226), (489, 312), (314, 312)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(16524., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (236, 584) 2727\n",
            "image after crop is  (236, 584)\n",
            "resized shapes are  600 800\n",
            "corners  [(269, 198), (580, 198), (580, 360), (269, 360)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(54876., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (456, 515) 7078\n",
            "image after crop is  (456, 515)\n",
            "resized shapes are  600 800\n",
            "corners  [(203, 129), (683, 129), (683, 445), (203, 445)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(137955., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (303, 530) 1234\n",
            "image after crop is  (303, 530)\n",
            "resized shapes are  600 800\n",
            "corners  [(303, 199), (690, 199), (690, 355), (303, 355)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(71162., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (464, 458) 2397\n",
            "image after crop is  (464, 458)\n",
            "resized shapes are  600 800\n",
            "corners  [(197, 125), (655, 125), (655, 435), (197, 435)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(121212., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (431, 613) 2319\n",
            "image after crop is  (431, 613)\n",
            "resized shapes are  600 800\n",
            "corners  [(366, 125), (653, 125), (653, 308), (366, 308)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(52624., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (456, 507) 2192\n",
            "image after crop is  (456, 507)\n",
            "resized shapes are  600 800\n",
            "corners  [(265, 207), (641, 207), (641, 449), (265, 449)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(79605., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (459, 569) 6799\n",
            "image after crop is  (459, 569)\n",
            "resized shapes are  600 800\n",
            "corners  [(345, 254), (493, 254), (493, 373), (345, 373)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(15372., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (366, 611) 337\n",
            "image after crop is  (366, 611)\n",
            "resized shapes are  600 800\n",
            "corners  [(353, 193), (579, 193), (579, 355), (353, 355)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(37636., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (424, 605) 3915\n",
            "image after crop is  (424, 605)\n",
            "resized shapes are  600 800\n",
            "corners  [(257, 180), (456, 180), (456, 338), (257, 338)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(31482., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (353, 599) 1768\n",
            "image after crop is  (353, 599)\n",
            "resized shapes are  600 800\n",
            "corners  [(202, 163), (456, 163), (456, 310), (202, 310)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(39858., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  128\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5144, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6971, device='cuda:0'), 'loss_box_reg': tensor(0.0030, device='cuda:0'), 'loss_objectness': tensor(0.6889, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1254, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  129\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.6150, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6949, device='cuda:0'), 'loss_box_reg': tensor(0.0023, device='cuda:0'), 'loss_objectness': tensor(0.6922, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2257, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (303, 478) 2622\n",
            "image after crop is  (303, 478)\n",
            "resized shapes are  600 800\n",
            "corners  [(347, 210), (536, 210), (536, 298), (347, 298)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(5140., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (421, 613) 2373\n",
            "image after crop is  (421, 613)\n",
            "resized shapes are  600 800\n",
            "corners  [(366, 125), (653, 125), (653, 308), (366, 308)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(52921., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (457, 579) 113\n",
            "image after crop is  (457, 579)\n",
            "resized shapes are  600 800\n",
            "corners  [(311, 179), (657, 179), (657, 393), (311, 393)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(69564., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (361, 722) 1870\n",
            "image after crop is  (361, 722)\n",
            "resized shapes are  600 800\n",
            "corners  [(408, 179), (530, 179), (530, 248), (408, 248)]\n",
            "found a negative box!!  [[404, 25, 526, 94]]\n",
            "skip points:  [154, 515, 4, 726]\n",
            "offset col, row  404 25\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(-6132., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (457, 582) 3673\n",
            "image after crop is  (457, 582)\n",
            "resized shapes are  600 800\n",
            "corners  [(256, 149), (656, 149), (656, 457), (256, 457)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(119387., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (426, 601) 5928\n",
            "image after crop is  (426, 601)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (454, 579) 6565\n",
            "image after crop is  (454, 579)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (457, 581) 3822\n",
            "image after crop is  (457, 581)\n",
            "resized shapes are  600 800\n",
            "corners  [(193, 114), (688, 114), (688, 458), (193, 458)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(164850., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (211, 596) 4979\n",
            "image after crop is  (211, 596)\n",
            "resized shapes are  600 800\n",
            "corners  [(180, 106), (728, 106), (728, 465), (180, 465)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(174156., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  130\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5480, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6956, device='cuda:0'), 'loss_box_reg': tensor(0.0022, device='cuda:0'), 'loss_objectness': tensor(0.6899, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1604, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  131\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (302, 612) 3255\n",
            "image after crop is  (302, 612)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5202, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6959, device='cuda:0'), 'loss_box_reg': tensor(0.0034, device='cuda:0'), 'loss_objectness': tensor(0.6906, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1304, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (421, 606) 3951\n",
            "image after crop is  (421, 606)\n",
            "resized shapes are  600 800\n",
            "corners  [(257, 180), (456, 180), (456, 338), (257, 338)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(31590., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (422, 601) 3882\n",
            "image after crop is  (422, 601)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (421, 583) 6230\n",
            "image after crop is  (421, 583)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (458, 606) 5292\n",
            "image after crop is  (458, 606)\n",
            "resized shapes are  600 800\n",
            "corners  [(426, 177), (515, 177), (515, 291), (426, 291)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(10120., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (425, 605) 6864\n",
            "image after crop is  (425, 605)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (455, 575) 70\n",
            "image after crop is  (455, 575)\n",
            "resized shapes are  600 800\n",
            "corners  [(311, 179), (657, 179), (657, 393), (311, 393)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(69564., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (421, 613) 3488\n",
            "image after crop is  (421, 613)\n",
            "resized shapes are  600 800\n",
            "corners  [(366, 189), (529, 189), (529, 330), (366, 330)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(23068., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (424, 611) 2312\n",
            "image after crop is  (424, 611)\n",
            "resized shapes are  600 800\n",
            "corners  [(366, 125), (653, 125), (653, 308), (366, 308)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(52624., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  132\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5996, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6948, device='cuda:0'), 'loss_box_reg': tensor(0.0017, device='cuda:0'), 'loss_objectness': tensor(0.6891, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2139, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  133\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.6439, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6953, device='cuda:0'), 'loss_box_reg': tensor(0.0022, device='cuda:0'), 'loss_objectness': tensor(0.6911, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2553, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (363, 609) 6288\n",
            "image after crop is  (363, 609)\n",
            "resized shapes are  600 800\n",
            "corners  [(414, 270), (534, 270), (534, 338), (414, 338)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(8755., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (301, 540) 2839\n",
            "image after crop is  (301, 540)\n",
            "resized shapes are  600 800\n",
            "corners  [(288, 196), (563, 196), (563, 352), (288, 352)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(41884., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (344, 598) 1497\n",
            "image after crop is  (344, 598)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (465, 582) 3727\n",
            "image after crop is  (465, 582)\n",
            "resized shapes are  600 800\n",
            "corners  [(256, 149), (656, 149), (656, 457), (256, 457)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(119840., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (424, 608) 3455\n",
            "image after crop is  (424, 608)\n",
            "resized shapes are  600 800\n",
            "corners  [(366, 189), (529, 189), (529, 330), (366, 330)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(23023., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (301, 716) 7319\n",
            "image after crop is  (301, 716)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (422, 613) 3907\n",
            "image after crop is  (422, 613)\n",
            "resized shapes are  600 800\n",
            "corners  [(257, 180), (456, 180), (456, 338), (257, 338)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(31590., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (420, 582) 6113\n",
            "image after crop is  (420, 582)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (357, 611) 3574\n",
            "image after crop is  (357, 611)\n",
            "resized shapes are  600 800\n",
            "corners  [(378, 221), (518, 221), (518, 331), (378, 331)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(15301., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (17, 598) 1514\n",
            "image after crop is  (17, 598)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  134\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.6272, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6954, device='cuda:0'), 'loss_box_reg': tensor(0.0027, device='cuda:0'), 'loss_objectness': tensor(0.6907, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2384, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  135\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.5967, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6954, device='cuda:0'), 'loss_box_reg': tensor(0.0017, device='cuda:0'), 'loss_objectness': tensor(0.6904, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2091, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (303, 511) 1220\n",
            "image after crop is  (303, 511)\n",
            "resized shapes are  600 800\n",
            "corners  [(303, 199), (690, 199), (690, 355), (303, 355)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(71060., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (203, 585) 5211\n",
            "image after crop is  (203, 585)\n",
            "resized shapes are  600 800\n",
            "corners  [(187, 125), (723, 125), (723, 431), (187, 431)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(130585., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (399, 660) 4784\n",
            "image after crop is  (399, 660)\n",
            "resized shapes are  600 800\n",
            "corners  [(274, 187), (488, 187), (488, 338), (274, 338)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(33150., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (407, 669) 4798\n",
            "image after crop is  (407, 669)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  136\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5855, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6963, device='cuda:0'), 'loss_box_reg': tensor(0.0021, device='cuda:0'), 'loss_objectness': tensor(0.6893, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1979, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (455, 512) 7238\n",
            "image after crop is  (455, 512)\n",
            "resized shapes are  600 800\n",
            "corners  [(184, 129), (654, 129), (654, 429), (184, 429)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(127200., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (409, 430) 5558\n",
            "image after crop is  (409, 430)\n",
            "resized shapes are  600 800\n",
            "corners  [(168, 179), (681, 179), (681, 400), (168, 400)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(84513., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (465, 516) 7185\n",
            "image after crop is  (465, 516)\n",
            "resized shapes are  600 800\n",
            "corners  [(184, 129), (654, 129), (654, 429), (184, 429)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(126024., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (424, 613) 2304\n",
            "image after crop is  (424, 613)\n",
            "resized shapes are  600 800\n",
            "corners  [(366, 125), (653, 125), (653, 308), (366, 308)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "image shape read in is  (600, 800, 3)\n",
            "setting target\n",
            "about to resize image. shape going in is @ (459, 574) 6669\n",
            "image after crop is  (459, 574)\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "resized shapes are  600 800\n",
            "area is  tensor(52624., dtype=torch.float64)\n",
            "corners  [(377, 249), (507, 249), (507, 353), (377, 353)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(11573., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (303, 536) 927\n",
            "image after crop is  (303, 536)\n",
            "resized shapes are  600 800\n",
            "image shape read in is  (600, 800, 3)\n",
            "corners  [(385, 201), (544, 201), (544, 309), (385, 309)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(16416., dtype=torch.float64)\n",
            "about to resize image. shape going in is @ (454, 578) 6765\n",
            "image after crop is  (454, 578)\n",
            "resized shapes are  600 800\n",
            "corners  [(345, 254), (493, 254), (493, 373), (345, 373)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "image shape read in is  (600, 800, 3)\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "about to resize image. shape going in is @ (420, 606) 4483\n",
            "area is  tensor(15752., dtype=torch.float64)\n",
            "image after crop is  (420, 606)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  137\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5924, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6949, device='cuda:0'), 'loss_box_reg': tensor(0.0022, device='cuda:0'), 'loss_objectness': tensor(0.6899, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2053, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  138\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.6372, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6954, device='cuda:0'), 'loss_box_reg': tensor(0.0022, device='cuda:0'), 'loss_objectness': tensor(0.6903, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2493, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (236, 588) 6077\n",
            "image after crop is  (236, 588)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (361, 720) 1896\n",
            "image after crop is  (361, 720)\n",
            "resized shapes are  600 800\n",
            "corners  [(408, 179), (530, 179), (530, 248), (408, 248)]\n",
            "found a negative box!!  [[404, 25, 526, 94]]\n",
            "skip points:  [154, 515, 4, 724]\n",
            "offset col, row  404 25\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(-6132., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (456, 570) 3311\n",
            "image after crop is  (456, 570)\n",
            "resized shapes are  600 800\n",
            "corners  [(330, 170), (498, 170), (498, 307), (330, 307)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(21186., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (323, 618) 5503\n",
            "image after crop is  (323, 618)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (454, 576) 196\n",
            "image after crop is  (454, 576)\n",
            "resized shapes are  600 800\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 706) 4649\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "image after crop is  (362, 706)\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "resized shapes are  600 800\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (423, 608) 4273\n",
            "resized shapes are  600 800\n",
            "image after crop is  (423, 608)\n",
            "corners  [(160, 152), (672, 152), (672, 364), (160, 364)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(109728., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (365, 612) 342\n",
            "image after crop is  (365, 612)\n",
            "resized shapes are  600 800\n",
            "corners  [(353, 193), (579, 193), (579, 355), (353, 355)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(37635., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (338, 607) 5362\n",
            "image after crop is  (338, 607)\n",
            "resized shapes are  600 800\n",
            "corners  [(373, 202), (540, 202), (540, 281), (373, 281)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(14985., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (423, 596) 4267\n",
            "image after crop is  (423, 596)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (457, 580) 80\n",
            "image after crop is  (457, 580)\n",
            "resized shapes are  600 800\n",
            "corners  [(311, 179), (657, 179), (657, 393), (311, 393)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(69564., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (458, 581) 787\n",
            "image after crop is  (458, 581)\n",
            "resized shapes are  600 800\n",
            "corners  [(184, 131), (497, 131), (497, 347), (184, 347)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(63954., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (427, 613) 3430\n",
            "image after crop is  (427, 613)\n",
            "resized shapes are  600 800\n",
            "corners  [(368, 200), (532, 200), (532, 325), (368, 325)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(20538., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  139\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.6002, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6957, device='cuda:0'), 'loss_box_reg': tensor(0.0016, device='cuda:0'), 'loss_objectness': tensor(0.6891, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2138, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  140\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.6559, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6958, device='cuda:0'), 'loss_box_reg': tensor(0.0016, device='cuda:0'), 'loss_objectness': tensor(0.6904, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2681, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (463, 573) 3776\n",
            "image after crop is  (463, 573)\n",
            "resized shapes are  600 800\n",
            "corners  [(193, 114), (688, 114), (688, 458), (193, 458)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(164424., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (246, 608) 2746\n",
            "image after crop is  (246, 608)\n",
            "resized shapes are  600 800\n",
            "corners  [(269, 198), (580, 198), (580, 360), (269, 360)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(55692., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (411, 507) 5658\n",
            "image after crop is  (411, 507)\n",
            "resized shapes are  600 800\n",
            "corners  [(272, 166), (590, 166), (590, 411), (272, 411)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(72102., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (455, 576) 148\n",
            "image after crop is  (455, 576)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (301, 601) 2592\n",
            "image after crop is  (301, 601)\n",
            "resized shapes are  600 800\n",
            "corners  [(347, 210), (536, 210), (536, 298), (347, 298)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(19162., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (221, 568) 5134\n",
            "image after crop is  (221, 568)\n",
            "resized shapes are  600 800\n",
            "corners  [(165, 120), (720, 120), (720, 441), (165, 441)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(151040., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (422, 430) 6176\n",
            "image after crop is  (422, 430)\n",
            "resized shapes are  600 800\n",
            "image shape read in is  (600, 800, 3)\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "about to resize image. shape going in is @ (381, 679) 7663\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "image after crop is  (381, 679)\n",
            "resized shapes are  600 800\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (423, 592) 4293\n",
            "image after crop is  (423, 592)\n",
            "resized shapes are  600 800\n",
            "corners  [(160, 152), (672, 152), (672, 364), (160, 364)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(109728., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (426, 608) 5910\n",
            "image after crop is  (426, 608)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  141\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5889, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6951, device='cuda:0'), 'loss_box_reg': tensor(0.0017, device='cuda:0'), 'loss_objectness': tensor(0.6889, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2031, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  142\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5870, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6949, device='cuda:0'), 'loss_box_reg': tensor(0.0022, device='cuda:0'), 'loss_objectness': tensor(0.6895, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2004, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (302, 544) 1110\n",
            "image after crop is  (302, 544)\n",
            "resized shapes are  600 800\n",
            "corners  [(244, 216), (436, 216), (436, 365), (244, 365)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(28420., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (456, 575) 3286\n",
            "image after crop is  (456, 575)\n",
            "resized shapes are  600 800\n",
            "corners  [(330, 170), (498, 170), (498, 307), (330, 307)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(21186., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (283, 614) 976\n",
            "image after crop is  (283, 614)\n",
            "resized shapes are  600 800\n",
            "corners  [(393, 219), (551, 219), (551, 327), (393, 327)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(16008., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (365, 607) 631\n",
            "image after crop is  (365, 607)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (17, 598) 1517\n",
            "image after crop is  (17, 598)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (459, 568) 6814\n",
            "image after crop is  (459, 568)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (421, 613) 2366\n",
            "image after crop is  (421, 613)\n",
            "resized shapes are  600 800\n",
            "corners  [(366, 125), (653, 125), (653, 308), (366, 308)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(52921., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (426, 606) 4137\n",
            "image after crop is  (426, 606)\n",
            "resized shapes are  600 800\n",
            "corners  [(160, 153), (582, 153), (582, 365), (160, 365)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(89880., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (363, 614) 411\n",
            "image after crop is  (363, 614)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (455, 514) 7278\n",
            "image after crop is  (455, 514)\n",
            "resized shapes are  600 800\n",
            "corners  [(184, 129), (654, 129), (654, 429), (184, 429)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(127489., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  143\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.4624, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6953, device='cuda:0'), 'loss_box_reg': tensor(0.0026, device='cuda:0'), 'loss_objectness': tensor(0.6934, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.0712, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  144\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (424, 611) 4018\n",
            "image after crop is  (424, 611)\n",
            "resized shapes are  600 800\n",
            "corners  [(276, 248), (474, 248), (474, 363), (276, 363)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(22932., dtype=torch.float64)\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5738, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6944, device='cuda:0'), 'loss_box_reg': tensor(0.0017, device='cuda:0'), 'loss_objectness': tensor(0.6904, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1875, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (24, 599) 5266\n",
            "image after crop is  (24, 599)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (364, 703) 4644\n",
            "image after crop is  (364, 703)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (363, 607) 1663\n",
            "image after crop is  (363, 607)\n",
            "resized shapes are  600 800\n",
            "corners  [(278, 181), (501, 181), (501, 305), (278, 305)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(29796., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (357, 611) 3573\n",
            "image after crop is  (357, 611)\n",
            "resized shapes are  600 800\n",
            "corners  [(378, 221), (518, 221), (518, 331), (378, 331)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(15301., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (365, 611) 2909\n",
            "image after crop is  (365, 611)\n",
            "resized shapes are  600 800\n",
            "corners  [(375, 189), (568, 189), (568, 344), (375, 344)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(30132., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (290, 611) 916\n",
            "image after crop is  (290, 611)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (454, 575) 71\n",
            "image after crop is  (454, 575)\n",
            "resized shapes are  600 800\n",
            "corners  [(311, 179), (657, 179), (657, 393), (311, 393)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(69564., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (455, 582) 3833\n",
            "image after crop is  (455, 582)\n",
            "resized shapes are  600 800\n",
            "corners  [(193, 114), (688, 114), (688, 458), (193, 458)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(165474., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (302, 609) 3375\n",
            "image after crop is  (302, 609)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (424, 603) 2286\n",
            "image after crop is  (424, 603)\n",
            "resized shapes are  600 800\n",
            "corners  [(377, 127), (675, 127), (675, 317), (377, 317)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(56832., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (455, 579) 256\n",
            "image after crop is  (455, 579)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  145\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.6248, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6959, device='cuda:0'), 'loss_box_reg': tensor(0.0011, device='cuda:0'), 'loss_objectness': tensor(0.6890, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2388, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  146\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.6282, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6947, device='cuda:0'), 'loss_box_reg': tensor(0.0010, device='cuda:0'), 'loss_objectness': tensor(0.6889, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2435, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 709) 4599\n",
            "image after crop is  (362, 709)\n",
            "resized shapes are  600 800\n",
            "corners  [(427, 219), (537, 219), (537, 326), (427, 326)]\n",
            "found a negative box!!  [[404, 66, 514, 173]]\n",
            "skip points:  [153, 515, 23, 732]\n",
            "offset col, row  404 66\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(-4740., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (379, 683) 7617\n",
            "image after crop is  (379, 683)\n",
            "resized shapes are  600 800\n",
            "corners  [(247, 256), (458, 256), (458, 423), (247, 423)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(24485., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (366, 607) 6356\n",
            "image after crop is  (366, 607)\n",
            "resized shapes are  600 800\n",
            "corners  [(411, 260), (530, 260), (530, 339), (411, 339)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(9605., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (85, 50) 7054\n",
            "image after crop is  (85, 50)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (475, 582) 3768\n",
            "image after crop is  (475, 582)\n",
            "resized shapes are  600 800\n",
            "corners  [(256, 149), (656, 149), (656, 457), (256, 457)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(119387., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (302, 611) 3353\n",
            "image after crop is  (302, 611)\n",
            "resized shapes are  600 800\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (421, 610) 4082\n",
            "image after crop is  (421, 610)\n",
            "resized shapes are  600 800\n",
            "corners  [(456, 249), (586, 249), (586, 341), (456, 341)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "setting target\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(10557., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (195, 568) 5214\n",
            "image after crop is  (195, 568)\n",
            "resized shapes are  600 800\n",
            "corners  [(187, 125), (723, 125), (723, 431), (187, 431)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(124341., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "image after crop is  (470, 601)\n",
            "about to resize image. shape going in is @ (470, 601) 4984\n",
            "------------------------------------dataload loop is  147\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "resized shapes are  600 800\n",
            "corners  [(180, 106), (728, 106), (728, 465), (180, 465)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(174156., dtype=torch.float64)\n",
            "*** SUMMED losses is  tensor(3.7668, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6948, device='cuda:0'), 'loss_box_reg': tensor(0.0020, device='cuda:0'), 'loss_objectness': tensor(0.6923, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(2.3778, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  148\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5640, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6956, device='cuda:0'), 'loss_box_reg': tensor(0.0028, device='cuda:0'), 'loss_objectness': tensor(0.6889, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1766, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (413, 490) 5616\n",
            "image after crop is  (413, 490)\n",
            "resized shapes are  600 800\n",
            "corners  [(272, 166), (590, 166), (590, 411), (272, 411)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(68940., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 612) 3134\n",
            "image after crop is  (362, 612)\n",
            "resized shapes are  600 800\n",
            "corners  [(339, 183), (487, 183), (487, 300), (339, 300)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(17250., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (361, 607) 629\n",
            "image after crop is  (361, 607)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (424, 611) 3989\n",
            "image after crop is  (424, 611)\n",
            "resized shapes are  600 800\n",
            "corners  [(344, 184), (516, 184), (516, 325), (344, 325)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(24360., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (275, 608) 1050\n",
            "image after crop is  (275, 608)\n",
            "resized shapes are  600 800\n",
            "corners  [(395, 217), (631, 217), (631, 359), (395, 359)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(35432., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (319, 604) 5450\n",
            "image after crop is  (319, 604)\n",
            "resized shapes are  600 800\n",
            "corners  [(346, 208), (486, 208), (486, 280), (346, 280)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(11200., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (303, 606) 3350\n",
            "image after crop is  (303, 606)\n",
            "resized shapes are  600 800\n",
            "corners  [(456, 249), (586, 249), (586, 341), (456, 341)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(10385., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (367, 609) 6353\n",
            "image after crop is  (367, 609)\n",
            "resized shapes are  600 800\n",
            "corners  [(411, 260), (530, 260), (530, 339), (411, 339)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(9752., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (302, 606) 2499\n",
            "image after crop is  (302, 606)\n",
            "resized shapes are  600 800\n",
            "corners  [(314, 226), (489, 226), (489, 312), (314, 312)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(16650., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (381, 678) 7680\n",
            "image after crop is  (381, 678)\n",
            "resized shapes are  600 800\n",
            "corners  [(268, 256), (469, 256), (469, 400), (268, 400)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(20250., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (421, 594) 6016\n",
            "image after crop is  (421, 594)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  149\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.6174, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6946, device='cuda:0'), 'loss_box_reg': tensor(0.0021, device='cuda:0'), 'loss_objectness': tensor(0.6916, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2290, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  150\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.6125, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6960, device='cuda:0'), 'loss_box_reg': tensor(0.0018, device='cuda:0'), 'loss_objectness': tensor(0.6906, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2240, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (424, 612) 6851\n",
            "image after crop is  (424, 612)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (424, 606) 2254\n",
            "image after crop is  (424, 606)\n",
            "resized shapes are  600 800\n",
            "corners  [(377, 127), (675, 127), (675, 317), (377, 317)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(56727., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 609) 618\n",
            "image after crop is  (362, 609)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (423, 608) 5836\n",
            "image after crop is  (423, 608)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (305, 711) 7551\n",
            "image after crop is  (305, 711)\n",
            "resized shapes are  600 800\n",
            "corners  [(400, 239), (529, 239), (529, 326), (400, 326)]\n",
            "found a negative box!!  [[384, 55, 513, 142]]\n",
            "skip points:  [184, 489, 16, 727]\n",
            "offset col, row  384 55\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(-9945., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (463, 459) 6921\n",
            "image after crop is  (463, 459)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (468, 575) 3705\n",
            "image after crop is  (468, 575)\n",
            "resized shapes are  600 800\n",
            "corners  [(256, 149), (656, 149), (656, 457), (256, 457)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(119387., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (459, 461) 2443\n",
            "image after crop is  (459, 461)\n",
            "resized shapes are  600 800\n",
            "corners  [(197, 125), (655, 125), (655, 435), (197, 435)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(121535., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  151\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.7107, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6954, device='cuda:0'), 'loss_box_reg': tensor(0.0016, device='cuda:0'), 'loss_objectness': tensor(0.6916, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.3222, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  152\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(3.6763, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6970, device='cuda:0'), 'loss_box_reg': tensor(0.0019, device='cuda:0'), 'loss_objectness': tensor(0.6928, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(2.2846, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (297, 604) 5467\n",
            "image after crop is  (297, 604)\n",
            "resized shapes are  600 800\n",
            "corners  [(346, 208), (486, 208), (486, 280), (346, 280)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(11235., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (428, 607) 6829\n",
            "image after crop is  (428, 607)\n",
            "resized shapes are  600 800\n",
            "corners  [(397, 250), (536, 250), (536, 348), (397, 348)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (344, 598) 1460\n",
            "image after crop is  (344, 598)\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(13770., dtype=torch.float64)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (364, 612) 6296\n",
            "image after crop is  (364, 612)\n",
            "resized shapes are  600 800\n",
            "corners  [(414, 270), (534, 270), (534, 338), (414, 338)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(8787., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (301, 596) 949\n",
            "image after crop is  (301, 596)\n",
            "resized shapes are  600 800\n",
            "corners  [(385, 201), (544, 201), (544, 309), (385, 309)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(16182., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (422, 580) 6103\n",
            "image after crop is  (422, 580)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "image shape read in is  (600, 800, 3)\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "about to resize image. shape going in is @ (301, 611) 911\n",
            "image after crop is  (301, 611)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (344, 598) 1463\n",
            "image after crop is  (344, 598)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (361, 726) 1867\n",
            "image after crop is  (361, 726)\n",
            "resized shapes are  600 800\n",
            "corners  [(408, 179), (530, 179), (530, 248), (408, 248)]\n",
            "found a negative box!!  [[404, 25, 526, 94]]\n",
            "skip points:  [154, 515, 4, 730]\n",
            "offset col, row  404 25\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(-6132., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (454, 575) 6700\n",
            "image after crop is  (454, 575)\n",
            "resized shapes are  600 800\n",
            "corners  [(377, 249), (507, 249), (507, 353), (377, 353)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(11925., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (379, 674) 7616\n",
            "image after crop is  (379, 674)\n",
            "resized shapes are  600 800\n",
            "corners  [(247, 256), (458, 256), (458, 423), (247, 423)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(24485., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  153\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(4.8296, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6940, device='cuda:0'), 'loss_box_reg': tensor(0.0016, device='cuda:0'), 'loss_objectness': tensor(0.6940, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(3.4400, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  154\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.6611, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6955, device='cuda:0'), 'loss_box_reg': tensor(0.0023, device='cuda:0'), 'loss_objectness': tensor(0.6901, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2732, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (412, 458) 5529\n",
            "image after crop is  (412, 458)\n",
            "resized shapes are  600 800\n",
            "corners  [(168, 179), (681, 179), (681, 400), (168, 400)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "area is  tensor(76608., dtype=torch.float64)\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (456, 517) 7249\n",
            "image after crop is  (456, 517)\n",
            "resized shapes are  600 800\n",
            "corners  [(184, 129), (654, 129), (654, 429), (184, 429)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(127200., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 707) 4514\n",
            "image after crop is  (362, 707)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (366, 611) 3046\n",
            "image after crop is  (366, 611)\n",
            "resized shapes are  600 800\n",
            "corners  [(339, 183), (487, 183), (487, 300), (339, 300)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(17176., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (363, 611) 6259\n",
            "image after crop is  (363, 611)\n",
            "resized shapes are  600 800\n",
            "corners  [(414, 270), (534, 270), (534, 338), (414, 338)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(8772., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (366, 609) 623\n",
            "image after crop is  (366, 609)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (367, 611) 3085\n",
            "image after crop is  (367, 611)\n",
            "resized shapes are  600 800\n",
            "corners  [(339, 183), (487, 183), (487, 300), (339, 300)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(17176., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (444, 660) 5077\n",
            "image after crop is  (444, 660)\n",
            "resized shapes are  600 800\n",
            "corners  [(186, 117), (731, 117), (731, 446), (186, 446)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(171925., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  155\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5567, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6961, device='cuda:0'), 'loss_box_reg': tensor(0.0027, device='cuda:0'), 'loss_objectness': tensor(0.6893, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1686, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  156\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.6248, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6955, device='cuda:0'), 'loss_box_reg': tensor(0.0013, device='cuda:0'), 'loss_objectness': tensor(0.6882, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2397, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (411, 465) 5511\n",
            "image after crop is  (411, 465)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (423, 602) 6148\n",
            "image after crop is  (423, 602)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (457, 579) 105\n",
            "image after crop is  (457, 579)\n",
            "resized shapes are  600 800\n",
            "corners  [(311, 179), (657, 179), (657, 393), (311, 393)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(68991., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (364, 706) 4659\n",
            "image after crop is  (364, 706)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (454, 580) 6644\n",
            "image after crop is  (454, 580)\n",
            "resized shapes are  600 800\n",
            "corners  [(280, 169), (470, 169), (470, 298), (280, 298)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(22018., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (294, 610) 1313\n",
            "image after crop is  (294, 610)\n",
            "resized shapes are  600 800\n",
            "corners  [(314, 220), (619, 220), (619, 375), (314, 375)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(52779., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (304, 716) 7583\n",
            "image after crop is  (304, 716)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (295, 608) 1323\n",
            "image after crop is  (295, 608)\n",
            "resized shapes are  600 800\n",
            "corners  [(344, 218), (684, 218), (684, 374), (344, 374)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(60604., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (205, 624) 4957\n",
            "image after crop is  (205, 624)\n",
            "resized shapes are  600 800\n",
            "corners  [(180, 106), (728, 106), (728, 465), (180, 465)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(185070., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (302, 600) 1073\n",
            "about to resize image. shape going in is @ (301, 706) 7458\n",
            "image after crop is  (302, 600)\n",
            "image after crop is  (301, 706)\n",
            "resized shapes are  600 800\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  157\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.6505, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6939, device='cuda:0'), 'loss_box_reg': tensor(0.0004, device='cuda:0'), 'loss_objectness': tensor(0.6887, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2675, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  158\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.6644, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6960, device='cuda:0'), 'loss_box_reg': tensor(0.0020, device='cuda:0'), 'loss_objectness': tensor(0.6911, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2752, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (420, 608) 5673\n",
            "about to resize image. shape going in is @ (461, 579) 6786\n",
            "image after crop is  (461, 579)\n",
            "resized shapes are  600 800\n",
            "image after crop is  (420, 608)\n",
            "corners  [(345, 254), (493, 254), (493, 373), (345, 373)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "resized shapes are  600 800\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "corners  [(259, 148), (470, 148), (470, 290), (259, 290)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "area is  tensor(16016., dtype=torch.float64)\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(30282., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (397, 670) 4847\n",
            "image after crop is  (397, 670)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (309, 608) 1350\n",
            "image after crop is  (309, 608)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (427, 600) 4200\n",
            "image after crop is  (427, 600)\n",
            "resized shapes are  600 800\n",
            "corners  [(241, 149), (681, 149), (681, 371), (241, 371)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(97240., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (227, 598) 2753\n",
            "image after crop is  (227, 598)\n",
            "resized shapes are  600 800\n",
            "corners  [(269, 198), (580, 198), (580, 360), (269, 360)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(55692., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (456, 573) 751\n",
            "image after crop is  (456, 573)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 706) 4586\n",
            "image after crop is  (362, 706)\n",
            "resized shapes are  600 800\n",
            "corners  [(427, 219), (537, 219), (537, 326), (427, 326)]\n",
            "found a negative box!!  [[404, 66, 514, 173]]\n",
            "skip points:  [153, 515, 23, 729]\n",
            "offset col, row  404 66\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(-4740., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (397, 677) 4773\n",
            "image after crop is  (397, 677)\n",
            "resized shapes are  600 800\n",
            "about to resize image. shape going in is @ (402, 669) 4827\n",
            "image after crop is  (402, 669)\n",
            "corners  [(274, 187), (488, 187), (488, 338), (274, 338)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "resized shapes are  600 800\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(33000., dtype=torch.float64)\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (420, 613) 2240\n",
            "image after crop is  (420, 613)\n",
            "resized shapes are  600 800\n",
            "corners  [(377, 127), (675, 127), (675, 317), (377, 317)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(57135., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  159\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5567, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6949, device='cuda:0'), 'loss_box_reg': tensor(0.0028, device='cuda:0'), 'loss_objectness': tensor(0.6891, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1699, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  160\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (455, 580) 119\n",
            "image after crop is  (455, 580)\n",
            "resized shapes are  600 800\n",
            "corners  [(311, 179), (657, 179), (657, 393), (311, 393)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(69375., dtype=torch.float64)\n",
            "*** SUMMED losses is  tensor(1.5944, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6957, device='cuda:0'), 'loss_box_reg': tensor(0.0015, device='cuda:0'), 'loss_objectness': tensor(0.6895, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2076, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (399, 673) 4924\n",
            "image after crop is  (399, 673)\n",
            "resized shapes are  600 800\n",
            "corners  [(311, 203), (483, 203), (483, 354), (311, 354)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(24600., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (221, 620) 5020\n",
            "image after crop is  (221, 620)\n",
            "resized shapes are  600 800\n",
            "corners  [(186, 117), (731, 117), (731, 446), (186, 446)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(162408., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (366, 703) 4522\n",
            "image after crop is  (366, 703)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 609) 319\n",
            "image after crop is  (362, 609)\n",
            "resized shapes are  600 800\n",
            "corners  [(353, 193), (579, 193), (579, 355), (353, 355)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(37632., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (303, 606) 2492\n",
            "image after crop is  (303, 606)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (455, 578) 65\n",
            "image after crop is  (455, 578)\n",
            "resized shapes are  600 800\n",
            "corners  [(311, 179), (657, 179), (657, 393), (311, 393)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(68796., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (364, 607) 6376\n",
            "image after crop is  (364, 607)\n",
            "resized shapes are  600 800\n",
            "corners  [(411, 260), (530, 260), (530, 339), (411, 339)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(9720., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (475, 576) 3752\n",
            "image after crop is  (475, 576)\n",
            "resized shapes are  600 800\n",
            "corners  [(256, 149), (656, 149), (656, 457), (256, 457)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(119387., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (353, 611) 1433\n",
            "image after crop is  (353, 611)\n",
            "resized shapes are  600 800\n",
            "corners  [(187, 158), (452, 158), (452, 307), (187, 307)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(42173., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (420, 603) 4055\n",
            "image after crop is  (420, 603)\n",
            "resized shapes are  600 800\n",
            "corners  [(276, 248), (474, 248), (474, 363), (276, 363)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(23010., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  161\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (454, 608) 5319\n",
            "image after crop is  (454, 608)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "*** SUMMED losses is  tensor(2.6564, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6956, device='cuda:0'), 'loss_box_reg': tensor(0.0017, device='cuda:0'), 'loss_objectness': tensor(0.6897, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2694, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  162\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.7403, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6960, device='cuda:0'), 'loss_box_reg': tensor(0.0014, device='cuda:0'), 'loss_objectness': tensor(0.6906, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.3523, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (455, 573) 6621\n",
            "image after crop is  (455, 573)\n",
            "resized shapes are  600 800\n",
            "corners  [(280, 169), (470, 169), (470, 298), (280, 298)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(22018., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (454, 574) 127\n",
            "image after crop is  (454, 574)\n",
            "resized shapes are  600 800\n",
            "corners  [(311, 179), (657, 179), (657, 393), (311, 393)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(69564., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (306, 604) 1275\n",
            "image after crop is  (306, 604)\n",
            "resized shapes are  600 800\n",
            "corners  [(303, 199), (690, 199), (690, 355), (303, 355)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(70850., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (363, 608) 1682\n",
            "image after crop is  (363, 608)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 710) 4561\n",
            "image after crop is  (362, 710)\n",
            "resized shapes are  600 800\n",
            "corners  [(427, 219), (537, 219), (537, 326), (427, 326)]\n",
            "found a negative box!!  [[404, 66, 514, 173]]\n",
            "skip points:  [153, 515, 23, 733]\n",
            "offset col, row  404 66\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(-4740., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (428, 605) 4132\n",
            "image after crop is  (428, 605)\n",
            "resized shapes are  600 800\n",
            "corners  [(160, 153), (582, 153), (582, 365), (160, 365)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(89880., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (422, 608) 6413\n",
            "image after crop is  (422, 608)\n",
            "resized shapes are  600 800\n",
            "corners  [(239, 196), (633, 196), (633, 411), (239, 411)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(85410., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (305, 602) 1391\n",
            "image after crop is  (305, 602)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  163\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.7329, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6949, device='cuda:0'), 'loss_box_reg': tensor(0.0021, device='cuda:0'), 'loss_objectness': tensor(0.6926, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.3434, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  164\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5436, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6960, device='cuda:0'), 'loss_box_reg': tensor(0.0021, device='cuda:0'), 'loss_objectness': tensor(0.6905, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1550, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (460, 516) 7179\n",
            "image after crop is  (460, 516)\n",
            "resized shapes are  600 800\n",
            "corners  [(184, 129), (654, 129), (654, 429), (184, 429)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (421, 605) 3518\n",
            "image after crop is  (421, 605)\n",
            "resized shapes are  600 800\n",
            "corners  [(366, 189), (529, 189), (529, 330), (366, 330)]\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "area is  tensor(126024., dtype=torch.float64)\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(23068., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (424, 612) 3986\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (420, 606) 3450\n",
            "image after crop is  (420, 606)\n",
            "resized shapes are  600 800\n",
            "image after crop is  (424, 612)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "corners  [(344, 184), (516, 184), (516, 325), (344, 325)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "setting target\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(24360., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (426, 538) 4231\n",
            "image after crop is  (426, 538)\n",
            "resized shapes are  600 800\n",
            "corners  [(241, 149), (681, 149), (681, 371), (241, 371)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "setting target\n",
            "area is  tensor(97461., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (364, 614) 2980\n",
            "image after crop is  (364, 614)\n",
            "resized shapes are  600 800\n",
            "corners  [(375, 189), (568, 189), (568, 344), (375, 344)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(30020., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (290, 564) 5112\n",
            "image after crop is  (290, 564)\n",
            "resized shapes are  600 800\n",
            "corners  [(165, 120), (720, 120), (720, 441), (165, 441)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(135675., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (25, 715) 641\n",
            "image after crop is  (25, 715)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (258, 582) 2856\n",
            "image after crop is  (258, 582)\n",
            "resized shapes are  600 800\n",
            "corners  [(288, 196), (563, 196), (563, 352), (288, 352)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(46168., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 611) 3024\n",
            "image after crop is  (362, 611)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  165\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.6130, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6960, device='cuda:0'), 'loss_box_reg': tensor(0.0017, device='cuda:0'), 'loss_objectness': tensor(0.6883, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2270, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  166\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5281, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6958, device='cuda:0'), 'loss_box_reg': tensor(0.0026, device='cuda:0'), 'loss_objectness': tensor(0.6897, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (363, 608) 1425\n",
            "image after crop is  (363, 608)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (465, 576) 6727\n",
            "image after crop is  (465, 576)\n",
            "resized shapes are  600 800\n",
            "corners  [(377, 249), (507, 249), (507, 353), (377, 353)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(11664., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 612) 6386\n",
            "image after crop is  (362, 612)\n",
            "resized shapes are  600 800\n",
            "corners  [(411, 260), (530, 260), (530, 339), (411, 339)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(9545., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (421, 608) 2382\n",
            "image after crop is  (421, 608)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (459, 513) 2174\n",
            "image after crop is  (459, 513)\n",
            "resized shapes are  600 800\n",
            "corners  [(265, 207), (641, 207), (641, 449), (265, 449)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(77792., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (423, 607) 2353\n",
            "image after crop is  (423, 607)\n",
            "resized shapes are  600 800\n",
            "corners  [(366, 125), (653, 125), (653, 308), (366, 308)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(52921., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (340, 598) 1475\n",
            "image after crop is  (340, 598)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  167\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(3.7353, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6948, device='cuda:0'), 'loss_box_reg': tensor(0.0016, device='cuda:0'), 'loss_objectness': tensor(0.6937, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(2.3451, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  168\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5706, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (305, 522) 2607\n",
            "image after crop is  (305, 522)\n",
            "resized shapes are  600 800\n",
            "corners  [(347, 210), (536, 210), (536, 298), (347, 298)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(13176., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (425, 602) 4177\n",
            "image after crop is  (425, 602)\n",
            "resized shapes are  600 800\n",
            "corners  [(241, 149), (681, 149), (681, 371), (241, 371)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(97680., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (305, 610) 3364\n",
            "image after crop is  (305, 610)\n",
            "resized shapes are  600 800\n",
            "corners  [(456, 249), (586, 249), (586, 341), (456, 341)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(10472., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (381, 679) 7664\n",
            "image after crop is  (381, 679)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (422, 608) 3999\n",
            "image after crop is  (422, 608)\n",
            "resized shapes are  600 800\n",
            "corners  [(344, 184), (516, 184), (516, 325), (344, 325)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(24360., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (353, 599) 1765\n",
            "image after crop is  (353, 599)\n",
            "resized shapes are  600 800\n",
            "corners  [(202, 163), (456, 163), (456, 310), (202, 310)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(39858., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (330, 579) 5417\n",
            "image after crop is  (330, 579)\n",
            "resized shapes are  600 800\n",
            "corners  [(346, 208), (486, 208), (486, 280), (346, 280)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(10452., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (303, 511) 1222\n",
            "image after crop is  (303, 511)\n",
            "resized shapes are  600 800\n",
            "corners  [(303, 199), (690, 199), (690, 355), (303, 355)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(71060., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (461, 574) 710\n",
            "image after crop is  (461, 574)\n",
            "resized shapes are  600 800\n",
            "corners  [(346, 128), (624, 128), (624, 339), (346, 339)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(55358., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 605) 324\n",
            "image after crop is  (362, 605)\n",
            "resized shapes are  600 800\n",
            "corners  [(353, 193), (579, 193), (579, 355), (353, 355)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(37636., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (422, 608) 4286\n",
            "image after crop is  (422, 608)\n",
            "resized shapes are  600 800\n",
            "corners  [(160, 152), (672, 152), (672, 364), (160, 364)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(109728., dtype=torch.float64)\n",
            "{'loss_classifier': tensor(0.6940, device='cuda:0'), 'loss_box_reg': tensor(0.0018, device='cuda:0'), 'loss_objectness': tensor(0.6890, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1858, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  169\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5477, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6945, device='cuda:0'), 'loss_box_reg': tensor(0.0029, device='cuda:0'), 'loss_objectness': tensor(0.6902, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1601, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  170\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.5881, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6957, device='cuda:0'), 'loss_box_reg': tensor(0.0021, device='cuda:0'), 'loss_objectness': tensor(0.6911, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.1993, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (455, 574) 822\n",
            "image after crop is  (455, 574)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (391, 654) 5013\n",
            "image after crop is  (391, 654)\n",
            "resized shapes are  600 800\n",
            "corners  [(180, 106), (728, 106), (728, 465), (180, 465)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(191856., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (458, 606) 5295\n",
            "image after crop is  (458, 606)\n",
            "resized shapes are  600 800\n",
            "corners  [(426, 177), (515, 177), (515, 291), (426, 291)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(10120., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (457, 578) 6827\n",
            "image after crop is  (457, 578)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (369, 611) 511\n",
            "image after crop is  (369, 611)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (237, 727) 6995\n",
            "image after crop is  (237, 727)\n",
            "resized shapes are  600 800\n",
            "corners  [(386, 185), (498, 185), (498, 281), (386, 281)]\n",
            "found a negative box!!  [[382, 31, 494, 127]]\n",
            "skip points:  [154, 391, 4, 731]\n",
            "offset col, row  382 31\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(-9348., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (367, 609) 6251\n",
            "image after crop is  (367, 609)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (421, 608) 6835\n",
            "image after crop is  (421, 608)\n",
            "resized shapes are  600 800\n",
            "corners  [(397, 250), (536, 250), (536, 348), (397, 348)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(13770., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (305, 502) 2609\n",
            "image after crop is  (305, 502)\n",
            "resized shapes are  600 800\n",
            "corners  [(347, 210), (536, 210), (536, 298), (347, 298)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "image shape read in is  (600, 800, 3)\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(10440., dtype=torch.float64)\n",
            "about to resize image. shape going in is @ (348, 605) 5444\n",
            "image after crop is  (348, 605)\n",
            "resized shapes are  600 800\n",
            "corners  [(346, 208), (486, 208), (486, 280), (346, 280)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(11200., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (302, 510) 1210\n",
            "image after crop is  (302, 510)\n",
            "resized shapes are  600 800\n",
            "corners  [(303, 199), (690, 199), (690, 355), (303, 355)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(70956., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 602) 6349\n",
            "image after crop is  (362, 602)\n",
            "resized shapes are  600 800\n",
            "corners  [(411, 260), (530, 260), (530, 339), (411, 339)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(9737., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (301, 606) 1076\n",
            "image after crop is  (301, 606)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  171\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.6458, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6946, device='cuda:0'), 'loss_box_reg': tensor(0.0016, device='cuda:0'), 'loss_objectness': tensor(0.6905, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2591, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  172\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.5520, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6962, device='cuda:0'), 'loss_box_reg': tensor(0.0021, device='cuda:0'), 'loss_objectness': tensor(0.6900, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.1637, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (361, 607) 1704\n",
            "image after crop is  (361, 607)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (302, 567) 1146\n",
            "image after crop is  (302, 567)\n",
            "resized shapes are  600 800\n",
            "corners  [(221, 201), (628, 201), (628, 352), (221, 352)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(65741., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (304, 605) 2487\n",
            "image after crop is  (304, 605)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (391, 657) 5012\n",
            "image after crop is  (391, 657)\n",
            "resized shapes are  600 800\n",
            "corners  [(180, 106), (728, 106), (728, 465), (180, 465)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(191856., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (421, 612) 6460\n",
            "image after crop is  (421, 612)\n",
            "resized shapes are  600 800\n",
            "corners  [(239, 196), (633, 196), (633, 411), (239, 411)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(85410., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (235, 641) 4950\n",
            "image after crop is  (235, 641)\n",
            "resized shapes are  600 800\n",
            "corners  [(180, 106), (728, 106), (728, 465), (180, 465)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(187302., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (366, 596) 1687\n",
            "image after crop is  (366, 596)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (287, 602) 5399\n",
            "image after crop is  (287, 602)\n",
            "resized shapes are  600 800\n",
            "corners  [(373, 202), (540, 202), (540, 281), (373, 281)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(14768., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (361, 611) 3014\n",
            "image after crop is  (361, 611)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  173\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5870, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6948, device='cuda:0'), 'loss_box_reg': tensor(0.0017, device='cuda:0'), 'loss_objectness': tensor(0.6886, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2019, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  174\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(3.7466, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6957, device='cuda:0'), 'loss_box_reg': tensor(0.0012, device='cuda:0'), 'loss_objectness': tensor(0.6950, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(2.3548, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (25, 709) 672\n",
            "image after crop is  (25, 709)\n",
            "resized shapes are  600 800\n",
            "corners  [(171, 132), (484, 132), (484, 325), (171, 325)]\n",
            "found a negative box!!  [[154, -421, 467, -228]]\n",
            "skip points:  [553, 578, 17, 726]\n",
            "offset col, row  154 -421\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(-162567., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (304, 572) 1359\n",
            "image after crop is  (304, 572)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (458, 581) 789\n",
            "image after crop is  (458, 581)\n",
            "resized shapes are  600 800\n",
            "corners  [(184, 131), (497, 131), (497, 347), (184, 347)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(63954., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (365, 613) 306\n",
            "image after crop is  (365, 613)\n",
            "resized shapes are  600 800\n",
            "corners  [(353, 193), (579, 193), (579, 355), (353, 355)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(37635., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (285, 606) 1329\n",
            "image after crop is  (285, 606)\n",
            "resized shapes are  600 800\n",
            "corners  [(300, 220), (686, 220), (686, 375), (300, 375)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(70620., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (364, 614) 447\n",
            "image after crop is  (364, 614)\n",
            "resized shapes are  600 800\n",
            "corners  [(404, 175), (451, 175), (451, 230), (404, 230)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(1080., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (463, 458) 2394\n",
            "image after crop is  (463, 458)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (200, 605) 1650\n",
            "image after crop is  (200, 605)\n",
            "resized shapes are  600 800\n",
            "corners  [(267, 174), (490, 174), (490, 298), (267, 298)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(29920., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 611) 3016\n",
            "image after crop is  (362, 611)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (304, 477) 863\n",
            "image after crop is  (304, 477)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (457, 519) 2111\n",
            "image after crop is  (457, 519)\n",
            "resized shapes are  600 800\n",
            "corners  [(286, 179), (610, 179), (610, 414), (286, 414)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(67408., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  175\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(3.0258, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6956, device='cuda:0'), 'loss_box_reg': tensor(0.0022, device='cuda:0'), 'loss_objectness': tensor(0.6933, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.6348, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  176\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.6075, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6945, device='cuda:0'), 'loss_box_reg': tensor(0.0011, device='cuda:0'), 'loss_objectness': tensor(0.6888, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2232, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (364, 706) 4502\n",
            "image after crop is  (364, 706)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (457, 582) 3671\n",
            "image after crop is  (457, 582)\n",
            "resized shapes are  600 800\n",
            "corners  [(256, 149), (656, 149), (656, 457), (256, 457)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(119387., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (337, 725) 6959\n",
            "image after crop is  (337, 725)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (424, 606) 4105\n",
            "image after crop is  (424, 606)\n",
            "resized shapes are  600 800\n",
            "corners  [(160, 153), (582, 153), (582, 365), (160, 365)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(89880., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (459, 572) 6798\n",
            "image after crop is  (459, 572)\n",
            "resized shapes are  600 800\n",
            "corners  [(345, 254), (493, 254), (493, 373), (345, 373)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(15372., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (422, 612) 3903\n",
            "image after crop is  (422, 612)\n",
            "resized shapes are  600 800\n",
            "corners  [(257, 180), (456, 180), (456, 338), (257, 338)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(31590., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (459, 572) 37\n",
            "about to resize image. shape going in is @ (425, 606) 4406\n",
            "image after crop is  (459, 572)\n",
            "image after crop is  (425, 606)\n",
            "resized shapes are  600 800\n",
            "resized shapes are  600 800\n",
            "corners  [(217, 153), (644, 153), (644, 349), (217, 349)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(83692., dtype=torch.float64)\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (458, 506) 2126\n",
            "image after crop is  (458, 506)\n",
            "resized shapes are  600 800\n",
            "corners  [(265, 207), (641, 207), (641, 449), (265, 449)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(79352., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  177\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5385, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6959, device='cuda:0'), 'loss_box_reg': tensor(0.0021, device='cuda:0'), 'loss_objectness': tensor(0.6900, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1505, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  178\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.5306, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6961, device='cuda:0'), 'loss_box_reg': tensor(0.0016, device='cuda:0'), 'loss_objectness': tensor(0.6913, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.1416, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (475, 573) 3754\n",
            "image after crop is  (475, 573)\n",
            "resized shapes are  600 800\n",
            "image shape read in is  (600, 800, 3)\n",
            "corners  [(256, 149), (656, 149), (656, 457), (256, 457)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "about to resize image. shape going in is @ (268, 607) 1063\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "image after crop is  (268, 607)\n",
            "area is  tensor(118916., dtype=torch.float64)\n",
            "resized shapes are  600 800\n",
            "corners  [(395, 217), (631, 217), (631, 359), (395, 359)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(35432., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 601) 6383\n",
            "image after crop is  (362, 601)\n",
            "resized shapes are  600 800\n",
            "corners  [(411, 260), (530, 260), (530, 339), (411, 339)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(9605., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (421, 605) 5738\n",
            "image after crop is  (421, 605)\n",
            "resized shapes are  600 800\n",
            "corners  [(358, 155), (525, 155), (525, 295), (358, 295)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(23490., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (464, 517) 7188\n",
            "image after crop is  (464, 517)\n",
            "resized shapes are  600 800\n",
            "corners  [(184, 129), (654, 129), (654, 429), (184, 429)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "image shape read in is  (600, 800, 3)\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "about to resize image. shape going in is @ (305, 612) 3377\n",
            "image after crop is  (305, 612)\n",
            "area is  tensor(126616., dtype=torch.float64)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (327, 607) 5358\n",
            "image after crop is  (327, 607)\n",
            "resized shapes are  600 800\n",
            "corners  [(373, 202), (540, 202), (540, 281), (373, 281)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(15029., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (324, 613) 1600\n",
            "image after crop is  (324, 613)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (324, 615) 5500\n",
            "image after crop is  (324, 615)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (423, 609) 4277\n",
            "image after crop is  (423, 609)\n",
            "resized shapes are  600 800\n",
            "corners  [(160, 152), (672, 152), (672, 364), (160, 364)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(109728., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (369, 612) 1626\n",
            "image after crop is  (369, 612)\n",
            "resized shapes are  600 800\n",
            "corners  [(253, 169), (477, 169), (477, 293), (253, 293)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(29915., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  179\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5818, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6954, device='cuda:0'), 'loss_box_reg': tensor(0.0017, device='cuda:0'), 'loss_objectness': tensor(0.6902, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1945, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  180\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.5911, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6960, device='cuda:0'), 'loss_box_reg': tensor(0.0020, device='cuda:0'), 'loss_objectness': tensor(0.6923, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2008, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (458, 579) 88\n",
            "image after crop is  (458, 579)\n",
            "resized shapes are  600 800\n",
            "corners  [(311, 179), (657, 179), (657, 393), (311, 393)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(68796., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (461, 579) 6535\n",
            "image after crop is  (461, 579)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "setting target\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (381, 678) 7637\n",
            "image after crop is  (381, 678)\n",
            "resized shapes are  600 800\n",
            "corners  [(247, 256), (458, 256), (458, 423), (247, 423)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(24696., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (460, 453) 2412\n",
            "image after crop is  (460, 453)\n",
            "resized shapes are  600 800\n",
            "corners  [(197, 125), (655, 125), (655, 435), (197, 435)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(119900., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (420, 590) 6122\n",
            "image after crop is  (420, 590)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (462, 578) 3806\n",
            "image after crop is  (462, 578)\n",
            "resized shapes are  600 800\n",
            "corners  [(193, 114), (688, 114), (688, 458), (193, 458)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(164208., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (423, 606) 4370\n",
            "image after crop is  (423, 606)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (297, 607) 2569\n",
            "image after crop is  (297, 607)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (297, 602) 1306\n",
            "image after crop is  (297, 602)\n",
            "resized shapes are  600 800\n",
            "------------------------------------dataload loop is  181\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "corners  [(303, 199), (611, 199), (611, 370), (303, 370)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(57304., dtype=torch.float64)\n",
            "*** SUMMED losses is  tensor(1.5650, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6951, device='cuda:0'), 'loss_box_reg': tensor(0.0021, device='cuda:0'), 'loss_objectness': tensor(0.6893, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1785, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  182\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.6256, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6957, device='cuda:0'), 'loss_box_reg': tensor(0.0026, device='cuda:0'), 'loss_objectness': tensor(0.6911, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.2362, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (422, 606) 3911\n",
            "image after crop is  (422, 606)\n",
            "resized shapes are  600 800\n",
            "corners  [(257, 180), (456, 180), (456, 338), (257, 338)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(31590., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (279, 612) 975\n",
            "image after crop is  (279, 612)\n",
            "resized shapes are  600 800\n",
            "corners  [(397, 230), (556, 230), (556, 338), (397, 338)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(16182., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (379, 678) 7706\n",
            "image after crop is  (379, 678)\n",
            "resized shapes are  600 800\n",
            "corners  [(268, 256), (469, 256), (469, 400), (268, 400)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(19856., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (304, 718) 7585\n",
            "image after crop is  (304, 718)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (363, 608) 3036\n",
            "image after crop is  (363, 608)\n",
            "resized shapes are  600 800\n",
            "corners  [(339, 183), (487, 183), (487, 300), (339, 300)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(17214., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (361, 610) 2953\n",
            "image after crop is  (361, 610)\n",
            "resized shapes are  600 800\n",
            "corners  [(375, 189), (568, 189), (568, 344), (375, 344)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(30020., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (420, 716) 695\n",
            "image after crop is  (420, 716)\n",
            "resized shapes are  600 800\n",
            "corners  [(171, 132), (484, 132), (484, 325), (171, 325)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(61800., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  183\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(1.5870, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6959, device='cuda:0'), 'loss_box_reg': tensor(0.0021, device='cuda:0'), 'loss_objectness': tensor(0.6891, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1998, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "------------------------------------dataload loop is  184\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n",
            "*** SUMMED losses is  tensor(2.5398, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.6951, device='cuda:0'), 'loss_box_reg': tensor(0.0017, device='cuda:0'), 'loss_objectness': tensor(0.6908, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.1522, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (458, 518) 7272\n",
            "image after crop is  (458, 518)\n",
            "resized shapes are  600 800\n",
            "corners  [(184, 129), (654, 129), (654, 429), (184, 429)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(127489., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (365, 600) 1802\n",
            "image after crop is  (365, 600)\n",
            "resized shapes are  600 800\n",
            "corners  [(422, 169), (633, 169), (633, 332), (422, 332)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(34905., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (451, 610) 5232\n",
            "image after crop is  (451, 610)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (363, 614) 409\n",
            "image after crop is  (363, 614)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (233, 585) 2729\n",
            "image after crop is  (233, 585)\n",
            "resized shapes are  600 800\n",
            "corners  [(269, 198), (580, 198), (580, 360), (269, 360)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(54940., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (301, 546) 2795\n",
            "image after crop is  (301, 546)\n",
            "resized shapes are  600 800\n",
            "corners  [(288, 196), (563, 196), (563, 352), (288, 352)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(42658., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (422, 608) 6416\n",
            "image after crop is  (422, 608)\n",
            "resized shapes are  600 800\n",
            "corners  [(239, 196), (633, 196), (633, 411), (239, 411)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(85410., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (364, 728) 2011\n",
            "image after crop is  (364, 728)\n",
            "resized shapes are  600 800\n",
            "corners  [(401, 174), (509, 174), (509, 234), (401, 234)]\n",
            "found a negative box!!  [[397, 21, 505, 81]]\n",
            "skip points:  [153, 517, 4, 732]\n",
            "offset col, row  397 21\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(-8569., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (333, 613) 1741\n",
            "image after crop is  (333, 613)\n",
            "resized shapes are  600 800\n",
            "corners  [(202, 163), (456, 163), (456, 310), (202, 310)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(39894., dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK4AdvJxevK6",
        "outputId": "ac927892-ce7a-4713-d995-00602f527d1c"
      },
      "source": [
        "for p in modelr.roi_heads.box_head.parameters():\n",
        "    print(p)\n",
        "    #p.requires_grad = False"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 1.8585e-03, -5.4933e-04, -2.8207e-03,  ...,  1.5852e-03,\n",
            "          5.6929e-04, -6.5263e-05],\n",
            "        [-2.5673e-03,  2.2570e-04,  1.4173e-03,  ...,  1.7588e-03,\n",
            "         -1.7610e-03, -5.8144e-04],\n",
            "        [-2.9732e-03, -5.0275e-04, -3.6420e-04,  ..., -1.9960e-03,\n",
            "          1.3166e-03, -3.3239e-04],\n",
            "        ...,\n",
            "        [ 1.4313e-03, -2.6857e-03,  1.1164e-03,  ..., -1.3285e-03,\n",
            "          2.2267e-03, -2.4887e-03],\n",
            "        [ 6.3599e-04,  1.0612e-03,  1.3666e-03,  ..., -7.1515e-05,\n",
            "          2.0967e-03, -1.8350e-03],\n",
            "        [ 2.9938e-03,  2.4329e-03, -1.8795e-03,  ..., -2.1472e-03,\n",
            "          1.3312e-03,  1.4403e-03]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([-0.0003,  0.0020,  0.0028,  ...,  0.0013, -0.0002, -0.0008],\n",
            "       device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([[ 3.6921e-03,  7.8220e-03,  3.0758e-02,  ...,  1.4931e-02,\n",
            "          2.2406e-02,  8.1859e-03],\n",
            "        [-1.5936e-02, -1.4017e-02,  2.2420e-02,  ..., -2.9720e-02,\n",
            "         -1.8099e-02, -1.4475e-02],\n",
            "        [ 1.0764e-02, -1.6776e-03,  2.6901e-02,  ..., -2.0590e-02,\n",
            "         -1.1772e-05,  2.7697e-02],\n",
            "        ...,\n",
            "        [-8.8865e-04,  1.4193e-02,  2.9705e-02,  ..., -1.3783e-02,\n",
            "         -2.6981e-02, -1.7874e-03],\n",
            "        [ 1.0329e-02, -2.8640e-03,  2.1847e-04,  ...,  2.1277e-02,\n",
            "         -9.0455e-03,  2.9815e-02],\n",
            "        [ 3.1141e-02, -1.1081e-02, -5.5793e-03,  ..., -3.7925e-03,\n",
            "          6.5917e-03, -1.1867e-02]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([ 0.0062, -0.0301, -0.0281,  ..., -0.0232, -0.0258, -0.0132],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEGA08hhzaCS",
        "outputId": "c241201f-a8ce-4bf2-fc93-d67dbb04d364"
      },
      "source": [
        "summary(modelr)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "FasterRCNN                                    --\n",
              "├─GeneralizedRCNNTransform: 1-1               --\n",
              "├─Sequential: 1-2                             --\n",
              "│    └─Conv2d: 2-1                            (9,408)\n",
              "│    └─BatchNorm2d: 2-2                       (128)\n",
              "│    └─ReLU: 2-3                              --\n",
              "│    └─MaxPool2d: 2-4                         --\n",
              "│    └─Sequential: 2-5                        --\n",
              "│    │    └─Bottleneck: 3-1                   (75,008)\n",
              "│    │    └─Bottleneck: 3-2                   (70,400)\n",
              "│    │    └─Bottleneck: 3-3                   (70,400)\n",
              "│    └─Sequential: 2-6                        --\n",
              "│    │    └─Bottleneck: 3-4                   (379,392)\n",
              "│    │    └─Bottleneck: 3-5                   (280,064)\n",
              "│    │    └─Bottleneck: 3-6                   (280,064)\n",
              "│    │    └─Bottleneck: 3-7                   (280,064)\n",
              "│    └─Sequential: 2-7                        --\n",
              "│    │    └─Bottleneck: 3-8                   (1,512,448)\n",
              "│    │    └─Bottleneck: 3-9                   (1,117,184)\n",
              "│    │    └─Bottleneck: 3-10                  (1,117,184)\n",
              "│    │    └─Bottleneck: 3-11                  (1,117,184)\n",
              "│    │    └─Bottleneck: 3-12                  (1,117,184)\n",
              "│    │    └─Bottleneck: 3-13                  (1,117,184)\n",
              "│    └─Sequential: 2-8                        --\n",
              "│    │    └─Bottleneck: 3-14                  (6,039,552)\n",
              "│    │    └─Bottleneck: 3-15                  (4,462,592)\n",
              "│    │    └─Bottleneck: 3-16                  (4,462,592)\n",
              "│    └─AdaptiveAvgPool2d: 2-9                 --\n",
              "├─RegionProposalNetwork: 1-3                  --\n",
              "│    └─AnchorGenerator: 2-10                  --\n",
              "│    └─RPNHead: 2-11                          --\n",
              "│    │    └─Conv2d: 3-17                      37,750,784\n",
              "│    │    └─Conv2d: 3-18                      30,735\n",
              "│    │    └─Conv2d: 3-19                      122,940\n",
              "├─RoIHeads: 1-4                               --\n",
              "│    └─MultiScaleRoIAlign: 2-12               --\n",
              "│    └─TwoMLPHead: 2-13                       --\n",
              "│    │    └─Linear: 3-20                      (102,761,472)\n",
              "│    │    └─Linear: 3-21                      (1,049,600)\n",
              "│    └─FastRCNNPredictor: 2-14                --\n",
              "│    │    └─Linear: 3-22                      (2,050)\n",
              "│    │    └─Linear: 3-23                      (8,200)\n",
              "├─Sequential: 1-5                             --\n",
              "│    └─Linear: 2-15                           524,544\n",
              "│    └─ReLU: 2-16                             --\n",
              "│    └─Dropout: 2-17                          --\n",
              "│    └─Linear: 2-18                           514\n",
              "│    └─LogSoftmax: 2-19                       --\n",
              "======================================================================\n",
              "Total params: 165,758,871\n",
              "Trainable params: 38,429,517\n",
              "Non-trainable params: 127,329,354\n",
              "======================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4IswwLpjunl",
        "outputId": "aaf58a88-5ab1-409e-d76b-9bee5c4585fb"
      },
      "source": [
        "for p in modelr.rpn.parameters():\n",
        "    print(p)\n",
        "    p.requires_grad = True"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[-3.8021e-03,  7.1899e-03, -5.9421e-03],\n",
            "          [-6.9943e-03, -4.4166e-03, -1.2117e-02],\n",
            "          [-6.2612e-03, -8.6908e-03,  1.4711e-02]],\n",
            "\n",
            "         [[ 2.5024e-02,  1.6703e-02,  2.8196e-04],\n",
            "          [-4.4547e-05,  6.0470e-03, -8.3912e-03],\n",
            "          [ 1.4632e-02, -2.7523e-03, -1.4353e-02]],\n",
            "\n",
            "         [[ 8.4682e-03,  9.8392e-03,  1.0757e-03],\n",
            "          [-6.8948e-03,  2.2619e-03, -1.0033e-02],\n",
            "          [ 1.1456e-02,  1.2935e-02,  6.0327e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.8044e-02,  1.0413e-02,  2.4839e-02],\n",
            "          [-5.2523e-03, -2.9470e-02,  2.6697e-02],\n",
            "          [ 3.6123e-03, -3.5279e-03, -9.3954e-03]],\n",
            "\n",
            "         [[-4.7678e-03, -1.0986e-02,  3.0743e-03],\n",
            "          [ 3.4524e-03,  5.9410e-03, -1.4388e-03],\n",
            "          [-4.3474e-03, -1.8733e-03, -2.4490e-02]],\n",
            "\n",
            "         [[-9.1903e-03, -6.2867e-03,  2.8602e-03],\n",
            "          [ 9.3687e-03, -3.4566e-03, -2.0187e-03],\n",
            "          [ 1.1140e-02, -9.4412e-03, -1.6602e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.0695e-02,  2.6119e-02, -8.8205e-03],\n",
            "          [ 6.7792e-04, -1.0379e-03, -2.6817e-02],\n",
            "          [-1.7274e-03,  1.5443e-02, -1.3398e-02]],\n",
            "\n",
            "         [[-1.0925e-02, -2.5922e-04, -3.3311e-03],\n",
            "          [ 4.4843e-03, -1.0147e-02,  4.0186e-03],\n",
            "          [-6.4802e-03,  1.7116e-02,  3.1548e-03]],\n",
            "\n",
            "         [[-9.3457e-03, -2.8736e-03, -2.0583e-02],\n",
            "          [ 1.0636e-02,  1.0362e-02, -1.0634e-02],\n",
            "          [-1.9631e-03, -4.4194e-03,  1.0636e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.5718e-03,  1.1173e-02, -7.2655e-03],\n",
            "          [ 3.7863e-04, -1.7021e-02, -1.8852e-03],\n",
            "          [ 5.1861e-03, -4.0075e-03, -1.7816e-02]],\n",
            "\n",
            "         [[-2.8403e-03,  1.3747e-02, -1.7969e-03],\n",
            "          [ 5.5332e-03,  2.1090e-02,  3.6205e-03],\n",
            "          [-1.8413e-02, -1.2884e-02,  1.3463e-02]],\n",
            "\n",
            "         [[-6.3986e-03, -2.2427e-03,  2.5112e-03],\n",
            "          [ 1.8630e-02, -1.8542e-02,  1.5274e-03],\n",
            "          [ 7.4183e-03,  1.6845e-02,  4.0683e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 3.3481e-03,  1.1043e-02, -6.4703e-03],\n",
            "          [-4.4461e-04, -6.0162e-03,  5.1857e-03],\n",
            "          [-6.8855e-03, -1.5512e-02, -6.2535e-03]],\n",
            "\n",
            "         [[ 5.7079e-03, -9.9228e-03, -4.7147e-03],\n",
            "          [-6.8242e-03,  5.9054e-03,  1.6216e-02],\n",
            "          [-2.8788e-03, -2.6355e-04, -8.1482e-03]],\n",
            "\n",
            "         [[ 6.5415e-03, -1.9382e-03, -8.4907e-03],\n",
            "          [ 6.8846e-03,  1.1755e-02,  3.2767e-03],\n",
            "          [ 1.2690e-02,  3.2425e-04, -3.8971e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.8789e-04,  1.0069e-02,  4.5282e-03],\n",
            "          [-4.9752e-03,  5.9966e-03, -5.9728e-03],\n",
            "          [ 9.9498e-03,  2.1124e-03, -4.4024e-04]],\n",
            "\n",
            "         [[ 1.1211e-02,  1.1237e-02,  1.4049e-02],\n",
            "          [-2.6997e-03,  1.7132e-03,  1.7379e-02],\n",
            "          [ 9.1268e-03, -5.8526e-03, -6.2839e-03]],\n",
            "\n",
            "         [[-5.1512e-03,  1.4398e-03, -1.6439e-02],\n",
            "          [-1.3074e-02, -3.5459e-03,  1.1804e-02],\n",
            "          [ 6.9580e-03,  5.5944e-03,  5.3369e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-6.0113e-03,  8.9607e-03,  6.8529e-03],\n",
            "          [ 1.2149e-02, -1.3165e-02,  8.8103e-03],\n",
            "          [-2.8688e-03,  1.1030e-02, -3.5375e-03]],\n",
            "\n",
            "         [[ 1.4981e-02, -1.3490e-02, -1.3314e-02],\n",
            "          [-9.8053e-04, -3.4236e-03,  1.0715e-02],\n",
            "          [ 4.8971e-03, -2.5439e-03, -1.1014e-02]],\n",
            "\n",
            "         [[ 4.0240e-03, -3.2274e-03,  4.4786e-03],\n",
            "          [ 3.7819e-04,  1.1012e-02, -7.4634e-03],\n",
            "          [ 1.1517e-02, -1.3909e-02, -5.5629e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.1329e-03, -6.9890e-03,  3.2027e-03],\n",
            "          [ 1.4354e-02,  5.4489e-03,  3.9830e-03],\n",
            "          [-1.1266e-02, -8.2129e-03, -7.2372e-03]],\n",
            "\n",
            "         [[ 6.9885e-03, -4.9445e-03,  5.6331e-03],\n",
            "          [ 2.4105e-02,  9.3180e-04, -1.3196e-02],\n",
            "          [ 8.1290e-03, -5.2450e-03, -4.0407e-03]],\n",
            "\n",
            "         [[ 7.1159e-03,  1.8593e-02,  8.5766e-03],\n",
            "          [ 7.2579e-03,  1.8544e-03, -6.9934e-03],\n",
            "          [ 2.0190e-02, -1.6754e-02,  1.1500e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.0173e-03,  9.1394e-03,  7.8653e-03],\n",
            "          [-5.1184e-03, -2.1508e-03,  1.1214e-02],\n",
            "          [ 4.7333e-03, -5.1844e-03, -1.5831e-02]],\n",
            "\n",
            "         [[-5.8798e-03,  5.0504e-03,  2.4465e-03],\n",
            "          [ 1.1036e-02,  1.2983e-02,  4.7562e-03],\n",
            "          [ 8.3367e-03,  5.3811e-04, -3.0521e-03]],\n",
            "\n",
            "         [[ 1.3726e-02, -2.0040e-03, -8.6189e-03],\n",
            "          [-6.0912e-04, -5.5277e-03,  8.7976e-03],\n",
            "          [-2.3270e-02,  5.1987e-03, -2.4154e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.9756e-02, -1.3075e-02, -1.9085e-02],\n",
            "          [ 1.3305e-02,  1.1220e-02,  1.4429e-03],\n",
            "          [-7.8090e-03, -1.2954e-03, -8.4147e-03]],\n",
            "\n",
            "         [[ 3.1625e-03,  6.0588e-03,  2.4668e-02],\n",
            "          [ 2.0805e-02, -5.5673e-03, -2.0967e-03],\n",
            "          [ 2.5620e-03,  4.1404e-03, -3.8291e-03]],\n",
            "\n",
            "         [[-6.1169e-03,  1.4716e-02,  1.4289e-03],\n",
            "          [-4.4884e-03,  2.5475e-03, -1.3179e-02],\n",
            "          [ 1.2604e-02,  1.9564e-03,  6.6195e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.7973e-03, -1.5016e-03, -5.2371e-03],\n",
            "          [-1.7687e-02, -6.1916e-03,  9.3960e-03],\n",
            "          [-1.1603e-02,  1.4017e-03,  1.6261e-02]],\n",
            "\n",
            "         [[ 1.2673e-02,  2.4919e-03,  2.0493e-02],\n",
            "          [ 5.4783e-03,  1.8914e-02,  2.1146e-02],\n",
            "          [-2.4550e-02, -7.1218e-04, -1.7812e-03]],\n",
            "\n",
            "         [[-8.8763e-03, -3.5410e-04, -2.2023e-03],\n",
            "          [ 5.8146e-03, -1.8965e-02,  3.4640e-03],\n",
            "          [-1.0211e-02,  4.4290e-03, -1.0115e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.3362e-03,  4.4448e-03, -9.4782e-03],\n",
            "          [-4.8316e-03,  4.2133e-03,  2.8692e-03],\n",
            "          [-3.2179e-04, -2.1327e-02,  3.2437e-03]],\n",
            "\n",
            "         [[ 1.6467e-02,  2.1606e-02,  2.8504e-03],\n",
            "          [ 7.8541e-03,  1.5740e-03, -1.2150e-02],\n",
            "          [ 7.0038e-03,  7.7396e-03,  1.5404e-02]],\n",
            "\n",
            "         [[-1.3885e-02,  1.8684e-03,  6.3874e-03],\n",
            "          [-8.6063e-03, -1.8304e-02, -2.8362e-03],\n",
            "          [-3.7584e-03, -7.1184e-03,  1.2637e-03]]]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0127]],\n",
            "\n",
            "         [[-0.0005]],\n",
            "\n",
            "         [[ 0.0161]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0028]],\n",
            "\n",
            "         [[ 0.0067]],\n",
            "\n",
            "         [[-0.0028]]],\n",
            "\n",
            "\n",
            "        [[[-0.0021]],\n",
            "\n",
            "         [[ 0.0034]],\n",
            "\n",
            "         [[-0.0007]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0332]],\n",
            "\n",
            "         [[ 0.0190]],\n",
            "\n",
            "         [[ 0.0036]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0132]],\n",
            "\n",
            "         [[ 0.0080]],\n",
            "\n",
            "         [[-0.0035]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0122]],\n",
            "\n",
            "         [[-0.0013]],\n",
            "\n",
            "         [[-0.0101]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0081]],\n",
            "\n",
            "         [[ 0.0010]],\n",
            "\n",
            "         [[ 0.0032]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0115]],\n",
            "\n",
            "         [[ 0.0140]],\n",
            "\n",
            "         [[-0.0157]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0045]],\n",
            "\n",
            "         [[ 0.0195]],\n",
            "\n",
            "         [[ 0.0065]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0128]],\n",
            "\n",
            "         [[-0.0135]],\n",
            "\n",
            "         [[ 0.0088]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0011]],\n",
            "\n",
            "         [[ 0.0051]],\n",
            "\n",
            "         [[-0.0088]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0119]],\n",
            "\n",
            "         [[-0.0169]],\n",
            "\n",
            "         [[-0.0103]]]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([[[[ 2.7258e-03]],\n",
            "\n",
            "         [[-1.2079e-02]],\n",
            "\n",
            "         [[-3.9095e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.1032e-03]],\n",
            "\n",
            "         [[ 1.2058e-02]],\n",
            "\n",
            "         [[ 1.0723e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.1781e-03]],\n",
            "\n",
            "         [[ 7.2195e-03]],\n",
            "\n",
            "         [[ 6.6695e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5353e-03]],\n",
            "\n",
            "         [[-9.1231e-03]],\n",
            "\n",
            "         [[ 8.4627e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.0628e-05]],\n",
            "\n",
            "         [[ 2.5343e-03]],\n",
            "\n",
            "         [[ 8.5559e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0902e-03]],\n",
            "\n",
            "         [[ 8.9656e-04]],\n",
            "\n",
            "         [[-2.0643e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-8.2930e-03]],\n",
            "\n",
            "         [[ 1.5981e-04]],\n",
            "\n",
            "         [[-1.7956e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.4241e-02]],\n",
            "\n",
            "         [[ 2.7728e-02]],\n",
            "\n",
            "         [[-3.5733e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 9.3530e-04]],\n",
            "\n",
            "         [[-9.6034e-03]],\n",
            "\n",
            "         [[-4.4944e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.6315e-02]],\n",
            "\n",
            "         [[ 1.3490e-03]],\n",
            "\n",
            "         [[ 2.2873e-02]]],\n",
            "\n",
            "\n",
            "        [[[-9.2457e-03]],\n",
            "\n",
            "         [[ 4.9945e-03]],\n",
            "\n",
            "         [[ 1.1361e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3597e-03]],\n",
            "\n",
            "         [[ 1.5680e-02]],\n",
            "\n",
            "         [[-7.7430e-03]]]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTEVY6GtocP2",
        "outputId": "f47403c6-38f7-4099-c3d3-6c50c3d929da"
      },
      "source": [
        "loss_dict"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss_classifier': tensor(0.7129, device='cuda:0'),\n",
              " 'loss_box_reg': tensor(0.0091, device='cuda:0'),\n",
              " 'loss_objectness': tensor(0.6918, device='cuda:0',\n",
              "        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
              " 'loss_rpn_box_reg': tensor(0.0903, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoHox6spRHU7"
      },
      "source": [
        "#print(summary(modelr,(3,244,244), batch_size = 5))\n",
        "#print(modelr)\n",
        "len(targets)\n",
        "tsave  = targets\n",
        "\n",
        "print(targets[0].keys())\n",
        "target_new = []\n",
        "for ii in range(0,5):\n",
        "    d={}\n",
        "    d['boxes'] = targets[0]['boxes'][ii]\n",
        "    d['labels'] = targets[0]['labels'][ii]\n",
        "    d['image_id'] = targets[0]['image_id'][ii]\n",
        "    d['area'] = targets[0]['area'][ii]\n",
        "    target_new.append(d)\n",
        "\n",
        "len(target_new)\n",
        "target_new[4]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwQAjjiSffsI"
      },
      "source": [
        "print(loss_dict)\n",
        "ll=nn.CrossEntropyLoss(loss_dict['loss_classifier'])\n",
        "print(ll)\n",
        "ll.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKoAfTd5cKjR"
      },
      "source": [
        "summary(modelr)\n",
        "fname = full_file_list[1419]\n",
        "print(fname)\n",
        "img_data = image.imread(fname) #'/content/gdrive/Shareddrives/BreastUS/Annotated data/unzipped_updated/1_srh8gfhj_a_0629s3fh_0.mp4-2021_07_15_17_21_15-labelme 3.0.zip/default/frame_000030.PNG')\n",
        "plt.figure()\n",
        "plt.imshow(img_data[:,:,0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Afis5F7nwH7"
      },
      "source": [
        "idx = targets['image_id']\n",
        "print(targets['boxes'])\n",
        "full_file_list[idx[0]]\n",
        "len(bounding_box)\n",
        "print(len(full_file_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trATj4Qg6yHc"
      },
      "source": [
        "idx=targets['image_id']\n",
        "print(idx)\n",
        "\n",
        "for fcount,ival in enumerate(idx): #imagelist):\n",
        "    print(full_file_list[ival])\n",
        "    idata = images[fcount]\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.imshow(idata[0,:,:].cpu(),cmap='gray')\n",
        "\n",
        "    cancer_status = targets['labels'][fcount]\n",
        "    if (cancer_status == 0):\n",
        "        clabel = 'Benign'\n",
        "    else:\n",
        "        clabel = 'Malignant'\n",
        "    findex=full_file_list[ival].find('updated/')\n",
        "    sname = full_file_list[ival][findex+8:]\n",
        "    findex = sname.find('/')\n",
        "    sname = sname[:findex] +'_' + str(int(ival)) + '_' + clabel\n",
        "    plt.title(sname)\n",
        "\n",
        "\n",
        "\n",
        "    for counter,blist in enumerate(range(0,len(out_custom[fcount]['boxes']))): #out['boxes']):\n",
        "        for ii in range(0,blist):\n",
        "            points = out_custom[fcount]['boxes'][ii]\n",
        "\n",
        "            #ii = out[counter]['boxes'][0]\n",
        "            newbox = [torch.detach(points[0]),torch.detach(points[1]),torch.detach(points[2]),torch.detach(points[3])]\n",
        "            #idata = imagelist[counter]\n",
        "\n",
        "            rect = patches.Rectangle((np.uint(newbox[0].cpu()),\n",
        "                                    np.uint(newbox[1].cpu())),\n",
        "                                    np.uint(newbox[2].cpu())-(np.uint(newbox[0].cpu())),\n",
        "                                    np.uint(newbox[3].cpu())-(np.uint(newbox[1].cpu())),\n",
        "                                    linewidth=0.8,\n",
        "                                    edgecolor='r',\n",
        "                                    facecolor='none')\n",
        "\n",
        "\n",
        "\n",
        "                # Get the current reference\n",
        "            ax = plt.gca()\n",
        "                # Add the patch to the Axes\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "\n",
        "    #add annotation box\n",
        "    points = targets['boxes'][fcount]\n",
        "    print('points are ', points)\n",
        "    rect = patches.Rectangle((np.uint(points[0][0].cpu()),\n",
        "                    np.uint(points[0][1].cpu())),\n",
        "                    np.uint(points[0][2].cpu())-(np.uint(points[0][0].cpu())),\n",
        "                    np.uint(points[0][3].cpu())-(np.uint(points[0][1].cpu())),\n",
        "                    linewidth=0.8,\n",
        "                    edgecolor='g',\n",
        "                    facecolor='none')\n",
        "    # Get the current reference\n",
        "    ax = plt.gca()\n",
        "        # Add the patch to the Axes\n",
        "    ax.add_patch(rect)\n",
        "    #add annotation box\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbFL--9jDzoL"
      },
      "source": [
        "targets['labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dH9NaLGRXNg"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "#del resnet50\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "#from torchsummary import summary\n",
        "!pip3 install -q torchinfo\n",
        "\n",
        "from torchinfo import summary as s2 #works with lists of tensors\n",
        "\n",
        "# load a model pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 2  # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "\n",
        "if (train_on_gpu ==1):\n",
        "    model=model.to(dev)\n",
        "else:\n",
        "    pass\n",
        "resnet50= model #.roi_heads.box_predictor\n",
        "resnet50.eval()\n",
        "#summary(resnet50,[(3, 600, 600)])\n",
        "print(model)\n",
        "#summary(resnet50,(3,800,800))\n",
        "a=sum([param.nelement() for param in model.parameters()])\n",
        "print(a)\n",
        "print(model.rpn)\n",
        "\n",
        "bb=10\n",
        "s2(model,input_size=(bb, 3, 800, 800))\n",
        "del resnet50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oySb7-V-R7H"
      },
      "source": [
        "in_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSD6PjUhsIj1"
      },
      "source": [
        "### -------------------DEBUG NEW MODEL TYPES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_Zc70YmRZl3"
      },
      "source": [
        "%pdb off\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "#el model\n",
        "\n",
        "#del backbone\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# load a pre-trained model for classification and return\n",
        "# only the features\n",
        "#backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "#backbone.out_channels = 1280\n",
        "%pdb off\n",
        "backbone = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
        "backbone.out_channels = 512\n",
        "\n",
        "#backbone = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "#backbone.out_channels = 1280\n",
        "#anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "anchor_generator = AnchorGenerator(sizes=((8, 16, 32, 64, 128),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# let's define what are the feature maps that we will\n",
        "# use to perform the region of interest cropping, as well as\n",
        "# the size of the crop after rescaling.\n",
        "# if your backbone returns a Tensor, featmap_names is expected to\n",
        "# be [0]. More generally, the backbone should return an\n",
        "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
        "# feature maps to use.\n",
        "''' mobilenet\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "'''\n",
        "\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(['feat1', 'feat3'], 3, 2)\n",
        "from collections import OrderedDict\n",
        "i = OrderedDict()\n",
        "i['feat1'] = torch.rand(1, 5, 64, 64)\n",
        "i['feat2'] = torch.rand(1, 5, 32, 32)  # this feature won't be used in the pooling\n",
        "i['feat3'] = torch.rand(1, 5, 16, 16)\n",
        "# create some random bounding boxes\n",
        "boxes = torch.rand(6, 4) * 256; boxes[:, 2:] += boxes[:, :2]\n",
        "# original image size, before computing the feature maps\n",
        "image_sizes = [(512, 512)]\n",
        "output = roi_pooler(i, [boxes], image_sizes)\n",
        "print(output.shape)\n",
        "\n",
        "\n",
        "\n",
        "# put the pieces together inside a FasterRCNN model\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=2,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "if (train_on_gpu ==1):\n",
        "    model=model.to(dev)\n",
        "else:\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "# For training\n",
        "images= torch.rand(1, 3, 600, 1200) \n",
        "boxes= torch.Tensor([[[10,10,100,100]]])  #torch.rand(1, 11,4)*100 #11, 4)\n",
        "\n",
        "labels = torch.randint(1, 91, (4, 11))\n",
        "#images = list(image for image in images)\n",
        "\n",
        "### Moving image data to the device\n",
        "imagelist=[]\n",
        "for ii in range(0,len(images)):\n",
        "    #imagelist.append(images[ii])\n",
        "    if (train_on_gpu):\n",
        "        imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32).to(dev))\n",
        "    else:\n",
        "        imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "targets = []\n",
        "for i in range(len(images)):\n",
        "    d = {}\n",
        "    d['boxes'] = boxes[i]\n",
        "    d['labels'] = labels[i]\n",
        "    targets.append(d)\n",
        "\n",
        "output = model(imagelist) #, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QX330WaZBLg"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer,\n",
        "                                       step_size=7,\n",
        "                                       gamma=0.1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#training_data set above by calling Custom Dataset\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(training_data,\n",
        "                                            batch_size=10,\n",
        "                                            shuffle=False, #True,\n",
        "                                            num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "images,targets = next(iter(data_loader))\n",
        "#images = list(image for image in images)\n",
        "\n",
        "imagelist=[]\n",
        "for ii in range(0,len(images)):\n",
        "    #imagelist.append(images[ii])\n",
        "    if (train_on_gpu):\n",
        "        imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32).to(dev))\n",
        "    else:\n",
        "        imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32))\n",
        "print(np.shape(images), len(images))\n",
        "print(np.shape(imagelist[0]))\n",
        "\n",
        "tdata=[]\n",
        "ddata={}\n",
        "\n",
        "import tensorflow as tf\n",
        "for ii in range(0,len(targets['boxes'])):\n",
        "    if (train_on_gpu):\n",
        "        ddata['boxes'] = targets['boxes'][ii].to(dev) #torch.DoubleTensor(targets['boxes'][ii])\n",
        "\n",
        "        ddata['labels']=targets['labels'][ii].to(dev)\n",
        "        ddata['area'] = targets['area'][ii].to(dev)\n",
        "        tdata.append(ddata)\n",
        "    else:\n",
        "        ddata['boxes'] = targets['boxes'][ii] #torch.DoubleTensor(targets['boxes'][ii])\n",
        "\n",
        "        ddata['labels']=targets['labels'][ii]\n",
        "        ddata['area'] = targets['area'][ii]\n",
        "        tdata.append(ddata)\n",
        "\n",
        "\n",
        "if (train_on_gpu ==1):\n",
        "    imagelist = [ t.to(dev) for t in imagelist ]\n",
        "#target_list = [ {'boxes':d['boxes'].to(device), 'labels':d['labels']} for d in target_list ]\n",
        "\n",
        "model.eval()  # Set model to training mode\n",
        "\n",
        "print('!!! FORWARD PASS !!!!')\n",
        "out=model(imagelist,tdata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44H7SaIHtRDi"
      },
      "source": [
        "model_children = list(model.children())\n",
        "print(model_children)\n",
        "no_of_layers=0\n",
        "conv_layers=[]\n",
        "\n",
        "rpn = model_children[2]\n",
        "#print(model_children[2])\n",
        "\n",
        "\n",
        "img = np.zeros((10,3,600,600))\n",
        "\n",
        "img= np.float32(img)\n",
        "result = model(torch.tensor(img).to(dev))\n",
        "\n",
        "'''\n",
        "    print('--  ',type(child))\n",
        "    if type(child)==nn.Conv2d:\n",
        "        no_of_layers+=1\n",
        "        conv_layers.append(child)\n",
        "    elif type(child)==nn.Sequential:\n",
        "        for layer in child.children():\n",
        "            if type(layer)==nn.Conv2d:\n",
        "                no_of_layers+=1\n",
        "                conv_layers.append(layer)\n",
        "print(no_of_layers)\n",
        "\n",
        "\n",
        "results = [conv_layers[0](img)]\n",
        "for i in range(1, len(conv_layers)):\n",
        "    results.append(conv_layers[i](results[-1]))\n",
        "outputs = results\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG5KgWUumXQD"
      },
      "source": [
        "model.roi_heads.box_predictor.cls_score(torch.tensor(img).to(dev))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VaveGUssEz1"
      },
      "source": [
        "### RUN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcRYVBE5TrgV"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# load a pre-trained model for classification and return\n",
        "# only the features\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "# FasterRCNN needs to know the number of\n",
        "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
        "# so we need to add it here\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "# let's make the RPN generate 5 x 3 anchors per spatial\n",
        "# location, with 5 different sizes and 3 different aspect\n",
        "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
        "# map could potentially have different sizes and\n",
        "# aspect ratios\n",
        "#anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "anchor_generator = AnchorGenerator(sizes=((8, 16, 32, 64, 128),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# let's define what are the feature maps that we will\n",
        "# use to perform the region of interest cropping, as well as\n",
        "# the size of the crop after rescaling.\n",
        "# if your backbone returns a Tensor, featmap_names is expected to\n",
        "# be [0]. More generally, the backbone should return an\n",
        "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
        "# feature maps to use.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "# put the pieces together inside a FasterRCNN model\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=2,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLWa6D72_kQB"
      },
      "source": [
        "'''\n",
        "\n",
        "#Show summary of model setup and move model to the GPU\n",
        " #train_on_gpu = torch.cuda.is_available()\n",
        "from torchsummary import summary\n",
        "\n",
        "if (train_on_gpu == 1):\n",
        "    #dev=torch.device(\"cuda\") \n",
        "    model.to(dev)\n",
        "    model.eval()\n",
        "    summary(model,(3,600,800), batch_size = 300, device='cuda')\n",
        "elif ( (train_on_tpu == 1) and (train_on_gpu == 0)):\n",
        "    ### TPU with pytorch has some issues\n",
        "    model_vgg16.to(dev)\n",
        "    summary(model_vgg16,(3,244,244), batch_size = bsize, device=dev)\n",
        "else:\n",
        "    summary(model,(3,600,800), batch_size = bsize)\n",
        "'''\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWURd5-GuUAJ"
      },
      "source": [
        "#MODEL RUN\n",
        "These are the assertions for the pretrained model\n",
        "'''\n",
        "    739                 floating_point_types = (torch.float, torch.double, torch.half)\n",
        "    740                 assert t[\"boxes\"].dtype in floating_point_types, 'target boxes must of float type'\n",
        "--> 741                 assert t[\"labels\"].dtype == torch.int64, 'target labels must of int64 type'\n",
        "    742                 if self.has_keypoint():\n",
        "    743                     assert t[\"keypoints\"].dtype == torch.float32, 'target keypoints must of float type'\n",
        "'''\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaO7Cn-VT4z2"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "\n",
        "#model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "#dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 2  # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "\n",
        "#from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "train_on_gpu = 0\n",
        "if (train_on_gpu):\n",
        "    model.to(dev)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer,\n",
        "                                       step_size=7,\n",
        "                                       gamma=0.1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#training_data set above by calling Custom Dataset\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(training_data,\n",
        "                                            batch_size=10,\n",
        "                                            shuffle=False, #True,\n",
        "                                            num_workers=2)\n",
        "\n",
        "##dataloader_training = DataLoader(train_subset, batch_size=bsize,shuffle=True, num_workers=2) #only 2 workers for Colab CPU\n",
        "# For Training\n",
        "\n",
        "\n",
        "images,targets = next(iter(data_loader))\n",
        "#images = list(image for image in images)\n",
        "\n",
        "imagelist=[]\n",
        "for ii in range(0,len(images)):\n",
        "    #imagelist.append(images[ii])\n",
        "    if (train_on_gpu):\n",
        "        imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32).to(dev))\n",
        "    else:\n",
        "        imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32))\n",
        "print(np.shape(images), len(images))\n",
        "print(np.shape(imagelist[0]))\n",
        "\n",
        "tdata=[]\n",
        "ddata={}\n",
        "\n",
        "import tensorflow as tf\n",
        "for ii in range(0,len(targets['boxes'])):\n",
        "    if (train_on_gpu):\n",
        "        ddata['boxes'] = targets['boxes'][ii].to(dev) #torch.DoubleTensor(targets['boxes'][ii])\n",
        "\n",
        "        ddata['labels']=targets['labels'][ii].to(dev)\n",
        "        ddata['area'] = targets['area'][ii].to(dev)\n",
        "        tdata.append(ddata)\n",
        "    else:\n",
        "        ddata['boxes'] = targets['boxes'][ii] #torch.DoubleTensor(targets['boxes'][ii])\n",
        "\n",
        "        ddata['labels']=targets['labels'][ii]\n",
        "        ddata['area'] = targets['area'][ii]\n",
        "        tdata.append(ddata)\n",
        "\n",
        "\n",
        "if (train_on_gpu ==1):\n",
        "    imagelist = [ t.to(dev) for t in imagelist ]\n",
        "#target_list = [ {'boxes':d['boxes'].to(device), 'labels':d['labels']} for d in target_list ]\n",
        "\n",
        "model.eval()  # Set model to training mode\n",
        "\n",
        "print('!!! FORWARD PASS !!!!')\n",
        "out=model(imagelist,tdata)\n",
        "#out = model(images, tdata)\n",
        "\n",
        "losses = sum(loss for loss in out.values())\n",
        "\n",
        "\n",
        "#targets_formatted = [{'boxes', targets['boxes'],\n",
        "#            'labels',targets['labels']}]\n",
        "#output = model(images,targets)   # Returns losses and detections\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)           # Returns predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfjUvHjXyl-4"
      },
      "source": [
        "#PLOT ANCHORS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYOXySHubZ9R"
      },
      "source": [
        "out[0]\n",
        "\n",
        "\n",
        "for fcount,numfiles in enumerate(range(0,len(out),1)):\n",
        "    idata = imagelist[fcount]\n",
        "    plt.figure(figsize=(8, 6), dpi=30)\n",
        "    plt.imshow(idata[0,:,:].cpu(),cmap='gray')\n",
        "    plt.title(str(fcount))\n",
        "\n",
        "\n",
        "    for counter,blist in enumerate(range(0,len(out[fcount]['boxes']))): #out['boxes']):\n",
        "        for ii in range(0,blist):\n",
        "            points = out[fcount]['boxes'][ii]\n",
        "\n",
        "            #ii = out[counter]['boxes'][0]\n",
        "            newbox = [torch.detach(points[0]),torch.detach(points[1]),torch.detach(points[2]),torch.detach(points[3])]\n",
        "            #idata = imagelist[counter]\n",
        "\n",
        "            rect = patches.Rectangle((np.uint(newbox[0].cpu()),\n",
        "                                    np.uint(newbox[1].cpu())),\n",
        "                                    np.uint(newbox[2].cpu())-(np.uint(newbox[0].cpu())),\n",
        "                                    np.uint(newbox[3].cpu())-(np.uint(newbox[1].cpu())),\n",
        "                                    linewidth=0.8,\n",
        "                                    edgecolor='r',\n",
        "                                    facecolor='none')\n",
        "\n",
        "\n",
        "\n",
        "                # Get the current reference\n",
        "            ax = plt.gca()\n",
        "                # Add the patch to the Axes\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "print('counter is ',counter)\n",
        "    \n",
        "#x = torchvision.conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(out) #(base_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RQc79uabOjo"
      },
      "source": [
        "idata = imagelist[5]\n",
        "plt.figure(figsize=(8, 6), dpi=80)\n",
        "plt.imshow(idata[0,:,:].cpu(),cmap='gray')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASlpLcLS3wJr"
      },
      "source": [
        "print(len(targets[\"boxes\"]))\n",
        "#print(targets['boxes'][\n",
        "\n",
        "fdata = []\n",
        "box_data={}\n",
        "label_data={}\n",
        "ddata={}\n",
        "\n",
        "for ii in range(0,3):\n",
        "    ddata['boxes']=targets['boxes'][ii]\n",
        "    ddata['labels']=targets['labels'][ii]\n",
        "    fdata.append(ddata)\n",
        "\n",
        "  \n",
        "\n",
        "print(fdata)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR9duulEKD8C"
      },
      "source": [
        "targets.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcxPKk2lfQLl"
      },
      "source": [
        "x = [torch.rand(3, 300, 400), torch.rand(3, 300, 400)]\n",
        "a = torch.tensor([[100,100,200,200],[110,110,350,400]], dtype=torch.float64)\n",
        "b = torch.tensor([1,2],dtype=torch.int64)\n",
        "targets2 = [{'boxes': a, 'labels':  b},\n",
        "            {'boxes': a, 'labels':  b}]\n",
        "#print(targets2)\n",
        "for target in targets2:\n",
        "    #print(target)\n",
        "    boxes = target[\"boxes\"]\n",
        "\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "targets_formatted = [{k: v for k, v in t.items()} for t in targets2]\n",
        "targets_formatted\n",
        "#imagesx = images[0:1]\n",
        "#output = model(imagesx,targets)   # Returns losses and detections\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQpUW1ZwZDgt"
      },
      "source": [
        "print(type(targets['boxes']))\n",
        "a={}\n",
        "a[\"t1\"]=4\n",
        "a[\"t2\"] = [1,2]\n",
        "a=list(a)\n",
        "for counter,aa in enumerate(a):\n",
        "    print('aa = ',aa)\n",
        "    for bb in aa.items():\n",
        "        print(bb,counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zO96z47PRis"
      },
      "source": [
        "#a = [{k: v for k, v in t.items()} for t in targets]\n",
        "a=targets[\"boxes\"]\n",
        "if isinstance(a,torch.Tensor):\n",
        "    print('is instance')\n",
        "for t in targets:\n",
        "    print(t)\n",
        "    for k,v in t.items():\n",
        "        print(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJJh5XVWgPmG"
      },
      "source": [
        "import torchvision\n",
        "#model = torchvision.models.vgg16(pretrained=True)\n",
        "model=torchvision.models.resnet50(pretrained=True)\n",
        "#fe = list(model.features)\n",
        "\n",
        "model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXMzB_4zGlAc"
      },
      "source": [
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=3,\n",
        "                                                gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc-2gzhSHesm"
      },
      "source": [
        "    num_epochs = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # train for one epoch, printing every 10 iterations\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "        # update the learning rate\n",
        "        lr_scheduler.step()\n",
        "        # evaluate on the test dataset\n",
        "        evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "    print(\"That's it!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KDkNEYNt5Fi"
      },
      "source": [
        "import json\n",
        "#x = box_info\n",
        "#json.loads(x)\n",
        "#type(box_info)\n",
        "\n",
        "from ast import literal_eval #as make_tuple\n",
        "a=literal_eval(box_info)\n",
        "print(a[0])\n",
        "type(a[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WJ5mZROnju7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgzzBJKxPUtV"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# Create a ZipFile Object and load sample.zip in it\n",
        "full_file = os.path.join(label_data_dir, label_files[0])\n",
        "with ZipFile(full_file, 'r') as zipObj:\n",
        "   # Get a list of all archived file names from the zip\n",
        "   listOfFileNames = zipObj.namelist()\n",
        "   for fnames in listOfFileNames:\n",
        "       print(fnames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQxlbtgCRovN"
      },
      "source": [
        "#SCRATCH AREA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZIOIkPGR1H8"
      },
      "source": [
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=''):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "        # convert the PIL Image into a numpy array\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4El0WZb2SAL7"
      },
      "source": [
        "import transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZK1RHwiRrTn"
      },
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "dataset = PennFudanDataset('PennFudanPed')# get_transform(train=True))\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        " dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        " collate_fn=utils.collate_fn)\n",
        "# For Training\n",
        "images,targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "output = model(images,targets)   # Returns losses and detections\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)      "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}