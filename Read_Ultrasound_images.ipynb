{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Read_Ultrasound_images",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZXs9CFE07Cyhk3Mugs5gB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb017f4159ec42adb369e1b2f68f89d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3a67d81fd0664bbdb24be7c3cd9e3d7b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b8c9897f04c245bab8f3423b4ced2d05",
              "IPY_MODEL_f4713c478df149828349b4dd6a37df4b",
              "IPY_MODEL_0a9d4f004158424785ff6f9994ceaaa8"
            ]
          }
        },
        "3a67d81fd0664bbdb24be7c3cd9e3d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b8c9897f04c245bab8f3423b4ced2d05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8416abd2914d42ebb3bd66c80ba016e9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae5f17d798bc48d5b1246fc185afd02c"
          }
        },
        "f4713c478df149828349b4dd6a37df4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f542efdfbc084fb59e718a5c2a0cebe3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102530333,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102530333,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a6478e991a0498d9193abfb837b2c2d"
          }
        },
        "0a9d4f004158424785ff6f9994ceaaa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_13e3f3a95af44e18a9f06325e2c26908",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 134MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2b9cd890ae294bc5a15d1f95573c5858"
          }
        },
        "8416abd2914d42ebb3bd66c80ba016e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae5f17d798bc48d5b1246fc185afd02c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f542efdfbc084fb59e718a5c2a0cebe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a6478e991a0498d9193abfb837b2c2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "13e3f3a95af44e18a9f06325e2c26908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2b9cd890ae294bc5a15d1f95573c5858": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/einsteinxx/UCLA_BIOENG_596_FALL_21/blob/main/Read_Ultrasound_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NkBEnltCrpg",
        "outputId": "07f7cef0-b02d-45d8-bec9-d14a73a9e09e"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import gc  #debug memory leaks in matplotlib\n",
        "import csv #read in description files\n",
        "import random #used to select random slice for patches\n",
        "import cv2\n",
        "\n",
        "\n",
        "!pip3 install -q torchinfo\n",
        "!pip3 install -Uqq ipdb\n",
        "################################################################################\n",
        "#ULTRASOUND NEEDS\n",
        "import PIL\n",
        "#from PIL import Image\n",
        "# Open the image form working directory\n",
        "#image = Image.open(full_file)\n",
        "from matplotlib import image\n",
        "from ast import literal_eval #used to break out bounding boxes from strings\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import torchinfo\n",
        "\n",
        "from torchinfo import summary\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "\n",
        "\n",
        "import torchinfo\n",
        "\n",
        "\n",
        "import ipdb\n",
        "################################################################################\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "#\n",
        "# Read Data from google drive\n",
        "#\n",
        "from google.colab import drive #for loading gdrive data\n",
        "from google.colab import files\n",
        "\n",
        "# install dependencies not included by Colab\n",
        "# use pip3 to ensure compatibility w/ Google Deep Learning Images \n",
        "!pip3 install -q pydicom \n",
        "!pip3 install -q tqdm \n",
        "!pip3 install -q imgaug\n",
        "!pip3 install -q pickle5\n",
        "\n",
        "import pydicom #to read dicom files\n",
        "from pydicom import dcmread\n",
        "import pickle5 as pickle; #generic storage of image arra\n",
        "\n",
        "# Load data from google drive\n",
        "#\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data_dir = '/content/gdrive/Shareddrives/BreastUS'\n",
        "local_dir = '/content/gdrive/My Drive/BreastUS' #for local storage\n",
        "\n",
        "\n",
        "'''\n",
        "top_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES' \n",
        "csv_dir = '/content/gdrive/My Drive/DBT_DATA/TRAINING_DATA/'\n",
        "\n",
        "\n",
        "#output patch save dir\n",
        "patch_normal_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES/NORMAL'\n",
        "patch_actionable_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES/ACTIONABLE' \n",
        "patch_benign_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES/BENIGN'\n",
        "patch_cancer_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES/CANCER'\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "### Enable GPU, if present\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if (train_on_gpu):\n",
        "    !nvidia-smi -L\n",
        "    !nvidia-smi \n",
        "    dev=torch.device(\"cuda\")\n",
        "else:\n",
        "    print('GPU NOT FOUND!!! USING CPU INSTEAD!!!!!')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 40 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 92 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 337 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 348 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 358 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 368 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 378 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 389 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 399 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 409 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 419 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 430 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 440 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 450 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 460 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 471 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 481 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 491 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 501 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 512 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 522 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 532 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 542 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 552 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 563 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 573 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 583 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 593 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 604 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 614 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 624 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 634 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 645 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 655 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 665 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 675 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 686 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 696 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 706 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 716 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 727 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 737 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 747 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 757 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 768 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 778 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 788 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 791 kB 5.1 MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |▉                               | 10 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20 kB 34.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30 kB 41.5 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40 kB 44.0 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51 kB 44.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 61 kB 47.5 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 71 kB 50.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 81 kB 50.5 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92 kB 52.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 112 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 122 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 133 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 143 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 153 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 163 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 174 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 184 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 194 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 204 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 215 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 225 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 235 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 245 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 256 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 266 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 276 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 286 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 296 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 307 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 317 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 327 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 337 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 348 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 358 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 368 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 374 kB 54.1 MB/s \n",
            "\u001b[?25h  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.23 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.30.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 256 kB 5.1 MB/s \n",
            "\u001b[?25hMounted at /content/gdrive\n",
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-05833e0c-7b53-8987-dc7f-41dd584136db)\n",
            "Sun Nov 28 10:30:25 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdVnU-1rU__i"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jui81Q6f872p",
        "outputId": "117e4404-002a-44f5-db73-58912bf5a12b"
      },
      "source": [
        "#\n",
        "# Get DICOM info\n",
        "#\n",
        "local_files = os.listdir(local_dir)\n",
        "\n",
        "for dicom_file in local_files:\n",
        "    filename = os.path.join(local_dir,dicom_file)\n",
        "    if (os.path.isdir(filename) == 1):\n",
        "        #skip any directories found in list\n",
        "        continue\n",
        "    if ('dcm' in filename):\n",
        "        print(filename)\n",
        "        ds = dcmread(filename, force=True)\n",
        "        for element in ds:\n",
        "            print(element)\n",
        "\n",
        "    else:\n",
        "        print('Non-dicom file found ',filename)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/BreastUS/1.dcm\n",
            "(0008, 0005) Specific Character Set              CS: 'ISO_IR 100'\n",
            "(0008, 0008) Image Type                          CS: ['DERIVED', 'PRIMARY', 'SMALL PARTS']\n",
            "(0008, 0012) Instance Creation Date              DA: '20211111'\n",
            "(0008, 0013) Instance Creation Time              TM: '132938'\n",
            "(0008, 0016) SOP Class UID                       UI: Ultrasound Multi-frame Image Storage\n",
            "(0008, 0018) SOP Instance UID                    UI: 1.2.840.113663.1500.1.365952523.3.2.20210204.132938.671\n",
            "(0008, 0020) Study Date                          DA: '20211111'\n",
            "(0008, 0021) Series Date                         DA: '20211111'\n",
            "(0008, 0023) Content Date                        DA: '20211111'\n",
            "(0008, 002a) Acquisition DateTime                DT: '20211111'\n",
            "(0008, 0030) Study Time                          TM: '130201'\n",
            "(0008, 0031) Series Time                         TM: '130201'\n",
            "(0008, 0033) Content Time                        TM: '132938'\n",
            "(0008, 0050) Accession Number                    SH: 'A_TR6Z2X0X'\n",
            "(0008, 0060) Modality                            CS: 'US'\n",
            "(0008, 0068) Presentation Intent Type            CS: 'FOR PRESENTATION'\n",
            "(0008, 0070) Manufacturer                        LO: 'Philips Medical Systems'\n",
            "(0008, 0080) Institution Name                    LO: 'BKWIC,SM,CA 90404 RM2'\n",
            "(0008, 0090) Referring Physician's Name          PN: 'LEE^MINNA'\n",
            "(0008, 1010) Station Name                        SH: 'USSM'\n",
            "(0008, 1030) Study Description                   LO: 'RIGHT BREAST BIOPSY ULTRASOUND GUIDED'\n",
            "(0008, 1032) Procedure Code Sequence             SQ: <Sequence, length 1>\n",
            "(0008, 103e) Series Description                  LO: 'MM US RT BREAST BIOPSY'\n",
            "(0008, 1070) Operators' Name                     PN: ''\n",
            "(0008, 1090) Manufacturer's Model Name           LO: 'iU22'\n",
            "(0008, 1111) Referenced Performed Procedure Step SQ: <Sequence, length 1>\n",
            "(0008, 2111) Derivation Description              ST: ''\n",
            "(0009, 0000) Private Creator                     UL: 14\n",
            "(0009, 0010) Private tag data                    LO: 'GEIIS'\n",
            "(0010, 0010) Patient's Name                      PN: '10088_1_0K0L7T04'\n",
            "(0010, 0020) Patient ID                          LO: '10088_1_0K0L7T04'\n",
            "(0010, 0021) Issuer of Patient ID                LO: '001R41:20090813:023040546:015090'\n",
            "(0010, 0030) Patient's Birth Date                DA: '19000101'\n",
            "(0010, 0040) Patient's Sex                       CS: ''\n",
            "(0010, 1020) Patient's Size                      DS: None\n",
            "(0010, 1030) Patient's Weight                    DS: None\n",
            "(0010, 1040) Patient's Address                   LO: 'MQ DE-IDENTIFIED'\n",
            "(0010, 2160) Ethnic Group                        SH: 'MQ DE-IDENTIFIED'\n",
            "(0018, 1000) Device Serial Number                LO: '365952523'\n",
            "(0018, 1020) Software Versions                   LO: 'PMS5.1 Ultrasound iU22_6.0.2.144'\n",
            "(0018, 1030) Protocol Name                       LO: 'Free Form'\n",
            "(0018, 1063) Frame Time                          DS: '31.154'\n",
            "(0018, 1088) Heart Rate                          IS: '0'\n",
            "(0018, 5010) Transducer Data                     LO: ['L12_5', '', '']\n",
            "(0018, 5020) Processing Function                 LO: 'SM_PRTS_ADV_BREAST'\n",
            "(0018, 6011) Sequence of Ultrasound Regions      SQ: <Sequence, length 1>\n",
            "(0020, 000d) Study Instance UID                  UI: 1.2.840.114350.2.300.2.798268.2.463966434.1\n",
            "(0020, 000e) Series Instance UID                 UI: 1.2.840.113663.1500.1.365952523.2.1.20210204.130201.984\n",
            "(0020, 0010) Study ID                            SH: '465241817'\n",
            "(0020, 0011) Series Number                       IS: '1'\n",
            "(0020, 0013) Instance Number                     IS: '1'\n",
            "(0028, 0002) Samples per Pixel                   US: 3\n",
            "(0028, 0004) Photometric Interpretation          CS: 'RGB'\n",
            "(0028, 0006) Planar Configuration                US: 0\n",
            "(0028, 0008) Number of Frames                    IS: '1'\n",
            "(0028, 0009) Frame Increment Pointer             AT: (0018, 1063)\n",
            "(0028, 0010) Rows                                US: 600\n",
            "(0028, 0011) Columns                             US: 800\n",
            "(0028, 0014) Ultrasound Color Data Present       US: 0\n",
            "(0028, 0100) Bits Allocated                      US: 8\n",
            "(0028, 0101) Bits Stored                         US: 8\n",
            "(0028, 0102) High Bit                            US: 7\n",
            "(0028, 0103) Pixel Representation                US: 0\n",
            "(0028, 0301) Burned In Annotation                CS: 'YES'\n",
            "(0028, 2110) Lossy Image Compression             CS: '01'\n",
            "(0028, 2112) Lossy Image Compression Ratio       DS: '0.0'\n",
            "(0032, 1032) Requesting Physician                PN: 'LEE^MINNA^K.^^MD'\n",
            "(0032, 1033) Requesting Service                  LO: 'Unspecified'\n",
            "(0040, 0244) Performed Procedure Step Start Date DA: '20211111'\n",
            "(0040, 0245) Performed Procedure Step Start Time TM: '130201'\n",
            "(0040, 0253) Performed Procedure Step ID         SH: '20211111'\n",
            "(0040, 0254) Performed Procedure Step Descriptio LO: 'MM US RT BREAST BIOPSY'\n",
            "(0040, 0260) Performed Protocol Code Sequence    SQ: <Sequence, length 1>\n",
            "(0040, 0280) Comments on the Performed Procedure ST: 'Breast'\n",
            "(0903, 0010) Private Creator                     LO: 'GEIIS PACS'\n",
            "(0903, 1010) [Reject Image Flag]                 US: 0\n",
            "(0903, 1011) [Significant Flag]                  US: 0\n",
            "(0903, 1012) [Confidential Flag]                 US: 0\n",
            "(0905, 0010) Private Creator                     LO: 'GEIIS'\n",
            "(0905, 1030) [Assigning Authority For Patient ID LO: '001R41:20090813:023040546:015090'\n",
            "(2050, 0020) Presentation LUT Shape              CS: 'IDENTITY'\n",
            "(7fd1, 0000) Private Creator                     UL: 350\n",
            "(7fd1, 0010) Private tag data                    LO: 'GEIIS'\n",
            "(7fd1, 1010) [GE IIS Compression ID]             UL: 17\n",
            "(7fd1, 1020) [GE IIS Multiframe Offsets]         UL: Array of 79 elements\n",
            "(7fe0, 0000) Group Length                        UL: 113760012\n",
            "(7fe0, 0010) Pixel Data                          OB: Array of 1440000 elements\n",
            "Non-dicom file found  /content/gdrive/My Drive/BreastUS/Copy of us_status_index.pickle\n",
            "Non-dicom file found  /content/gdrive/My Drive/BreastUS/us_status_index2nd.pickle\n",
            "Non-dicom file found  /content/gdrive/My Drive/BreastUS/us_status_index.pickle\n",
            "Non-dicom file found  /content/gdrive/My Drive/BreastUS/counters_to_remove.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afyEeFFcVJCa"
      },
      "source": [
        "def get_csv_data(filename):\n",
        "    fields = []\n",
        "    rows = []\n",
        "\n",
        "    # reading csv file\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        # creating a csv reader object\n",
        "        csvreader = csv.reader(csvfile)\n",
        "        \n",
        "        # extracting field names through first row\n",
        "        fields = next(csvreader)\n",
        "\n",
        "        # extracting each data row one by one\n",
        "        for row in csvreader:\n",
        "            rows.append(row)\n",
        "\n",
        "\n",
        "        # lines present\n",
        "        print(\"found rows: %d\"%(csvreader.line_num))\n",
        "    return fields, rows"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X-TI57eLjrY"
      },
      "source": [
        "#Load a dicom image and convert it using dcm_read to get view type (needs this \n",
        "#to correctly align image))\n",
        "\n",
        "csv_dir = os.path.join(data_dir,'Annotated data')\n",
        "\n",
        "annotated_dir = os.path.join(data_dir,'Annotated data')\n",
        "data_files = os.listdir(annotated_dir)\n",
        "\n",
        "\n",
        "label_data_dir = os.path.join(annotated_dir,'LabelMe_3.0_format_updated')\n",
        "label_files = os.listdir(label_data_dir)\n",
        "\n",
        "\n",
        "\n",
        "#sample_dicom = os.path.join(dicom_dir,dicom_files[0]) #for now, just use first\n",
        "\n",
        "#!ls '/content/gdrive/Shareddrives/BreastUS'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzhCbP7tbCX8"
      },
      "source": [
        "# GET CSV INFO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrtsfNXHWsUX",
        "outputId": "14752837-b4e0-449a-bd26-8329caa98832"
      },
      "source": [
        "#READ CSV FILES\n",
        "#pull out the box information and label info\n",
        "csv_list = os.listdir(csv_dir)\n",
        "\n",
        "for csv_file in csv_list:\n",
        "    filename = os.path.join(csv_dir,csv_file)\n",
        "    if (os.path.isdir(filename) == 1):\n",
        "        #skip any directories found in list\n",
        "        continue\n",
        "    if ('_final' in csv_file):\n",
        "        annotation_fields, annotation_rows = get_csv_data(filename)\n",
        "\n",
        "    else:\n",
        "        print('Non-archive file found ',filename)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found rows: 8843\n",
            "Non-archive file found  /content/gdrive/Shareddrives/BreastUS/Annotated data/annotations_updated.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnV_94fziBBe"
      },
      "source": [
        "#Sort data into fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3nGek1gaHBY",
        "outputId": "9c2c80f5-46e8-43f2-b896-af5d6f5f8528"
      },
      "source": [
        "\n",
        "\n",
        "print(np.shape(annotation_rows))\n",
        "\n",
        "print(annotation_fields)\n",
        "\n",
        "array_rows = np.array(annotation_rows)\n",
        "mrn = array_rows[:,1]\n",
        "accession = array_rows[:,2]\n",
        "video_id = array_rows[:,3]\n",
        "frame_id = array_rows[:,4]\n",
        "image_path = array_rows[:,5]\n",
        "bounding_box = array_rows[:,6]\n",
        "diagnosis = array_rows[:,7]\n",
        "biopsy_site = array_rows[:,8]\n",
        "diagnosis2 = array_rows[:,9]\n",
        "first50 = array_rows[:,10]\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8842, 11)\n",
            "['', 'coded_mrn', 'coded_accession', 'video_id', 'frame_id', 'image_path', 'bounding box', 'Final Diagnosis', 'Bx Site', 'Final Diagnosis.1', 'First 50']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne5HF42dCdpe"
      },
      "source": [
        "# CROPPING IMAGES "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RuH5Med3z7n"
      },
      "source": [
        "def crop_us_image(img, debug =0):\n",
        "################################################################################\n",
        "#\n",
        "## Look for coordinates to cut the US data out of the image overlay\n",
        "################################################################################\n",
        "\n",
        "\n",
        "#\n",
        "# Get valid rows for the embedded US image\n",
        "#\n",
        "    nrows, ncols = np.shape(img)\n",
        "    middle_column = np.uint(ncols/2)\n",
        "    middle_row = np.uint(nrows/2)\n",
        "\n",
        "    #find the 0 gaps before and after the image on the column\n",
        "    data_strip = img[:,middle_column]\n",
        "    index = np.where(data_strip == 0) #find first section of 0 values\n",
        "    first_row_gap = index[0][0]\n",
        "    diff_row = np.diff(index)\n",
        "\n",
        "\n",
        "    #find the large jump in diff, that will likely be US image then down to background 0\n",
        "    #second_row_gap = np.where(diff_row >= 150)\n",
        "    if (debug == 1):\n",
        "        print('diff in func ',diff_row)\n",
        "\n",
        "    leading_found = 0\n",
        "    trailing_found = 0\n",
        "    leading_row_edge=0\n",
        "    for counter,ii in enumerate(diff_row[0]):\n",
        "        case_status = 1 #assume case will be usable, if not change flag\n",
        "        if ((counter > 2) and \n",
        "            (counter < (np.size(diff_row)-6) )):\n",
        "\n",
        "            #find a set of diff 0s that skip a distance (edge)\n",
        "            if ( (diff_row[0][counter-3] <5 ) and\n",
        "                (diff_row[0][counter-2] <5) and\n",
        "                (diff_row[0][counter-1] <5) and\n",
        "                (ii> 10) and\n",
        "                (leading_found ==0)):\n",
        "                \n",
        "                leading_row_edge = index[0][counter]\n",
        "                leading_found = 1\n",
        "                \n",
        "                #print('**found a leading edge at counter, val, image row', counter,ii, leading_row_edge)\n",
        "                continue\n",
        "                 #only need first point\n",
        "            '''\n",
        "            if ( (diff_row[0][counter+3] ==1 ) and\n",
        "                (diff_row[0][counter+2] ==1) and\n",
        "                (diff_row[0][counter+1] ==1) and\n",
        "                (ii>=2) and \n",
        "                (leading_found == 1)):\n",
        "            '''\n",
        "            if (leading_found ==1): #(counter > leading_row_edge):\n",
        "\n",
        "                for jj in range(0,6):\n",
        "                    if (diff_row[0][counter+jj] <=5):\n",
        "                        numzero =1\n",
        "                    else:\n",
        "                        numzero = 0\n",
        "                        break\n",
        "                \n",
        "                if (numzero == 1):\n",
        "                    #print('finding trail at ',counter)\n",
        "                    trailing_row_edge=index[0][counter]\n",
        "                    trailing_found = 1 #fix this\n",
        "                    #print('!!found a trail edge at counter, val, image row',\n",
        "                    #    counter,ii, trailing_row_edge)\n",
        "                    break\n",
        "                 #only need first point\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        low_row = leading_row_edge #index[0][second_row_gap[1][0]]\n",
        "        high_row = trailing_row_edge #index[0][second_row_gap[1][0]+1] \n",
        "    except:\n",
        "        print('could not find a second row for img')\n",
        "        \n",
        "        skip_points = [0,0,0,0]\n",
        "        case_status = 0\n",
        "        return img, skip_points, case_status #0 signifies this one failed, remove it\n",
        "    #\n",
        "    # Clip columns by finding rows with continuous 0 vals. Find the diff between \n",
        "    # elements and take the first point where the black transitions to a non-zero \n",
        "    # value. This is the first cutoff. The second is where the transition from \n",
        "    # regular data to zeros occurs\n",
        "    #\n",
        "    data_rows = img[(np.uint(nrows*0.25), np.uint(nrows*0.5), np.uint(nrows*0.75)),:]\n",
        "    \n",
        "    if (debug == 1):\n",
        "        plt.figure()\n",
        "        x=np.arange(0,ncols)\n",
        "        plt.plot(x,data_rows[0,:],'r.-',x,data_rows[1,:], 'b.-',x,data_rows[2,:],'k.-')\n",
        "        \n",
        "\n",
        "    #top 75% row should be clear of all burned images at the beginning\n",
        "    xx = np.ediff1d(data_rows[2,:])\n",
        "    start= np.where(xx >00)\n",
        "\n",
        "    \n",
        "    if (not start): #empty list is FALSE\n",
        "        case_status = 0\n",
        "        skip_points = [0,0,0,0]\n",
        "        print('Could not find enough column points, skipping')\n",
        "        return img, skip_points, case_status #0 signifies this one failed, remove it\n",
        "\n",
        "    try:\n",
        "        start_column = start[0][0] +1 #since it's a diff, add one pixel\n",
        "        final_column = start[0][-1] -1 #last point where diff >0\n",
        "    except:\n",
        "        print('Failed to get a \"where\" point. Skipping')\n",
        "        case_status = 0\n",
        "        skip_points = [0,0,0,0]\n",
        "        return img, skip_points, case_status #0 signifies this one failed, remove it\n",
        "\n",
        "    '''\n",
        "    print(start[0])\n",
        "    plt.figure()\n",
        "    plt.imshow(img[low_row:high_row,start_column:final_column],cmap='gray')\n",
        "    plt.title('cropped')\n",
        "    '''\n",
        "\n",
        "    #print('inside cropping, shape is ', np.shape(img))\n",
        "    skip_points = [low_row, high_row,start_column, final_column]\n",
        "    return img[low_row:high_row,start_column:final_column], skip_points, case_status"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z7OLshP-VcO"
      },
      "source": [
        "if(0):\n",
        "    num_mrn = set(mrn)\n",
        "    num_mrn\n",
        "    image_path[0:3]\n",
        "\n",
        "    print(image_path[0])\n",
        "    filename = os.path.basename(image_path[0]) \n",
        "    [_,fpath] =image_path[0].split('drive/MyDrive/Annotated data/')\n",
        "    full_file = os.path.join(annotated_dir,fpath)\n",
        "    print(full_file)\n",
        "\n",
        "    print('filename exists: ',os.path.exists(full_file))\n",
        "    import imageio\n",
        "\n",
        "    #import PIL\n",
        "    #from PIL import Image\n",
        "    # Open the image form working directory\n",
        "    #image = Image.open(full_file)\n",
        "\n",
        "    #from matplotlib import image\n",
        "    #from matplotlib import pyplot\n",
        "    img = image.imread(full_file)\n",
        "\n",
        "\n",
        "\n",
        "    ## Convert the RGB input into grayscale\n",
        "    R, G, B = img[:,:,0], img[:,:,1], img[:,:,2]\n",
        "    imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.imshow(imgGray, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "    cropped_image,skip_points,case_status = crop_us_image(imgGray,0)\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.imshow(cropped_image, cmap='gray')\n",
        "    plt.title('cropped image')\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5jgDw8p9Lg3"
      },
      "source": [
        "if(0):\n",
        "    #del cropped_image,  img_data\n",
        "    img_data = image.imread(idata)\n",
        "    ## Convert the RGB input into grayscale\n",
        "    R, G, B = img_data[:,:,0], img_data[:,:,1], img_data[:,:,2]\n",
        "    imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "    print('shape of imgGray is ', np.shape(imgGray))\n",
        "    cropped_image, _ = crop_us_image(imgGray,0)\n",
        "    if (np.size(cropped_image) <1):\n",
        "        print('Failure during cropping for ', file_name)\n",
        "    print('shape is ', np.shape(cropped_image))\n",
        "    plt.figure()\n",
        "    plt.imshow(cropped_image)\n",
        "\n",
        "    cropped_image = cv2.resize(cropped_image, dsize=(480, 600),\n",
        "                                interpolation=cv2.INTER_CUBIC)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xClpqcOB55BT"
      },
      "source": [
        "### SAVE IMAGES TO DISK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "039DO_cZorqH"
      },
      "source": [
        "if(0):\n",
        "    #['', 'coded_mrn', 'coded_accession', 'video_id', 'frame_id', 'image_path', 'bounding box', 'Final Diagnosis', 'Bx Site', 'Final Diagnosis.1', 'First 50']\n",
        "    '''\n",
        "    mrn = array_rows[:,1]\n",
        "    accession = array_rows[:,2]\n",
        "    video_id = array_rows[:,3]\n",
        "    frame_id = array_rows[:,4]\n",
        "    image_path = array_rows[:,5]\n",
        "    bounding_box = array_rows[:,6]\n",
        "    diagnosis = array_rows[:,7]\n",
        "    biopsy_site = array_rows[:,8]\n",
        "    diagnosis2 = array_rows[:,9]\n",
        "    first50 = array_rows[:,10]\n",
        "    '''\n",
        "\n",
        "    mrn_list = set(mrn)\n",
        "\n",
        "    acc =set()\n",
        "    vid = set()\n",
        "\n",
        "    #try first US video in first mrn\n",
        "\n",
        "\n",
        "    for count,mm in enumerate(mrn):\n",
        "        \n",
        "        if mm in mrn[0]:\n",
        "            acc.add(accession[count])\n",
        "            vid.add(video_id[count])\n",
        "\n",
        "    screen_dir = '/content/gdrive/My Drive/BreastUS/SCREEN_CAPS/'\n",
        "    #video_save = os.path.join()\n",
        "\n",
        "\n",
        "    file_name =[]\n",
        "    vname=[]\n",
        "    boundary_box = []\n",
        "    for cc,video_temp in enumerate(video_id):\n",
        "        [_,fpath] =image_path[cc].split('drive/MyDrive/Annotated data/')\n",
        "        full_file = os.path.join(annotated_dir,fpath)\n",
        "        file_name.append(full_file)\n",
        "        vname.append(video_temp)\n",
        "        boundary_box.append(bounding_box[cc])\n",
        "\n",
        "    print(file_name[0])\n",
        "    print(vname[0])\n",
        "    print(len(file_name))\n",
        "\n",
        "\n",
        "    case_holder = []\n",
        "    failed_list = [] #record of images that failed cropping\n",
        "\n",
        "    skip_to = 8002 #-1\n",
        "    for count,ii in enumerate(file_name):\n",
        "\n",
        "        if (count < skip_to):\n",
        "            continue #skip these until we get back to the proper file\n",
        "\n",
        "        #\n",
        "        # Save folder information. Get the video name and make a folder with that \n",
        "        # long name in the screen caps folder\n",
        "        #\n",
        "        video_save = os.path.join(screen_dir,vname[count])\n",
        "        if(os.path.exists(video_save)):\n",
        "            print('folder found')\n",
        "        else:\n",
        "            os.mkdir(video_save)\n",
        "\n",
        "        img_data = image.imread(ii)\n",
        "        #print(np.shape(img_data))\n",
        "        #print('Processing: ', ii)\n",
        "        print(count)\n",
        "\n",
        "\n",
        "        corners=literal_eval(boundary_box[count]) #bounding_box[count])\n",
        "        #bounding box values are in (x,y) formats from xml\n",
        "        #print('corners, len = ',corners, len(corners))\n",
        "        #print('bounding box = ',boundary_box[count]) #bounding_box[count])\n",
        "\n",
        "        '''\n",
        "        Bounding box ordering is \n",
        "        point 1        point 2\n",
        "        ----------------------\n",
        "        point 4        point 3\n",
        "        '''\n",
        "        if (len(corners)>0):\n",
        "            #print('corners ',pos)\n",
        "            pos = np.uint(corners)\n",
        "            xmin = pos[0][0]\n",
        "            xmax = pos[1][0]\n",
        "            ymin = pos[0][1]\n",
        "            ymax = pos[2][1]\n",
        "            #boxes.append([xmin, ymin, xmax, ymax])\n",
        "            h=ymax-ymin\n",
        "            w=xmax-xmin\n",
        "        else:\n",
        "            #print('EMPTY bounding box')\n",
        "            xmin=np.uint(1)\n",
        "            xmax=np.uint(2)\n",
        "            ymin=np.uint(1)\n",
        "            ymax=np.uint(2)\n",
        "            h=0\n",
        "            w=0\n",
        "            #boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "\n",
        "\n",
        "        #skip points =[low_row, high_row,start_column, final_column]\n",
        "        ## Convert the RGB input into grayscale\n",
        "        R, G, B = img_data[:,:,0], img_data[:,:,1], img_data[:,:,2]\n",
        "        imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "\n",
        "\n",
        "        #\n",
        "        # Crop the image to the Ultrasound borders. If it fails during this \n",
        "        # process, mark it and continue to next image. That failed index will be\n",
        "        # stored to remove any failed images later\n",
        "        #\n",
        "        cropped_image, skip_points,fail_flag = crop_us_image(imgGray,0)\n",
        "\n",
        "\n",
        "        show_plots = 0\n",
        "        if (fail_flag == 1): #if cropping worked, process annotations\n",
        "\n",
        "            #print('skip points = ',skip_points)\n",
        "            #print('h = ymax-ymin',ymax,ymin,h)\n",
        "            offset_row = ymin-skip_points[0]\n",
        "            offset_col = xmin-skip_points[2]\n",
        "\n",
        "            if (len(corners)>0):\n",
        "                width = xmax-xmin\n",
        "                height = ymax-ymin\n",
        "                rect = patches.Rectangle((offset_col,offset_row),width,height,linewidth=1,edgecolor='r',facecolor='none')\n",
        "                #print('adding rectangle ',offset_row, offset_col, height, width)\n",
        "                #print(ymin, ymax, xmin, xmax)\n",
        "            else:\n",
        "                rect = patches.Rectangle((0,0),0,0,linewidth=1,edgecolor='r',facecolor='none')\n",
        "\n",
        "            if (show_plots == 1):\n",
        "                show_crop = 0\n",
        "                if (show_crop == 0):\n",
        "\n",
        "                    fig = plt.figure(figsize=(8, 6), dpi=80)\n",
        "                    #plt.figure(figsize=(8, 6), dpi=80)\n",
        "                    #plt.imshow(cropped_image, cmap='gray')\n",
        "                    plt.imshow(cropped_image,cmap='gray')\n",
        "                    # Get the current reference\n",
        "                    ax = plt.gca()\n",
        "                    # Add the patch to the Axes\n",
        "                    ax.add_patch(rect)\n",
        "\n",
        "                    fname = os.path.join(video_save,os.path.basename(ii))\n",
        "                    #fname = os.path.basename(ii)\n",
        "                    title_text = 'Frame ' + str(fname)\n",
        "                    plt.title(title_text)\n",
        "                    \n",
        "                    plt.savefig(os.path.join(screen_dir,str(fname)))\n",
        "                    #time.sleep(1)\n",
        "                    #plt.show()\n",
        "                    plt.close()\n",
        "\n",
        "\n",
        "                if (show_crop == 1):  \n",
        "                    if (len(corners)>0):\n",
        "                        plt.figure(figsize=(8, 6), dpi=80)\n",
        "                        plt.imshow(img_data,cmap='gray')\n",
        "                \n",
        "                    # Get the current reference\n",
        "                        \n",
        "                        rect = patches.Rectangle((corners[0][0],corners[0][1]),w,h,linewidth=1,edgecolor='r',facecolor='none')\n",
        "                        bx = plt.gca()\n",
        "                        # Add the patch to the Axes\n",
        "                        bx.add_patch(rect)\n",
        "                        plt.show()\n",
        "                        plt.close()\n",
        "\n",
        "        else:\n",
        "            print('Failed crop for : ',file_name)\n",
        "            failed_list.append(count)\n",
        "        #save the fail list and file name list to remove fails later\n",
        "        if (count%1000 == 0):\n",
        "            status_file = os.path.join(local_dir,'us_status_index2nd.pickle')\n",
        "\n",
        "            pickle.dump([failed_list],open( status_file, \"wb\" ),protocol=5 )\n",
        "\n",
        "\n",
        "    print('finished plotting all images')\n",
        "\n",
        "\n",
        "    status_file = os.path.join(local_dir,'us_status_index2nd.pickle')\n",
        "    pickle.dump([failed_list],open( status_file, \"wb\" ),protocol=5 )\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    #print('filename exists: ',os.path.exists(full_file))\n",
        "\n",
        "    #image = image.imread(full_file)\n",
        "    #print(np.shape(image))\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knTF2v1bBNao"
      },
      "source": [
        "if(0):\n",
        "    status_file = os.path.join(local_dir,'Copy of us_status_index.pickle')\n",
        "    f1 = pickle.load( open( status_file, \"rb\" ) )\n",
        "\n",
        "    print(len(f1[0]))\n",
        "\n",
        "\n",
        "\n",
        "    status_file = os.path.join(local_dir,'us_status_index2nd.pickle')\n",
        "    f2 = pickle.load( open( status_file, \"rb\" ) )\n",
        "\n",
        "    print(len(f2[0]))\n",
        "\n",
        "    bad_count = f1[0]+f2[0]\n",
        "    print(len(bad_count))\n",
        "\n",
        "    bad_files = os.path.join(local_dir,'counters_to_remove.pickle')\n",
        "    pickle.dump([bad_count],open( bad_files, \"wb\" ),protocol=5 )"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu6rJqoou_3n"
      },
      "source": [
        "if(0):\n",
        "    #Test out problem images\n",
        "    #idata = '/content/gdrive/Shareddrives/BreastUS/Annotated data/unzipped_updated/1_srh8gfhj_a_0629s3fh_0.mp4-2021_07_15_17_21_15-labelme 3.0.zip/default/frame_000036.PNG'\n",
        "    idata = file_name[8002]\n",
        "    img_data = image.imread(idata)\n",
        "    print(np.shape(img_data))\n",
        "\n",
        "    %pdb off\n",
        "\n",
        "\n",
        "    ## Convert the RGB input into grayscale\n",
        "    R, G, B = img_data[:,:,0], img_data[:,:,1], img_data[:,:,2]\n",
        "    imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.imshow(imgGray,cmap='gray')\n",
        "\n",
        "    cropped_image, skip_points,fail_flag = crop_us_image(imgGray,1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMS1eAmkyjYr"
      },
      "source": [
        "if(0):\n",
        "    idata = '/content/gdrive/Shareddrives/BreastUS/Annotated data/unzipped_updated/1_2w948u10_a_06g91d42_4.mp4-2021_08_20_17_24_25-labelme 3.0.zip/default/frame_000159.PNG'\n",
        "    idata = '/content/gdrive/Shareddrives/BreastUS/Annotated data/unzipped_updated/1_srh8gfhj_a_0629s3fh_0.mp4-2021_07_15_17_21_15-labelme 3.0.zip/default/frame_000036.PNG'\n",
        "\n",
        "    img_data = image.imread(idata)\n",
        "    print(np.shape(img_data))\n",
        "\n",
        "\n",
        "    ## Convert the RGB input into grayscale\n",
        "    R, G, B = img_data[:,:,0], img_data[:,:,1], img_data[:,:,2]\n",
        "    imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.imshow(imgGray,cmap='gray')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    nrows, ncols = np.shape(imgGray)\n",
        "    middle_column = np.uint(ncols/2)\n",
        "    middle_row = np.uint(nrows/2)\n",
        "\n",
        "    #find the 0 gaps before and after the image on the column\n",
        "    data_strip = imgGray[:,middle_column]\n",
        "    index = np.where(data_strip == 0) #find first section of 0 values\n",
        "    print('shape of index= ',np.shape(index))\n",
        "    print('shape of data strip = ',np.shape(data_strip))\n",
        "    print('data_strip 0:20 ',data_strip[0:20])\n",
        "    print('index 0:20 ', index[0][:])\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.plot(data_strip)\n",
        "    plt.ylabel('intensity')\n",
        "    plt.xlabel('rows')\n",
        "    ttext = 'Strip Slice at column ' +  str(middle_column)\n",
        "    plt.title(ttext)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(data_strip[400:500])\n",
        "    plt.title('zoomed')\n",
        "\n",
        "\n",
        "\n",
        "    first_row_gap = index[0][0]\n",
        "    diff_row = np.diff(index[0])\n",
        "\n",
        "    print('diff ' , diff_row)\n",
        "\n",
        "    for counter,ii in enumerate(diff_row):\n",
        "        if (counter > 2):\n",
        "            #find a set of diff 0s that skip a distance (edge)\n",
        "            if ( (diff_row[counter-3] <5 ) and\n",
        "                (diff_row[counter-2] <5) and\n",
        "                (diff_row[counter-1] <5) and\n",
        "                (ii> 50)):\n",
        "                print('found an edge at ', counter,ii, index[0][counter])\n",
        "                second_row_gap = index[0][counter]\n",
        "                break #only need first point\n",
        "\n",
        "    #find the large jump in diff, that will likely be US image then down to background 0\n",
        "    #second_row_gap = np.where(diff_row >= 150)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('Second Row Gap ',second_row_gap)\n",
        "    plt.figure()\n",
        "    plt.plot(data_strip,'r.-')\n",
        "\n",
        "\n",
        "\n",
        "    cropped_image, skip_points = crop_us_image(imgGray,1)\n",
        "    plt.figure()\n",
        "    plt.imshow(cropped_image,cmap='gray')\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjiPjns1ekPQ"
      },
      "source": [
        "### test code\n",
        "if(0):\n",
        "    '''\n",
        "    DOES NOT WORK!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    '''\n",
        "    import cv2\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    idata = '/content/gdrive/Shareddrives/BreastUS/Annotated data/unzipped_updated/1_2w948u10_a_06g91d42_4.mp4-2021_08_20_17_24_25-labelme 3.0.zip/default/frame_000159.PNG'\n",
        "\n",
        "    import cv2\n",
        "\n",
        "    # Load the image\n",
        "    img = cv2.imread(idata)\n",
        "\n",
        "    # convert to grayscale\n",
        "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    edged = cv2.Canny(img, 170, 490)\n",
        "    # Apply adaptive threshold\n",
        "    thresh = cv2.adaptiveThreshold(edged, 1, 1, 1, 11, 2)\n",
        "    thresh_color = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # apply some dilation and erosion to join the gaps - change iteration to detect more or less area's\n",
        "    thresh = cv2.dilate(thresh,None,iterations = 15)\n",
        "    thresh = cv2.erode(thresh,None,iterations = 15)\n",
        "\n",
        "    # Find the contours\n",
        "    contours,hierarchy = cv2.findContours(thresh,\n",
        "                                            cv2.RETR_TREE,\n",
        "                                            cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # For each contour, find the bounding rectangle and draw it\n",
        "    for cnt in contours:\n",
        "        x,y,w,h = cv2.boundingRect(cnt)\n",
        "        cv2.rectangle(img,\n",
        "                        (x,y),(x+w,y+h),\n",
        "                        (0,255,0),\n",
        "                        2)\n",
        "\n",
        "    cv2_imshow(img)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N19fiKU3fE7t"
      },
      "source": [
        "if(0):\n",
        "    #\n",
        "    # Test images as 3d volumes\n",
        "    #\n",
        "    for counter, ii in enumerate(file_name):\n",
        "        img_data = image.imread(ii)\n",
        "        print(np.shape(img_data))\n",
        "        if (counter >0):\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "        ## Convert the RGB input into grayscale\n",
        "        R, G, B = img_data[:,:,0], img_data[:,:,1], img_data[:,:,2]\n",
        "        imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "        cropped_image = crop_us_image(imgGray)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        #img = cv2.imread('your_image.jpg')\n",
        "        cropped_image = cv2.resize(cropped_image, dsize=(600, 480), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        nr,nc = np.shape(cropped_image)\n",
        "        img_volume = np.zeros((3,480,600)) #nr,nc))\n",
        "        img_volume[0,:,:] = cropped_image\n",
        "        img_volume[1,:,:] = cropped_image\n",
        "        img_volume[2,:,:] = cropped_image\n",
        "        \n",
        "        \n",
        "        plt.figure(figsize=(8, 6), dpi=80)\n",
        "        plt.imshow(img_volume[0,:,:], cmap='gray')\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrkfWeZJB7Iv"
      },
      "source": [
        "# Bounding Box information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB8180Q-rGe7",
        "outputId": "f2906883-e7e9-452b-f505-6b4d12a2225d"
      },
      "source": [
        "#\n",
        "# Bounding box info\n",
        "#\n",
        "import torch\n",
        "print('-------')\n",
        "uvids = set(video_id)\n",
        "\n",
        "for v in uvids:\n",
        "    for count,ii in enumerate(video_id):\n",
        "        if (v in ii):\n",
        "            #get bb info\n",
        "            box_info = bounding_box[count]\n",
        "            #box_info = box_info.strip('][') #.split(', ')\n",
        "\n",
        "            if (not (box_info =='[]')):\n",
        "                \n",
        "                corners=literal_eval(box_info)\n",
        "                bbox = torch.FloatTensor(corners)\n",
        "                #print(corners)\n",
        "            else:\n",
        "                continue\n",
        "            "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNbFf0FAskGM"
      },
      "source": [
        "def get_bb_stats(bounding_box):\n",
        "    total_area=[]\n",
        "    num_box=0\n",
        "    num_h=[]\n",
        "    num_w=[]\n",
        "    for index in range(0,len(bounding_box)):\n",
        "\n",
        "        corners=literal_eval(bounding_box[index])\n",
        "\n",
        "        boxes = []\n",
        "        area = 0\n",
        "        pos=literal_eval(bounding_box[index])\n",
        "\n",
        "        #pos = np.double(pos)\n",
        "        if (len(pos) !=0): #(pos):\n",
        "            #print('corners ',pos)\n",
        "            pos = np.int32(pos)\n",
        "            xmin = pos[0][1]\n",
        "            xmax = pos[2][1]\n",
        "            ymin = pos[0][0]\n",
        "            ymax = pos[1][0]\n",
        "            w=xmax-xmin\n",
        "            h = ymax-ymin\n",
        "            num_h.append(h)\n",
        "            num_w.append(w)\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            area += (xmax-xmin)*(ymax-ymin)\n",
        "            total_area.append(area)\n",
        "            num_box+=1\n",
        "    return total_area, num_box,num_h, num_w"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EoFrtsmB_PNG",
        "outputId": "42a9c3a1-2590-41f5-ee7a-d9a97a875163"
      },
      "source": [
        "#bounding_box\n",
        "ta,box_num,num_h, num_w=get_bb_stats(bounding_box)\n",
        "\n",
        "avg_area = sum(ta)/len(ta)\n",
        "print(avg_area)\n",
        "\n",
        "#avg width\n",
        "print(np.average(num_w))\n",
        "#get smallest box\n",
        "\n",
        "print('smallest width is ', np.min(num_w))\n",
        "print('largest width is  ', np.max(num_w))\n",
        "\n",
        "#avg height\n",
        "print(np.average(num_h))\n",
        "print('smallest height is ', np.min(num_h))\n",
        "print('largest height is  ',np.max(num_h))\n",
        "\n",
        "vv,_,_=plt.hist(num_h, bins = 50)\n",
        "plt.title('h')\n",
        "print('-----',vv)\n",
        "\n",
        "for ii in range(0,5):\n",
        "    bin_max = np.max(vv[ii*10:ii*10+9])\n",
        "    print('max Height bin is ', ii,bin_max)\n",
        "\n",
        "vv,_,_=plt.hist(num_w, bins = 50)\n",
        "plt.title('w')\n",
        "print('-----',vv)\n",
        "\n",
        "for ii in range(0,5):\n",
        "    bin_max = np.max(vv[ii*10:ii*10+9])\n",
        "    print('max Width bin is ', ii,bin_max)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(num_h, bins=50,alpha=0.5, label=\"Heights\")\n",
        "plt.hist(num_w,bins=50,alpha=0.5, label=\"Widths\")\n",
        "plt.title('Histogram of Annotation Heights')\n",
        "plt.ylabel('Number of Occurences')\n",
        "plt.xlabel('Annotation Length (pixels)')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(num_w,bins=50,alpha=0.5, label=\"Widths\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57268.8385847562\n",
            "171.43091005104736\n",
            "smallest width is  55\n",
            "largest width is   359\n",
            "276.0306284104911\n",
            "smallest height is  47\n",
            "largest height is   574\n",
            "----- [124.   0.   0.  66.  26. 120. 143. 317. 183. 288. 186. 369. 109. 232.\n",
            " 268. 232. 161. 180.   0.  48.  29. 207.  89. 115.   6. 280.  92.  29.\n",
            "  81.   1.   2.  87. 232. 168.   9. 164.  56.  82.  50.   0. 125.  85.\n",
            " 104.   0. 178.   0.  68. 161.  55.  74.]\n",
            "max Height bin is  0 317.0\n",
            "max Height bin is  1 369.0\n",
            "max Height bin is  2 280.0\n",
            "max Height bin is  3 232.0\n",
            "max Height bin is  4 178.0\n",
            "----- [176.   0. 212.  98.  39. 245. 178. 116. 281. 137. 250. 157. 102. 180.\n",
            " 347. 296. 432. 309.  91.   4.   0.  88. 169.  81.   0. 220. 202. 163.\n",
            "   0.  88.  87.  60.   0.   0.   0.   0.   0.   0.   0.   0. 125. 233.\n",
            "  85.  91.   0.  83.   0. 178.   0.  78.]\n",
            "max Width bin is  0 281.0\n",
            "max Width bin is  1 432.0\n",
            "max Width bin is  2 220.0\n",
            "max Width bin is  3 87.0\n",
            "max Width bin is  4 233.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([176.,   0., 212.,  98.,  39., 245., 178., 116., 281., 137., 250.,\n",
              "        157., 102., 180., 347., 296., 432., 309.,  91.,   4.,   0.,  88.,\n",
              "        169.,  81.,   0., 220., 202., 163.,   0.,  88.,  87.,  60.,   0.,\n",
              "          0.,   0.,   0.,   0.,   0.,   0.,   0., 125., 233.,  85.,  91.,\n",
              "          0.,  83.,   0., 178.,   0.,  78.]),\n",
              " array([ 55.  ,  61.08,  67.16,  73.24,  79.32,  85.4 ,  91.48,  97.56,\n",
              "        103.64, 109.72, 115.8 , 121.88, 127.96, 134.04, 140.12, 146.2 ,\n",
              "        152.28, 158.36, 164.44, 170.52, 176.6 , 182.68, 188.76, 194.84,\n",
              "        200.92, 207.  , 213.08, 219.16, 225.24, 231.32, 237.4 , 243.48,\n",
              "        249.56, 255.64, 261.72, 267.8 , 273.88, 279.96, 286.04, 292.12,\n",
              "        298.2 , 304.28, 310.36, 316.44, 322.52, 328.6 , 334.68, 340.76,\n",
              "        346.84, 352.92, 359.  ]),\n",
              " <a list of 50 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPBUlEQVR4nO3df6zddX3H8edrBcFMZwUa0rTNihNmyLIh6RhGYwyEDdiyugQNy6KN6dJkw0Rxy8CZbJpsiS6Z3UyMphvM6ozA0IXGuGwdP2L2B2iZgECHXH+FNkirAmqMbuh7f5xP9XC95/7ovff8+PT5SE7u9/v5fu8578/9nr76OZ/zPd+TqkKS1Jefm3QBkqS1Z7hLUocMd0nqkOEuSR0y3CWpQ6dNugCAc845p7Zv3z7pMiRpptx///3frKpNC22binDfvn07hw4dmnQZkjRTknx91DanZSSpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUNT8QlVzb69B7+0YPv1V1ww5kokgSN3SeqS4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ8sO9yQbknwhyafb+nlJ7ksyl+TWJC9o7We09bm2ffv6lC5JGmUlI/e3AYeH1t8H7K2qlwNPA7tb+27g6da+t+0nSRqjZYV7kq3AbwP/2NYDXAbc3nbZD7y+Le9s67Ttl7f9JUljstyR+98Bfwb8uK2fDTxTVc+19SPAlra8BXgCoG1/tu3/PEn2JDmU5NDx48dPsnxJ0kKWDPckvwMcq6r71/KBq2pfVe2oqh2bNm1ay7uWpFPecr6s49XA7ya5GjgT+AXg74GNSU5ro/OtwNG2/1FgG3AkyWnAS4BvrXnlkqSRlhy5V9U7q2prVW0HrgXuqqo/AO4Grmm77QLuaMsH2jpt+11VVWtatSRpUas5z/0G4B1J5hjMqd/U2m8Czm7t7wBuXF2JkqSVWtF3qFbVPcA9bfkrwCUL7PMD4A1rUJsk6ST5CVVJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQyv6DlWdOvYe/NKC7ddfccGYK5F0Mhy5S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkF+zp3Xl1/VJk7HkyD3JmUk+l+TBJI8keU9rPy/JfUnmktya5AWt/Yy2Pte2b1/fLkiS5lvOtMwPgcuq6teAi4Ark1wKvA/YW1UvB54Gdrf9dwNPt/a9bT9J0hgtGe418L22enq7FXAZcHtr3w+8vi3vbOu07ZcnyZpVLEla0rLeUE2yIckDwDHgIPBl4Jmqeq7tcgTY0pa3AE8AtO3PAmcvcJ97khxKcuj48eOr64Uk6XmWFe5V9aOqugjYClwCvGK1D1xV+6pqR1Xt2LRp02rvTpI0ZEVny1TVM0nuBl4FbExyWhudbwWOtt2OAtuAI0lOA14CfGsNa9YEjTr7RdJ0Wc7ZMpuSbGzLLwSuAA4DdwPXtN12AXe05QNtnbb9rqqqtSxakrS45YzcNwP7k2xg8J/BbVX16SSPArck+SvgC8BNbf+bgI8lmQO+DVy7DnVLkhaxZLhX1UPAKxdo/wqD+ff57T8A3rAm1UmSToqXH5CkDhnuktQhw12SOuSFw6aAF9eStNYcuUtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDnnJ3xnkJYIlLcWRuyR1yJH7KjiCljStHLlLUocMd0nqkOEuSR0y3CWpQ4a7JHXIs2UkTT3PTFs5R+6S1CHDXZI6ZLhLUocMd0nqkOEuSR3ybBlpEaPO0gDP1NB0c+QuSR0y3CWpQ4a7JHXolJtzdw5V0qnAkbskdchwl6QOLRnuSbYluTvJo0keSfK21n5WkoNJHm8/X9rak+QDSeaSPJTk4vXuhCTp+ZYzcn8O+JOquhC4FLguyYXAjcCdVXU+cGdbB7gKOL/d9gAfWvOqJUmLWvIN1ap6EniyLX83yWFgC7ATeF3bbT9wD3BDa/9oVRVwb5KNSTa3+5G64WVoNc1WNOeeZDvwSuA+4NyhwP4GcG5b3gI8MfRrR1rb/Pvak+RQkkPHjx9fYdmSpMUsO9yTvAj4JPD2qvrO8LY2Sq+VPHBV7auqHVW1Y9OmTSv5VUnSEpYV7klOZxDsH6+qT7Xmp5Jsbts3A8da+1Fg29Cvb21tkqQxWc7ZMgFuAg5X1fuHNh0AdrXlXcAdQ+1vbmfNXAo863y7JI3Xcj6h+mrgTcAXkzzQ2v4ceC9wW5LdwNeBN7ZtnwGuBuaA7wNvWdOKJUlLWs7ZMv8FZMTmyxfYv4DrVlmXJGkV/ISqJHXIcJekDhnuktQhw12SOnTKXc99HGblY+mLXdte0miz8G985sN9Fv7IkjRuTstIUocMd0nqkOEuSR2a+Tl3aSG+F6NTnSN3SeqQ4S5JHXJaRlpjTglpGjhyl6QOGe6S1CGnZTridICkExy5S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjrkh5immN9xKulkOXKXpA4Z7pLUIcNdkjpkuEtShwx3SeqQZ8tImhqeIbZ2HLlLUocMd0nqkOEuSR1yzn2MnE+UNC6O3CWpQ4a7JHXIcJekDhnuktShJcM9yc1JjiV5eKjtrCQHkzzefr60tSfJB5LMJXkoycXrWbwkaWHLGbl/BLhyXtuNwJ1VdT5wZ1sHuAo4v932AB9amzIlSSux5KmQVfXZJNvnNe8EXteW9wP3ADe09o9WVQH3JtmYZHNVPblWBWvlPAVTOvWc7Jz7uUOB/Q3g3La8BXhiaL8jre1nJNmT5FCSQ8ePHz/JMiRJC1n1h5iqqpLUSfzePmAfwI4dO1b8++th1Aj3+isuGHMlkrQ6JztyfyrJZoD281hrPwpsG9pva2uTJI3RyYb7AWBXW94F3DHU/uZ21sylwLPOt0vS+C05LZPkEwzePD0nyRHgL4H3Arcl2Q18HXhj2/0zwNXAHPB94C3rULM64BSY1pPPr+WdLfP7IzZdvsC+BVy32qIkSavjVSE103o4zXOlfTiVRp86eYa7pJnVw3/u68Vwl3TK63GO3nDXKaXHf8TSQgx3SRqDcQ8svOSvJHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUN+iEmaMX7KVsvhyF2SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjrkJ1SXwW9YlzRrHLlLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUoXUJ9yRXJnksyVySG9fjMSRJo635l3Uk2QB8ELgCOAJ8PsmBqnp0rR9L/Rn1xSjXX3HBmCuRZtt6jNwvAeaq6itV9b/ALcDOdXgcSdIIqaq1vcPkGuDKqvrDtv4m4Deq6q3z9tsD7Gmrvww8toy7Pwf45hqWOyn2Y7r00g/opy/2Y3l+sao2LbRhYt+hWlX7gH0r+Z0kh6pqxzqVNDb2Y7r00g/opy/2Y/XWY1rmKLBtaH1ra5Mkjcl6hPvngfOTnJfkBcC1wIF1eBxJ0ghrPi1TVc8leSvw78AG4OaqemSN7n5F0zhTzH5Ml176Af30xX6s0pq/oSpJmjw/oSpJHTLcJalDUx3uSb6W5ItJHkhyqLWdleRgksfbz5dOus75ktyc5FiSh4faFqw7Ax9ol2p4KMnFk6v8+Ub0491JjrZj8kCSq4e2vbP147EkvzWZqn9Wkm1J7k7yaJJHkryttc/UMVmkHzN1TJKcmeRzSR5s/XhPaz8vyX2t3lvbCRkkOaOtz7Xt2ydZ/wmL9OMjSb46dDwuau3jfV5V1dTegK8B58xr+xvgxrZ8I/C+Sde5QN2vBS4GHl6qbuBq4N+AAJcC9026/iX68W7gTxfY90LgQeAM4Dzgy8CGSfeh1bYZuLgtvxj4Uqt3po7JIv2YqWPS/q4vasunA/e1v/NtwLWt/cPAH7XlPwY+3JavBW6ddB+W6MdHgGsW2H+sz6upHrmPsBPY35b3A6+fYC0LqqrPAt+e1zyq7p3AR2vgXmBjks3jqXRxI/oxyk7glqr6YVV9FZhjcCmKiauqJ6vqv9vyd4HDwBZm7Jgs0o9RpvKYtL/r99rq6e1WwGXA7a19/vE4cZxuBy5PkjGVO9Ii/RhlrM+raQ/3Av4jyf3tcgUA51bVk235G8C5kyltxUbVvQV4Ymi/Iyz+D3YavLW9rLx5aFpsJvrRXtK/ksEoa2aPybx+wIwdkyQbkjwAHAMOMnhV8UxVPdd2Ga71J/1o258Fzh5vxQub34+qOnE8/rodj71JzmhtYz0e0x7ur6mqi4GrgOuSvHZ4Yw1e68zcuZyzWnfzIeCXgIuAJ4G/nWw5y5fkRcAngbdX1XeGt83SMVmgHzN3TKrqR1V1EYNPsF8CvGLCJZ2U+f1I8ivAOxn059eBs4AbJlHbVId7VR1tP48B/8rgSfDUiZcy7eexyVW4IqPqnqnLNVTVU+0J/WPgH/jpy/yp7keS0xkE4ser6lOteeaOyUL9mNVjAlBVzwB3A69iME1x4oOVw7X+pB9t+0uAb4251EUN9ePKNn1WVfVD4J+Y0PGY2nBP8vNJXnxiGfhN4GEGlzLY1XbbBdwxmQpXbFTdB4A3t3fSLwWeHZoqmDrz5gh/j8ExgUE/rm1nNpwHnA98btz1LaTNz94EHK6q9w9tmqljMqofs3ZMkmxKsrEtv5DBdz8cZhCO17Td5h+PE8fpGuCu9kprokb043+GBgxh8L7B8PEY3/NqPd+tXc0NeBmDd/ofBB4B3tXazwbuBB4H/hM4a9K1LlD7Jxi8PP4/BvNqu0fVzeCd8w8ymHP8IrBj0vUv0Y+PtTofak/WzUP7v6v14zHgqknXP1TXaxhMuTwEPNBuV8/aMVmkHzN1TIBfBb7Q6n0Y+IvW/jIG//nMAf8CnNHaz2zrc237yybdhyX6cVc7Hg8D/8xPz6gZ6/PKyw9IUoemdlpGknTyDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUof8HmlkYkqhksCYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM1oXD-itHex",
        "outputId": "997bea82-c310-437b-f551-384da4f70f99"
      },
      "source": [
        "#generate stats on bounding boxes\n",
        "uvids = set(video_id)\n",
        "\n",
        "box_collect=[]\n",
        "frames_collect=[]\n",
        "empty_annotations=[]\n",
        "for count, v in enumerate(uvids):\n",
        "    number_frames = 0\n",
        "    number_boxes = 0\n",
        "    \n",
        "    #print('---- loading: ', v)\n",
        "    for icount,ii in enumerate(video_id):\n",
        "        if (ii == v):\n",
        "            box_info = bounding_box[icount]\n",
        "            number_frames+=1\n",
        "            if (not (box_info =='[]')):\n",
        "                number_boxes+=1\n",
        "            else:\n",
        "                pass\n",
        "                #print('no annotation', frame_id[icount])\n",
        "    frames_collect.append(number_frames)\n",
        "    box_collect.append(number_boxes)\n",
        "    #print(v,number_frames,number_boxes)\n",
        "    if (number_boxes == 0):\n",
        "        empty_annotations.append(v)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Number of videos: ',np.size(frames_collect[:]))\n",
        "print('Number of annotated cases ',np.size(box_collect))\n",
        "print('Videos with no Annotations:', empty_annotations)\n",
        "    #get_bb_stats(v, video_id,bounding_box, image_path)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of videos:  102\n",
            "Number of annotated cases  102\n",
            "Videos with no Annotations: ['1_2w948u10_a_06g91d42_5.mp4-2021_07_15_17_21_37-labelme 3.0.zip', '1_srh8gfhj_a_0629s3fh_0.mp4-2021_07_15_17_21_15-labelme 3.0.zip', '1_2w948u10_a_06g91d42_2.mp4-2021_07_15_17_21_25-labelme 3.0.zip', '1_srh8gfhj_a_0629s3fh_3.mp4-2021_07_15_17_21_17-labelme 3.0.zip', '1_jq3lf35n_a_12h3r1u8_5.mp4-2021_07_15_17_21_17-labelme 3.0.zip', '1_2w948u10_a_06g91d42_0.mp4-2021_07_15_17_21_18-labelme 3.0.zip', '1_jq3lf35n_a_12h3r1u8_0.mp4-2021_07_15_17_21_32-labelme 3.0.zip', '1_jq3lf35n_a_12h3r1u8_1.mp4-2021_07_15_17_21_37-labelme 3.0.zip', '1_jq3lf35n_a_12h3r1u8_4.mp4-2021_07_15_17_21_22-labelme 3.0.zip', '1_jq3lf35n_a_12h3r1u8_2.mp4-2021_07_15_17_21_26-labelme 3.0.zip', '1_en9w0q3l_a_e128787p_2.mp4-2021_09_22_21_42_11-labelme 3.0.zip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxsXie0YGRUg"
      },
      "source": [
        "#GET COORDINATES FROM CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c-aUiEvo7RW"
      },
      "source": [
        "def get_coordinates(video_id,video_series):\n",
        "#video_series is the video_id for a series of PNG images taken from a movie.\n",
        "#Anything with this id will be a frame set that should be kept together\n",
        "\n",
        "#uvids = set(video_id)\n",
        "\n",
        "    bbox = []\n",
        "    for count,v in enumerate(video_id): #video_series:\n",
        "        if (v in video_series):\n",
        "            box_info = bounding_box[count]\n",
        "            #box_info = box_info.strip('][') #.split(', ')\n",
        "\n",
        "            if (not (box_info =='[]')):\n",
        "                \n",
        "                corners=literal_eval(box_info)\n",
        "                bbox.append(torch.FloatTensor(corners))\n",
        "                print(corners)\n",
        "            else:\n",
        "                bbox.append(torch.FloatTensor(0))\n",
        "\n",
        "    return bbox\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV73Fx3_s32K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215dc24a-f470-44aa-a91a-ad9bbc1b6f43"
      },
      "source": [
        "video_id[0]\n",
        "boxes = get_coordinates(video_id,video_id[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n",
            "[(274, 197), (615, 197), (615, 395), (274, 395)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWp6S054HoNv"
      },
      "source": [
        "#                         **** Data Loaders ****\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxErahOxJmLb"
      },
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 img_dir,\n",
        "                 label_data,\n",
        "                 category=[],\n",
        "                 file_count=1,\n",
        "                 file_list =[],\n",
        "                 transform=None,\n",
        "                 target_transform=None):\n",
        "        #self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.category = category\n",
        "        self.file_count = file_count\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.category_name =''\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "        self.imgs = file_list\n",
        "\n",
        "\n",
        "        '''\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "        '''\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image_dir = self.img_dir\n",
        "\n",
        "        file_name = self.file_list[index]\n",
        "\n",
        "\n",
        "        img_data = image.imread(file_name)\n",
        "        print('image shape read in is ',np.shape(img_data))\n",
        "\n",
        "\n",
        "        ## Convert the RGB input into grayscale\n",
        "        R, G, B = img_data[:,:,0], img_data[:,:,1], img_data[:,:,2]\n",
        "        imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "        cropped_image, skip_points,ff = crop_us_image(imgGray,0)\n",
        "        if (np.size(cropped_image) <1):\n",
        "            print('!!!!!!!!!!!!!!!!!!!Failure during cropping for ', file_name)\n",
        "            #we should exit out, but put a fake image for now\n",
        "            cropped_image = np.zeros((400,600))\n",
        "            print('!!!!!!!!!!!!!!!!!!! ZERO IMAGE PASSED IN')\n",
        "        print('about to resize image. shape going in is @',np.shape(cropped_image), index)\n",
        "        cropped_image = np.array(cropped_image)\n",
        "        print('image after crop is ', np.shape(cropped_image))\n",
        "        #if(type(image) == type(None)):\n",
        "        #    pass\n",
        "        #else:\n",
        "        #    image = cv2.resize(image, (h, w), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        cropped_image = cv2.resize(cropped_image,\n",
        "                                   dsize=(800,600), #(480,600)\n",
        "                                   interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        nr,nc = np.shape(cropped_image)\n",
        "        print('resized shapes are ',nr,nc)\n",
        "        '''\n",
        "        img = np.zeros((3,600,480), dtype=torch.DoubleTensor) #nr,nc))\n",
        "        img[0,:,:] = cropped_image.astype(torch.DoubleTensor)\n",
        "        img[1,:,:] = cropped_image.astype(torch.DoubleTensor)\n",
        "        img[2,:,:] = cropped_image.astype(torch.DoubleTensor)\n",
        "        '''\n",
        "\n",
        "\n",
        "        img = np.zeros((3,600,800), dtype=np.double)  #600,480), dtype=np.double) #nr,nc))\n",
        "        img[0,:,:] = cropped_image#.astype(np.double)\n",
        "        img[1,:,:] = cropped_image#.astype(np.double)\n",
        "        img[2,:,:] = cropped_image#.astype(np.double)\n",
        "        img=img.astype(np.double)\n",
        "\n",
        "        #img=torch.from_numpy(img)\n",
        "\n",
        "        #get label and pull category \n",
        "        #if (first50[index] == 'M'):\n",
        "        #    label = torch.as_tensor(1, dtype=torch.int64)\n",
        "        #else:\n",
        "        #    label = torch.as_tensor(0, dtype=torch.int64)\n",
        "            #label = torch.zeros([0], dtype=torch.int64)\n",
        "            \n",
        "        corners=literal_eval(bounding_box[index])\n",
        "                #bbox = torch.FloatTensor(corners)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(self.file_list) #bounding_box)\n",
        "        boxes = []\n",
        "        #area = 0\n",
        "        pos=literal_eval(bounding_box[index])\n",
        "\n",
        "\n",
        "        #if (len(corners)>0):\n",
        "        #    width = xmax-xmin\n",
        "        #    height = ymax-ymin\n",
        "        #    rect = patches.Rectangle((offset_col,offset_row),width,height,linewidth=1,edgecolor='r',facecolor='none')\n",
        "        #skip_points = [low_row, high_row,start_column, final_column]\n",
        "\n",
        "        #pos = np.double(pos)\n",
        "        if (len(pos) !=0): #(pos):\n",
        "            print('corners ',pos)\n",
        "            pos = np.int32(pos)\n",
        "            xmin = pos[0][0]\n",
        "            xmax = pos[1][0]\n",
        "            ymin = pos[0][1]\n",
        "            ymax = pos[2][1]\n",
        "            ### Correct for the cropped image as annotation points are for the \n",
        "            ### main image with subset\n",
        "            offset_row = ymin-skip_points[0]\n",
        "            offset_col = xmin-skip_points[2]\n",
        "\n",
        "            boxes.append([offset_col, offset_row, \n",
        "                          xmax-skip_points[2], \n",
        "                          ymax-skip_points[0]])\n",
        "            if (((xmax-skip_points[0]) < offset_col) or ((ymax-skip_points[2]) < offset_row)):\n",
        "                print('found a negative box!! ',  boxes)\n",
        "                print('skip points: ',skip_points)\n",
        "                print('offset col, row ',offset_col, offset_row)\n",
        "            #boxes.append([xmin, ymin, xmax, ymax])\n",
        "            area = (xmax-skip_points[0] - offset_col) * (ymax-skip_points[2] - offset_row) \n",
        "            #area += (xmax-xmin)*(ymax-ymin)#!! alter this\n",
        "        '''\n",
        "        else:\n",
        "            #empty_box = torch.zeros([0, 4)], dtype=torch.double)\n",
        "            #print('EMPTY bounding box')\n",
        "            xmin=np.int32(0) #torch.DoubleTensor(3)\n",
        "            xmax=np.int32(1) #torch.DoubleTensor(5)\n",
        "            ymin=np.int32(0) #torch.DoubleTensor(3)\n",
        "            ymax=np.int32(1) #torch.DoubleTensor(5)\n",
        "            boxes.append([0,0,0,0]) #[xmin, ymin, xmax, ymax])\n",
        "        '''\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        # Handle empty bounding boxes\n",
        "        if (len(pos) ==0): #num_objs == 0:\n",
        "            #boxes.append([0,0,0,0])\n",
        "            boxes.append([0,0,5,5]) #testing out something with an area\n",
        "\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.double)\n",
        "            #boxes = torch.zeros((0, 4), dtype=torch.double)\n",
        "            #area = 0 #10*10\n",
        "            area = torch.as_tensor(25, dtype=torch.double)\n",
        "\n",
        "            #label = torch.as_tensor(0, dtype=torch.int64) #put as background if no boxes\n",
        "            label = torch.zeros((1,), dtype=torch.int64)\n",
        "\n",
        "        else:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.double) \n",
        "            area = torch.as_tensor(area, dtype=torch.double)\n",
        "        \n",
        "            #label = torch.as_tensor(1, dtype=torch.int64)\n",
        "            label = torch.ones((1,), dtype=torch.int64)\n",
        "\n",
        "\n",
        "\n",
        "        print('shape of boxes is ', np.shape(boxes))\n",
        "\n",
        "\n",
        "        # there is only one class\n",
        "        #labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        #masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        #image_id = torch.tensor([idx])\n",
        "        #area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((len(self.imgs),), dtype=torch.double)\n",
        "\n",
        "        print('setting target')\n",
        "        print('label is ', label, type(label))\n",
        "        print('area is ', area)\n",
        "        \n",
        "\n",
        "\n",
        "        target = []\n",
        "        d = {}\n",
        "        d['boxes'] = boxes  #np.squeeze(boxes,0)\n",
        "        d['labels'] = label\n",
        "        d['image_id'] = torch.as_tensor(index, dtype=torch.double) \n",
        "        d['area'] = area \n",
        "        d['iscrowd'] = iscrowd \n",
        "        target.append(d)\n",
        "\n",
        "\n",
        "        '''\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = label\n",
        "        #target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = index #image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        '''\n",
        "        #self.transform(self.x_data[index]), self.transform(self.y_data[index])\n",
        "        #return {'image': torch.from_numpy(image),\n",
        "#                'landmarks': torch.from_numpy(landmarks)}\n",
        "        #return self.transform(img), self.transform(target)\n",
        "        \n",
        "        #print('image type before is ', type(img))\n",
        "        img = torch.as_tensor(img, dtype=torch.float32) #model has float32 \n",
        "        #img= torch.from_numpy(img)\n",
        "        #print('image type after is ', type(img))\n",
        "        #img = torchvision.transforms.ToTensor()(image)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI_gE6lyWv_y"
      },
      "source": [
        "#GENERATE ALL FILE PATHS "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "211LJzRiN9a1"
      },
      "source": [
        "full_file_list=[]\n",
        "for file_path in image_path:\n",
        "    #print(file_path)\n",
        "\n",
        "    filename = os.path.basename(file_path) \n",
        "    [_,fpath] =file_path.split('drive/MyDrive/Annotated data/')\n",
        "    full_file = os.path.join(annotated_dir,fpath)\n",
        "    full_file_list.append(full_file)\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amG8T6UodFdV"
      },
      "source": [
        "#SPLIT INTO TRAIN/VAL/TEST FOLDERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEBbhgjdc_FZ"
      },
      "source": [
        "#break into a training and validation set\n",
        "'''\n",
        "array_rows = np.array(annotation_rows)\n",
        "mrn = array_rows[:,1]\n",
        "accession = array_rows[:,2]\n",
        "video_id = array_rows[:,3]\n",
        "frame_id = array_rows[:,4]\n",
        "image_path = array_rows[:,5]\n",
        "bounding_box = array_rows[:,6]\n",
        "diagnosis = array_rows[:,7]\n",
        "biopsy_site = array_rows[:,8]\n",
        "diagnosis2 = array_rows[:,9]\n",
        "first50 = array_rows[:,10]\n",
        "'''\n",
        "types_acc = np.unique(accession)\n",
        "num_acc = len(types_acc)\n",
        "print('number of unique accs is ', num_acc, len(accession))\n",
        "\n",
        "unique_vidid = np.unique(video_id)\n",
        "\n",
        "train_pct = 0.8\n",
        "val_pct = 0.1\n",
        "test_pct = 1-train_pct-val_pct\n",
        "\n",
        "\n",
        "cancer_status ={}\n",
        "for vname in unique_vidid:\n",
        "    for pcount, pathname in enumerate(image_path):\n",
        "        if (vname in pathname):\n",
        "            cancer_status[vname] = first50[pcount]\n",
        "            break\n",
        "\n",
        "print(cancer_status)\n",
        "print(cancer_status.values())\n",
        "\n",
        "vid_vals = list(cancer_status.keys())\n",
        "cvals = list(cancer_status.values())\n",
        "print(cvals)\n",
        "print(vid_vals)\n",
        "\n",
        "\n",
        "#Remove the bad counters \n",
        "bad_files = os.path.join(local_dir,'counters_to_remove.pickle')\n",
        "bad_count = pickle.load( open( bad_files, \"rb\" ) )\n",
        "print(len(bad_count[0]))\n",
        "\n",
        "bad_vidid = np.unique(video_id[bad_count[0]])\n",
        "print('bad video ids: ', bad_vidid)\n",
        "\n",
        "\n",
        "good_list=[]\n",
        "\n",
        "for counter,ii in enumerate(image_path):\n",
        "    skip_me = 0\n",
        "    for jj in bad_vidid:\n",
        "        if(jj in video_id[counter]):\n",
        "            print('bad video id found: ',ii)\n",
        "            skip_me=1\n",
        "            break\n",
        "\n",
        "    if (skip_me == 1):\n",
        "        good_list.append(counter)\n",
        "\n",
        "\n",
        "print(len(bad_vidid))\n",
        "print('len of image path is ', len(image_path), len(bad_vidid), len(bad_count[0]))\n",
        "print('len good list: ',len(good_list))\n",
        "\n",
        "print(image_path[good_list[0]])\n",
        "print('bad count = ', bad_count[0][80:90])\n",
        "print(image_path[bad_count[0][500]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44VxBT_tRpQ_"
      },
      "source": [
        "#REMOVE PRE-DETERMINED LIST OF UNCROPPABLE IMAGES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ7b9QluP_Qg"
      },
      "source": [
        "#Remove the bad counters \n",
        "bad_files = os.path.join(local_dir,'counters_to_remove.pickle')\n",
        "bad_count = pickle.load( open( bad_files, \"rb\" ) )\n",
        "print(len(bad_count[0]))\n",
        "\n",
        "new_flist =[]\n",
        "new_bb = []\n",
        "new_first50=[]\n",
        "for counter, fname in enumerate(full_file_list):\n",
        "    if counter in bad_count[0]:\n",
        "        print('found bad counter ',counter)\n",
        "    else:\n",
        "        new_flist.append(fname)\n",
        "        new_bb.append(bounding_box[counter])\n",
        "        new_first50.append(first50[counter])\n",
        "\n",
        "full_file_list = new_flist\n",
        "bounding_box = new_bb\n",
        "first50=new_first50\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ABHNGsElYCU"
      },
      "source": [
        "num_train = np.uint16(np.floor(train_pct*len(unique_vidid)))\n",
        "num_val = np.floor(val_pct*len(unique_vidid))\n",
        "\n",
        "train_list = []\n",
        "val_list = []\n",
        "train_labels=[]\n",
        "val_labels =[]\n",
        "for counter, filename in enumerate(full_file_list):\n",
        "    for jcount,jj in enumerate(unique_vidid):\n",
        "        if((jj in filename) and (jcount <=num_train)):\n",
        "            train_list.append(filename)\n",
        "            train_labels.append(first50[counter])\n",
        "        else:\n",
        "            val_list.append(filename)\n",
        "            val_labels.append(first50[counter])\n",
        "\n",
        "\n",
        "print(len(train_list), len(val_list), num_train, num_val)\n",
        "print(len(full_file_list) * num_train/100)\n",
        "for ii in train_list:\n",
        "    print(ii)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzxqwcYqcfdE"
      },
      "source": [
        "#SET UP TRAINING DATA "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-0XJgWWHnPz"
      },
      "source": [
        "#load up with the pre-sized patch images\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "'''\n",
        "training_data = CustomDataset(img_dir=annotated_dir,\n",
        "                                label_data = first50,\n",
        "                                category = '', #full_category_name, \n",
        "                                file_count=len(full_file_list), #full_file_count,\n",
        "                                file_list = full_file_list, \n",
        "                                transform=None, \n",
        "                                target_transform=None)\n",
        "'''\n",
        "\n",
        "training_data = CustomDataset(img_dir=annotated_dir,\n",
        "                                label_data = train_labels,\n",
        "                                category = '', #full_category_name, \n",
        "                                file_count=len(train_list), #full_file_count,\n",
        "                                file_list = train_list, \n",
        "                                transform=None, \n",
        "                                target_transform=None)\n",
        "\n",
        "\n",
        "\n",
        "validation_data = CustomDataset(img_dir=annotated_dir,\n",
        "                                label_data = val_labels,\n",
        "                                category = '', #full_category_name, \n",
        "                                file_count=len(val_list), #full_file_count,\n",
        "                                file_list = val_list, \n",
        "                                transform=None, \n",
        "                                target_transform=None)\n",
        "\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx0rFlO-JRMV"
      },
      "source": [
        "if (train_on_gpu == 1):\n",
        "    m = nn.LogSoftmax(dim=1).cuda(dev)\n",
        "    nll_loss = nn.NLLLoss().cuda(dev)\n",
        "else:\n",
        "    m = nn.LogSoftmax(dim=1)\n",
        "    nll_loss = nn.NLLLoss()\n",
        "\n",
        "L1loss = nn.L1Loss()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snJpkHyrCBz1"
      },
      "source": [
        "# Model Import/Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szM2cIAhiXVc"
      },
      "source": [
        "#RESNET 50 FPN PRE-TRAINED  w/ CUSTOM BACKBONE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "eb017f4159ec42adb369e1b2f68f89d8",
            "3a67d81fd0664bbdb24be7c3cd9e3d7b",
            "b8c9897f04c245bab8f3423b4ced2d05",
            "f4713c478df149828349b4dd6a37df4b",
            "0a9d4f004158424785ff6f9994ceaaa8",
            "8416abd2914d42ebb3bd66c80ba016e9",
            "ae5f17d798bc48d5b1246fc185afd02c",
            "f542efdfbc084fb59e718a5c2a0cebe3",
            "6a6478e991a0498d9193abfb837b2c2d",
            "13e3f3a95af44e18a9f06325e2c26908",
            "2b9cd890ae294bc5a15d1f95573c5858"
          ]
        },
        "id": "H_W26LydiN3v",
        "outputId": "12e1ee48-73a9-4133-9bad-97c6c5e5f111"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "loss_func = nn.NLLLoss()\n",
        "\n",
        "#!pip3 -q install engine\n",
        "#from engine import train_one_epoch, evaluate\n",
        "#import utils\n",
        "\n",
        "anchor_generator = AnchorGenerator(sizes=((64, 128,192 , 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 0.75, 1.0),))\n",
        "num_classes = 2\n",
        "resnet50_fpn = torchvision.models.detection.maskrcnn_resnet50_fpn(num_classes=2)\n",
        "'''\n",
        "backbone_fpn = nn.Sequential(\n",
        "\tresnet50_fpn.backbone.body.conv1,\n",
        "\tresnet50_fpn.backbone.body.bn1,\n",
        "\tresnet50_fpn.backbone.body.relu,\n",
        "\tresnet50_fpn.backbone.body.maxpool,\n",
        "\tresnet50_fpn.backbone.body.layer1,\n",
        "\tresnet50_fpn.backbone.body.layer2,\n",
        "\tresnet50_fpn.backbone.body.layer3,\n",
        "\tresnet50_fpn.backbone.body.layer4\n",
        "\t)\n",
        "backbone_fpn.out_channels = 2048\n",
        "modelr = FasterRCNN(backbone_fpn,num_classes=2,rpn_anchor_generator=anchor_generator)\n",
        "'''\n",
        "\n",
        "resnet_net = torchvision.models.resnet50(pretrained=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "modules = list(resnet_net.children())[:-1]\n",
        "backbone = nn.Sequential(*modules)\n",
        "backbone.out_channels = 2048\n",
        "#backbone = resnet_fpn_backbone('resnet50', pretrained_backbone)\n",
        "modelr = FasterRCNN(backbone,num_classes=2,rpn_anchor_generator=anchor_generator)\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "# get the number of input features \n",
        "in_features = modelr.roi_heads.box_predictor.cls_score.in_features\n",
        "# define a new head for the detector with required number of classes\n",
        "modelr.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "\n",
        "#modelr.train() #eval()\n",
        "\n",
        "#kaggle setup\n",
        "# freeze the backbone (it will freeze the body and fpn params). We don't need to\n",
        "#updated any of the pre-trained weight parameters\n",
        "for param in modelr.parameters():\n",
        "    param.requires_grad = False \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# freeze the fc6 layer in roi_heads\n",
        "for p in modelr.roi_heads.box_head.fc6.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Add a fully connected layer at the end with two outputs for our classes\n",
        "#modelr.fc = nn.Sequential(\n",
        "#               nn.Linear(2048, 128),\n",
        "#               nn.ReLU(inplace=True),\n",
        "#               nn.Linear(128, 2)).to(dev)\n",
        "\n",
        "\n",
        "#fc_inputs = modelr.fc.in_features\n",
        "modelr.fc = nn.Sequential(\n",
        "    nn.Linear(2048, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, 2),\n",
        "    nn.LogSoftmax(dim=1) # For using NLLLoss()\n",
        ")\n",
        "for param in modelr.fc:\n",
        "    param.requires_grad = True \n",
        "\n",
        "\n",
        "for p in modelr.rpn.parameters():\n",
        "    print(p)\n",
        "    p.requires_grad = True\n",
        "\n",
        "print('Model Params with Grad=True')\n",
        "for n, param in modelr.named_parameters():\n",
        "  if param.requires_grad==True:\n",
        "    print(n)\n",
        "\n",
        "#for param in modelr.fc():\n",
        "#    param.requires_grad==True\n",
        "\n",
        "optimizer = optim.Adam(modelr.fc.parameters())\n",
        "\n",
        "\n",
        "if (train_on_gpu):\n",
        "    modelr = modelr.to(dev)\n",
        "\n",
        "\n",
        "batch_size = 5\n",
        "data_loader = torch.utils.data.DataLoader(training_data,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=True,\n",
        "                                            num_workers=2,\n",
        "                                          drop_last=True)\n",
        "\n",
        "\n",
        "data__val_loader = torch.utils.data.DataLoader(validation_data,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=True,\n",
        "                                            num_workers=2,\n",
        "                                          drop_last=True)\n",
        "\n",
        "# define list to store the smooth loss and learning rate\n",
        "smooth_loss_list  = []\n",
        "lr_list = []\n",
        "batch_num = 0\n",
        "avg_loss = 0\n",
        "best_loss = 0\n",
        "\n",
        "for epoch in range(0,1):\n",
        "    epoch_loss = []\n",
        "    #with torch.no_grad(): #trying enabled\n",
        "    with torch.set_grad_enabled(True):\n",
        "        for i, data in enumerate(data_loader, 0):\n",
        "            train_loss =0\n",
        "            print('------------------------------------dataload loop is ',i)\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            #inputs, labels = data\n",
        "            \n",
        "            \n",
        "            modelr.train() #eval()\n",
        "\n",
        "            images = data[0]\n",
        "            targets = data[1]\n",
        "            #targets = targets.to(dev)\n",
        "            #np.squeeze(targets[0]['boxes'], 1)\n",
        "\n",
        "            \n",
        "\n",
        "            imagelist=[]\n",
        "            for ii in range(0,len(images)):\n",
        "                #imagelist.append(images[ii])\n",
        "                if (train_on_gpu):\n",
        "                    imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32).to(dev))\n",
        "                else:\n",
        "                    imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32))\n",
        "            print(np.shape(images), len(images))\n",
        "            print(np.shape(imagelist[0]))\n",
        "\n",
        "\n",
        "            #\n",
        "            # Reformat the target dictionary\n",
        "            #\n",
        "            target_new = []\n",
        "            for ii in range(0,batch_size):\n",
        "                d={}\n",
        "                #d['boxes'] = targets[0]['boxes'][ii]\n",
        "                if (train_on_gpu == 1):\n",
        "                    d['boxes'] = torch.as_tensor(targets[0]['boxes'][ii]).to(dev)\n",
        "                    d['labels'] = torch.as_tensor(targets[0]['labels'][ii]).to(dev)\n",
        "                    d['image_id'] = torch.as_tensor(targets[0]['image_id'][ii]).to(dev)\n",
        "                    d['area'] = torch.as_tensor(targets[0]['area'][ii]).to(dev)\n",
        "                else:\n",
        "                    d['boxes'] = targets[0]['boxes'][ii]\n",
        "                    d['labels'] = targets[0]['labels'][ii]\n",
        "                    d['image_id'] = targets[0]['image_id'][ii]\n",
        "                    d['area'] = targets[0]['area'][ii]                  \n",
        "                target_new.append(d)\n",
        "\n",
        "            #Loss setup \n",
        "            #criterion = nn.CrossEntropyLoss()\n",
        "            criterion = nn.NLLLoss()\n",
        "                        #nn.L1Loss()\n",
        "            if (train_on_gpu ==1):\n",
        "                criterion.cuda(dev)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #forward pass\n",
        "            loss_dict = modelr(imagelist, target_new) #targets)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            \n",
        "            print('*** SUMMED losses is ', losses)\n",
        "            print(loss_dict)\n",
        "            train_loss += losses\n",
        "#            _, preds = torch.max(loss_dict, 1)\n",
        "\n",
        "\n",
        "            #print(nn.Softmax2d(out_custom['loss_classifier']))\n",
        "            #criterion(out_custom, targets)\n",
        "\n",
        "\n",
        "            #loss = criterion(out_custom, targets[0]['labels'])\n",
        "\n",
        "                # backward pass\n",
        "            losses.backward()\n",
        "            # update optimizer\n",
        "            optimizer.step()\n",
        "            #nn.loss1.backward()\n",
        "\n",
        "        ###############################################\n",
        "        # VALIDATION STEP\n",
        "        ###############################################\n",
        "            if (1):\n",
        "                modelr.eval()     # Optional when not using Model Specific layer\n",
        "                valid_loss = 0.0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for ii, vdata in enumerate(dataloader_validation, 0):\n",
        "\n",
        "\n",
        "                        val_images = vdata[0]\n",
        "                        val_targets = vdata[1]\n",
        "                        #targets = targets.to(dev)\n",
        "                        #np.squeeze(targets[0]['boxes'], 1)\n",
        "\n",
        "                        \n",
        "\n",
        "                        val_imagelist=[]\n",
        "                        for kk in range(0,len(val_images)):\n",
        "                            #imagelist.append(images[ii])\n",
        "                            if (train_on_gpu):\n",
        "                                val_imagelist.append(torch.as_tensor(val_images[kk], dtype=torch.float32).to(dev))\n",
        "                            else:\n",
        "                                val_imagelist.append(torch.as_tensor(val_images[kk], dtype=torch.float32))\n",
        "                        print(np.shape(val_images), len(val_images))\n",
        "                        print(np.shape(val_imagelist[0]))\n",
        "\n",
        "\n",
        "\n",
        "                        #\n",
        "                        # Reformat the target dictionary\n",
        "                        #\n",
        "                        target_val = []\n",
        "                        for ii in range(0,batch_size):\n",
        "                            d={}\n",
        "                            #d['boxes'] = targets[0]['boxes'][ii]\n",
        "                            if (train_on_gpu == 1):\n",
        "                                d['boxes'] = torch.as_tensor(val_targets[0]['boxes'][ii]).to(dev)\n",
        "                                d['labels'] = torch.as_tensor(val_targets[0]['labels'][ii]).to(dev)\n",
        "                                d['image_id'] = torch.as_tensor(val_targets[0]['image_id'][ii]).to(dev)\n",
        "                                d['area'] = torch.as_tensor(val_targets[0]['area'][ii]).to(dev)\n",
        "                            else:\n",
        "                                d['boxes'] = val_targets[0]['boxes'][ii]\n",
        "                                d['labels'] = val_targets[0]['labels'][ii]\n",
        "                                d['image_id'] = val_targets[0]['image_id'][ii]\n",
        "                                d['area'] = val_targets[0]['area'][ii]                  \n",
        "                            target_val.append(d)\n",
        "\n",
        "\n",
        "                            criterion = nn.NLLLoss()\n",
        "                                        #nn.L1Loss()\n",
        "                            if (train_on_gpu ==1):\n",
        "                                criterion.cuda(dev)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                            #forward pass\n",
        "                            loss_dict = modelr(val_imagelist) #targets)\n",
        "                            # zero the parameter gradients\n",
        "                            optimizer.zero_grad()\n",
        "                            \n",
        "                            losses = sum(loss for loss in loss_dict.values())\n",
        "                            \n",
        "                            print('*** SUMMED losses is ', losses)\n",
        "                            print(loss_dict)\n",
        "                            train_loss += losses\n",
        "                #            _, preds = torch.max(loss_dict, 1)\n",
        "\n",
        "                            stop\n",
        "                        # get the inputs; data is a list of [inputs, labels]\n",
        "                        #inputs, labels = data\n",
        "                        vinputs = vdata['image'].type(FloatTensor)\n",
        "                        vlabels = vdata['label'] #.type(FloatTensor)\n",
        "\n",
        "                        #assert(len(vlabels) == bsize), \"Wrong number of val labels\"\n",
        "\n",
        "                        if (train_on_gpu):\n",
        "                            vinputs, vlabels = vinputs.to(dev), vlabels.to(dev)\n",
        "\n",
        "                        target = model_vgg16(vinputs)\n",
        "\n",
        "                        #flatten for cross entropy loss\n",
        "                        target=torch.flatten(target, start_dim=1)\n",
        "                        loss = criterion(target,vlabels)\n",
        "                        valid_loss = loss.item()# * data.size(0)\n",
        "                        validation_loss[epoch].append(valid_loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    epoch_loss.append(train_loss/batch_size)\n",
        "        \n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb017f4159ec42adb369e1b2f68f89d8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[-3.4247e-03,  1.3412e-02, -6.6256e-03],\n",
            "          [ 1.2397e-02,  1.1618e-02, -1.9022e-02],\n",
            "          [ 6.1381e-03,  1.0584e-02,  6.7750e-03]],\n",
            "\n",
            "         [[-5.5181e-03,  2.4013e-03,  5.5899e-03],\n",
            "          [-5.6083e-04,  2.3257e-04, -1.4341e-02],\n",
            "          [ 6.6699e-03, -7.6832e-03, -1.1285e-02]],\n",
            "\n",
            "         [[ 4.8669e-03, -6.2805e-03,  3.0844e-03],\n",
            "          [ 4.5723e-03, -8.2697e-03,  1.2084e-02],\n",
            "          [ 2.1893e-02, -3.8473e-04, -1.6054e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.3399e-03, -3.9695e-03, -1.0886e-03],\n",
            "          [ 7.3726e-03, -7.8779e-04, -7.3428e-03],\n",
            "          [ 9.8785e-03,  3.3473e-03, -1.6629e-02]],\n",
            "\n",
            "         [[ 4.5548e-03, -3.6811e-03, -1.7846e-02],\n",
            "          [ 8.8746e-03, -1.0668e-02, -5.9155e-03],\n",
            "          [-4.3000e-03, -4.9429e-03,  1.3988e-02]],\n",
            "\n",
            "         [[-6.3120e-03, -9.3627e-04,  4.6813e-04],\n",
            "          [ 2.5296e-03,  2.5675e-03,  4.4233e-03],\n",
            "          [-2.3964e-03,  1.1522e-02, -1.2263e-02]]],\n",
            "\n",
            "\n",
            "        [[[-8.1768e-03,  9.3484e-03, -4.7431e-03],\n",
            "          [ 3.7052e-03,  5.0457e-03, -8.0572e-03],\n",
            "          [-1.7388e-02, -1.4063e-02,  1.0204e-02]],\n",
            "\n",
            "         [[ 1.3434e-02,  3.4031e-03,  9.0027e-03],\n",
            "          [ 1.4843e-03, -6.6341e-04,  2.3938e-03],\n",
            "          [ 1.2484e-02,  5.0398e-03, -2.1339e-03]],\n",
            "\n",
            "         [[-6.1382e-03, -1.6835e-03, -8.3155e-03],\n",
            "          [-2.1688e-02, -6.7452e-03,  6.9854e-03],\n",
            "          [-1.7825e-02,  2.6748e-03,  2.1553e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.9511e-02, -9.0487e-03,  4.0356e-03],\n",
            "          [ 8.5985e-03, -1.4236e-02,  5.7679e-03],\n",
            "          [-1.8848e-03, -6.2043e-03,  2.1144e-03]],\n",
            "\n",
            "         [[-1.2907e-03,  3.5018e-03, -1.9444e-02],\n",
            "          [ 1.6143e-02,  3.1894e-03,  1.6777e-02],\n",
            "          [-1.0009e-02, -2.3218e-03,  3.5173e-03]],\n",
            "\n",
            "         [[ 1.1701e-02,  1.1373e-02,  1.8806e-03],\n",
            "          [-2.6212e-02, -2.5621e-02,  1.7514e-02],\n",
            "          [-7.2240e-03,  3.4985e-03, -2.5972e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4651e-03,  4.7825e-04, -2.8168e-03],\n",
            "          [-2.5627e-03,  2.9129e-03,  1.1807e-02],\n",
            "          [-5.3471e-03, -9.1537e-03, -1.5519e-02]],\n",
            "\n",
            "         [[ 9.2430e-04,  5.0029e-03,  5.4382e-03],\n",
            "          [-2.6893e-03,  9.4081e-03, -8.3479e-03],\n",
            "          [-1.7907e-02, -2.5866e-02,  1.8628e-02]],\n",
            "\n",
            "         [[ 4.7857e-03, -6.9539e-03,  2.0291e-02],\n",
            "          [ 8.3350e-03, -6.3835e-03, -2.4616e-02],\n",
            "          [-1.2351e-03,  1.1495e-03, -1.1106e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.4102e-03,  2.5255e-02,  8.8075e-03],\n",
            "          [-5.0289e-03,  4.4256e-04, -2.5496e-02],\n",
            "          [ 1.5030e-02, -2.9085e-03,  9.9203e-03]],\n",
            "\n",
            "         [[-1.0960e-02, -1.1633e-02, -7.8825e-03],\n",
            "          [-1.0115e-02,  5.9141e-03,  7.4665e-03],\n",
            "          [-1.3135e-02, -1.2684e-02,  2.5722e-03]],\n",
            "\n",
            "         [[-4.3460e-03, -3.1015e-03, -8.7880e-03],\n",
            "          [ 4.1381e-03,  9.2102e-03, -3.1286e-03],\n",
            "          [-1.1239e-02, -2.2851e-03, -4.2023e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-4.6153e-03, -2.8331e-04,  5.5714e-04],\n",
            "          [-1.1257e-02,  1.8175e-03,  7.6937e-03],\n",
            "          [-3.2010e-03,  8.7782e-03,  5.7095e-03]],\n",
            "\n",
            "         [[-2.4574e-03, -6.9325e-03, -1.2251e-02],\n",
            "          [ 7.2308e-03,  9.2625e-03,  3.1916e-03],\n",
            "          [ 6.7464e-03, -8.3297e-03,  8.7535e-03]],\n",
            "\n",
            "         [[ 6.9709e-03,  8.5113e-03,  1.7998e-03],\n",
            "          [-9.6497e-03, -8.2968e-04, -2.2189e-03],\n",
            "          [-3.9084e-03,  8.1805e-03,  9.1943e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.8199e-03,  5.9567e-03,  8.1317e-03],\n",
            "          [ 3.1070e-03,  5.2104e-03,  9.4077e-03],\n",
            "          [ 8.5226e-03, -1.6668e-03, -6.5400e-03]],\n",
            "\n",
            "         [[ 1.2067e-02,  2.5032e-02,  1.5747e-02],\n",
            "          [-3.2575e-03, -5.7559e-03,  1.1898e-02],\n",
            "          [ 1.1937e-02,  2.7246e-03,  6.3402e-05]],\n",
            "\n",
            "         [[-1.1209e-02, -2.7143e-03, -2.6404e-03],\n",
            "          [-7.0808e-03, -2.3586e-03,  1.3437e-03],\n",
            "          [-8.4425e-04, -2.0578e-03,  6.5723e-03]]],\n",
            "\n",
            "\n",
            "        [[[-4.8383e-03,  6.5986e-03,  1.0786e-02],\n",
            "          [-5.5083e-03, -4.2855e-03, -2.7841e-03],\n",
            "          [ 1.6392e-02, -1.0248e-02,  5.7204e-03]],\n",
            "\n",
            "         [[-1.0251e-02, -1.8912e-03, -1.1929e-02],\n",
            "          [-1.5443e-02,  1.0163e-03, -1.1665e-04],\n",
            "          [ 3.8663e-03,  1.4337e-03, -6.8096e-03]],\n",
            "\n",
            "         [[-1.1954e-02, -1.7433e-02, -8.7833e-03],\n",
            "          [ 1.6034e-02,  5.2390e-03, -1.0237e-02],\n",
            "          [ 1.2884e-02, -2.0191e-02, -1.0874e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.1817e-03, -4.1469e-04, -1.4580e-03],\n",
            "          [ 9.7691e-03, -7.7004e-03,  3.3327e-03],\n",
            "          [-1.9572e-03, -1.0241e-02,  7.1171e-03]],\n",
            "\n",
            "         [[ 6.8726e-03,  2.1384e-02, -1.0433e-03],\n",
            "          [-7.4594e-03,  1.6396e-03, -3.1366e-03],\n",
            "          [-1.6149e-02, -1.9049e-02, -8.2780e-03]],\n",
            "\n",
            "         [[-8.0974e-03,  3.3416e-03, -9.5040e-03],\n",
            "          [ 6.4208e-03,  1.5340e-02,  7.6883e-03],\n",
            "          [-1.0125e-02,  5.4410e-04, -2.9356e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 7.1560e-04,  8.2597e-04,  6.5798e-04],\n",
            "          [-2.5270e-02, -1.2538e-02,  5.6508e-03],\n",
            "          [-1.0071e-02, -1.1495e-02,  8.4116e-03]],\n",
            "\n",
            "         [[-2.4929e-03,  8.4484e-04, -1.7987e-02],\n",
            "          [ 4.4163e-03,  1.3433e-02,  1.2479e-02],\n",
            "          [-1.0282e-02,  6.1972e-03, -2.0163e-03]],\n",
            "\n",
            "         [[ 1.6203e-02, -5.2350e-03, -2.0927e-02],\n",
            "          [ 1.1528e-02, -6.4396e-03, -8.2324e-03],\n",
            "          [-2.7821e-02,  9.2684e-04, -1.1181e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7869e-02,  1.0197e-02,  6.2030e-03],\n",
            "          [-1.6025e-02, -4.7422e-03,  6.7574e-03],\n",
            "          [ 4.3945e-03, -1.5973e-02,  7.8648e-03]],\n",
            "\n",
            "         [[-5.1004e-03,  2.5156e-03,  5.1608e-04],\n",
            "          [ 1.6748e-02, -1.2835e-02,  8.0128e-03],\n",
            "          [-2.8970e-03,  7.7403e-03,  1.6010e-02]],\n",
            "\n",
            "         [[-1.5556e-02,  3.4285e-03,  3.8198e-03],\n",
            "          [-1.2204e-03, -4.4581e-04,  1.1033e-02],\n",
            "          [ 1.1766e-02, -2.4744e-03,  1.0769e-02]]]])\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0188]],\n",
            "\n",
            "         [[ 0.0186]],\n",
            "\n",
            "         [[-0.0120]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0053]],\n",
            "\n",
            "         [[-0.0170]],\n",
            "\n",
            "         [[ 0.0142]]],\n",
            "\n",
            "\n",
            "        [[[-0.0101]],\n",
            "\n",
            "         [[ 0.0008]],\n",
            "\n",
            "         [[ 0.0081]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0034]],\n",
            "\n",
            "         [[ 0.0138]],\n",
            "\n",
            "         [[-0.0174]]],\n",
            "\n",
            "\n",
            "        [[[-0.0122]],\n",
            "\n",
            "         [[-0.0178]],\n",
            "\n",
            "         [[ 0.0142]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0055]],\n",
            "\n",
            "         [[ 0.0071]],\n",
            "\n",
            "         [[-0.0002]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0110]],\n",
            "\n",
            "         [[-0.0080]],\n",
            "\n",
            "         [[-0.0061]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0052]],\n",
            "\n",
            "         [[ 0.0008]],\n",
            "\n",
            "         [[ 0.0064]]],\n",
            "\n",
            "\n",
            "        [[[-0.0026]],\n",
            "\n",
            "         [[ 0.0017]],\n",
            "\n",
            "         [[ 0.0041]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0060]],\n",
            "\n",
            "         [[ 0.0031]],\n",
            "\n",
            "         [[ 0.0118]]],\n",
            "\n",
            "\n",
            "        [[[-0.0132]],\n",
            "\n",
            "         [[ 0.0069]],\n",
            "\n",
            "         [[-0.0047]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0006]],\n",
            "\n",
            "         [[ 0.0094]],\n",
            "\n",
            "         [[ 0.0034]]]])\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Parameter containing:\n",
            "tensor([[[[-1.5497e-02]],\n",
            "\n",
            "         [[ 1.3469e-03]],\n",
            "\n",
            "         [[-9.0681e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.3633e-03]],\n",
            "\n",
            "         [[ 3.5703e-03]],\n",
            "\n",
            "         [[ 1.0008e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 9.2163e-05]],\n",
            "\n",
            "         [[-2.1589e-02]],\n",
            "\n",
            "         [[-1.1126e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.2105e-02]],\n",
            "\n",
            "         [[ 1.0325e-02]],\n",
            "\n",
            "         [[ 9.4856e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7516e-03]],\n",
            "\n",
            "         [[ 2.8168e-03]],\n",
            "\n",
            "         [[ 4.5634e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.7896e-03]],\n",
            "\n",
            "         [[ 8.7731e-03]],\n",
            "\n",
            "         [[ 1.0945e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.5786e-02]],\n",
            "\n",
            "         [[-4.9133e-03]],\n",
            "\n",
            "         [[ 1.3086e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.1870e-02]],\n",
            "\n",
            "         [[ 1.3485e-02]],\n",
            "\n",
            "         [[-1.0614e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.5794e-02]],\n",
            "\n",
            "         [[-1.4225e-02]],\n",
            "\n",
            "         [[ 8.8817e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.0620e-03]],\n",
            "\n",
            "         [[-4.6046e-03]],\n",
            "\n",
            "         [[-1.9863e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 8.0220e-03]],\n",
            "\n",
            "         [[-9.5892e-04]],\n",
            "\n",
            "         [[-1.0503e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.7372e-03]],\n",
            "\n",
            "         [[-5.2171e-03]],\n",
            "\n",
            "         [[-3.6911e-03]]]])\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Model Params with Grad=True\n",
            "rpn.head.conv.weight\n",
            "rpn.head.conv.bias\n",
            "rpn.head.cls_logits.weight\n",
            "rpn.head.cls_logits.bias\n",
            "rpn.head.bbox_pred.weight\n",
            "rpn.head.bbox_pred.bias\n",
            "fc.0.weight\n",
            "fc.0.bias\n",
            "fc.3.weight\n",
            "fc.3.bias\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (17, 595) 1505\n",
            "image after crop is  (17, 595)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (364, 613) 2936\n",
            "image after crop is  (364, 613)\n",
            "resized shapes are  600 800\n",
            "corners  [(375, 189), (568, 189), (568, 344), (375, 344)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(30107., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (458, 580) 758\n",
            "image after crop is  (458, 580)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (205, 624) 4958\n",
            "image after crop is  (205, 624)\n",
            "resized shapes are  600 800\n",
            "corners  [(180, 106), (728, 106), (728, 465), (180, 465)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(185070., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (303, 609) 2821\n",
            "image after crop is  (303, 609)\n",
            "resized shapes are  600 800\n",
            "corners  [(288, 196), (563, 196), (563, 352), (288, 352)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(46428., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (361, 605) 1706\n",
            "image after crop is  (361, 605)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (303, 478) 2623\n",
            "image after crop is  (303, 478)\n",
            "resized shapes are  600 800\n",
            "corners  [(347, 210), (536, 210), (536, 298), (347, 298)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(5140., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (367, 706) 4531\n",
            "image after crop is  (367, 706)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (458, 461) 2445\n",
            "image after crop is  (458, 461)\n",
            "resized shapes are  600 800\n",
            "corners  [(197, 125), (655, 125), (655, 435), (197, 435)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(121535., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (421, 606) 4362\n",
            "image after crop is  (421, 606)\n",
            "resized shapes are  600 800\n",
            "corners  [(160, 152), (672, 152), (672, 364), (160, 364)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(109728., dtype=torch.float64)\n",
            "------------------------------------dataload loop is  0\n",
            "torch.Size([5, 3, 600, 800]) 5\n",
            "torch.Size([3, 600, 800])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** SUMMED losses is  tensor(1.6386, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "{'loss_classifier': tensor(0.7170, device='cuda:0'), 'loss_box_reg': tensor(0.0010, device='cuda:0'), 'loss_objectness': tensor(0.7002, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.2204, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (301, 611) 1368\n",
            "image after crop is  (301, 611)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (284, 612) 968\n",
            "image after crop is  (284, 612)\n",
            "resized shapes are  600 800\n",
            "corners  [(398, 234), (556, 234), (556, 342), (398, 342)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(16008., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (335, 609) 5340\n",
            "image after crop is  (335, 609)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (279, 613) 1033\n",
            "image after crop is  (279, 613)\n",
            "resized shapes are  600 800\n",
            "corners  [(395, 217), (631, 217), (631, 359), (395, 359)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(35432., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (366, 611) 1698\n",
            "image after crop is  (366, 611)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (424, 606) 4108\n",
            "image after crop is  (424, 606)\n",
            "resized shapes are  600 800\n",
            "corners  [(160, 153), (582, 153), (582, 365), (160, 365)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(89880., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (362, 706) 4756\n",
            "image after crop is  (362, 706)\n",
            "resized shapes are  600 800\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([0]) <class 'torch.Tensor'>\n",
            "area is  tensor(25., dtype=torch.float64)\n",
            "image shape read in is  (600, 800, 3)\n",
            "about to resize image. shape going in is @ (361, 604) 2920\n",
            "image after crop is  (361, 604)\n",
            "resized shapes are  600 800\n",
            "corners  [(375, 189), (568, 189), (568, 344), (375, 344)]\n",
            "shape of boxes is  torch.Size([1, 4])\n",
            "setting target\n",
            "label is  tensor([1]) <class 'torch.Tensor'>\n",
            "area is  tensor(30107., dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-9b7881a14090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# VALIDATION STEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m###############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miteration_counter\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0mmodelr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Optional when not using Model Specific layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'iteration_counter' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK4AdvJxevK6"
      },
      "source": [
        "for p in modelr.roi_heads.box_head.parameters():\n",
        "    print(p)\n",
        "    #p.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEGA08hhzaCS"
      },
      "source": [
        "summary(modelr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4IswwLpjunl"
      },
      "source": [
        "for p in modelr.rpn.parameters():\n",
        "    print(p)\n",
        "    p.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTEVY6GtocP2"
      },
      "source": [
        "loss_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoHox6spRHU7"
      },
      "source": [
        "#print(summary(modelr,(3,244,244), batch_size = 5))\n",
        "#print(modelr)\n",
        "len(targets)\n",
        "tsave  = targets\n",
        "\n",
        "print(targets[0].keys())\n",
        "target_new = []\n",
        "for ii in range(0,5):\n",
        "    d={}\n",
        "    d['boxes'] = targets[0]['boxes'][ii]\n",
        "    d['labels'] = targets[0]['labels'][ii]\n",
        "    d['image_id'] = targets[0]['image_id'][ii]\n",
        "    d['area'] = targets[0]['area'][ii]\n",
        "    target_new.append(d)\n",
        "\n",
        "len(target_new)\n",
        "target_new[4]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwQAjjiSffsI"
      },
      "source": [
        "print(loss_dict)\n",
        "ll=nn.CrossEntropyLoss(loss_dict['loss_classifier'])\n",
        "print(ll)\n",
        "ll.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKoAfTd5cKjR"
      },
      "source": [
        "summary(modelr)\n",
        "fname = full_file_list[1419]\n",
        "print(fname)\n",
        "img_data = image.imread(fname) #'/content/gdrive/Shareddrives/BreastUS/Annotated data/unzipped_updated/1_srh8gfhj_a_0629s3fh_0.mp4-2021_07_15_17_21_15-labelme 3.0.zip/default/frame_000030.PNG')\n",
        "plt.figure()\n",
        "plt.imshow(img_data[:,:,0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Afis5F7nwH7"
      },
      "source": [
        "idx = targets['image_id']\n",
        "print(targets['boxes'])\n",
        "full_file_list[idx[0]]\n",
        "len(bounding_box)\n",
        "print(len(full_file_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trATj4Qg6yHc"
      },
      "source": [
        "idx=targets['image_id']\n",
        "print(idx)\n",
        "\n",
        "for fcount,ival in enumerate(idx): #imagelist):\n",
        "    print(full_file_list[ival])\n",
        "    idata = images[fcount]\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.imshow(idata[0,:,:].cpu(),cmap='gray')\n",
        "\n",
        "    cancer_status = targets['labels'][fcount]\n",
        "    if (cancer_status == 0):\n",
        "        clabel = 'Benign'\n",
        "    else:\n",
        "        clabel = 'Malignant'\n",
        "    findex=full_file_list[ival].find('updated/')\n",
        "    sname = full_file_list[ival][findex+8:]\n",
        "    findex = sname.find('/')\n",
        "    sname = sname[:findex] +'_' + str(int(ival)) + '_' + clabel\n",
        "    plt.title(sname)\n",
        "\n",
        "\n",
        "\n",
        "    for counter,blist in enumerate(range(0,len(out_custom[fcount]['boxes']))): #out['boxes']):\n",
        "        for ii in range(0,blist):\n",
        "            points = out_custom[fcount]['boxes'][ii]\n",
        "\n",
        "            #ii = out[counter]['boxes'][0]\n",
        "            newbox = [torch.detach(points[0]),torch.detach(points[1]),torch.detach(points[2]),torch.detach(points[3])]\n",
        "            #idata = imagelist[counter]\n",
        "\n",
        "            rect = patches.Rectangle((np.uint(newbox[0].cpu()),\n",
        "                                    np.uint(newbox[1].cpu())),\n",
        "                                    np.uint(newbox[2].cpu())-(np.uint(newbox[0].cpu())),\n",
        "                                    np.uint(newbox[3].cpu())-(np.uint(newbox[1].cpu())),\n",
        "                                    linewidth=0.8,\n",
        "                                    edgecolor='r',\n",
        "                                    facecolor='none')\n",
        "\n",
        "\n",
        "\n",
        "                # Get the current reference\n",
        "            ax = plt.gca()\n",
        "                # Add the patch to the Axes\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "\n",
        "    #add annotation box\n",
        "    points = targets['boxes'][fcount]\n",
        "    print('points are ', points)\n",
        "    rect = patches.Rectangle((np.uint(points[0][0].cpu()),\n",
        "                    np.uint(points[0][1].cpu())),\n",
        "                    np.uint(points[0][2].cpu())-(np.uint(points[0][0].cpu())),\n",
        "                    np.uint(points[0][3].cpu())-(np.uint(points[0][1].cpu())),\n",
        "                    linewidth=0.8,\n",
        "                    edgecolor='g',\n",
        "                    facecolor='none')\n",
        "    # Get the current reference\n",
        "    ax = plt.gca()\n",
        "        # Add the patch to the Axes\n",
        "    ax.add_patch(rect)\n",
        "    #add annotation box\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbFL--9jDzoL"
      },
      "source": [
        "targets['labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dH9NaLGRXNg"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "#del resnet50\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "#from torchsummary import summary\n",
        "!pip3 install -q torchinfo\n",
        "\n",
        "from torchinfo import summary as s2 #works with lists of tensors\n",
        "\n",
        "# load a model pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 2  # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "\n",
        "if (train_on_gpu ==1):\n",
        "    model=model.to(dev)\n",
        "else:\n",
        "    pass\n",
        "resnet50= model #.roi_heads.box_predictor\n",
        "resnet50.eval()\n",
        "#summary(resnet50,[(3, 600, 600)])\n",
        "print(model)\n",
        "#summary(resnet50,(3,800,800))\n",
        "a=sum([param.nelement() for param in model.parameters()])\n",
        "print(a)\n",
        "print(model.rpn)\n",
        "\n",
        "bb=10\n",
        "s2(model,input_size=(bb, 3, 800, 800))\n",
        "del resnet50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oySb7-V-R7H"
      },
      "source": [
        "in_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSD6PjUhsIj1"
      },
      "source": [
        "### -------------------DEBUG NEW MODEL TYPES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_Zc70YmRZl3"
      },
      "source": [
        "%pdb off\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "#el model\n",
        "\n",
        "#del backbone\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# load a pre-trained model for classification and return\n",
        "# only the features\n",
        "#backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "#backbone.out_channels = 1280\n",
        "%pdb off\n",
        "backbone = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
        "backbone.out_channels = 512\n",
        "\n",
        "#backbone = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "#backbone.out_channels = 1280\n",
        "#anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "anchor_generator = AnchorGenerator(sizes=((8, 16, 32, 64, 128),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# let's define what are the feature maps that we will\n",
        "# use to perform the region of interest cropping, as well as\n",
        "# the size of the crop after rescaling.\n",
        "# if your backbone returns a Tensor, featmap_names is expected to\n",
        "# be [0]. More generally, the backbone should return an\n",
        "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
        "# feature maps to use.\n",
        "''' mobilenet\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "'''\n",
        "\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(['feat1', 'feat3'], 3, 2)\n",
        "from collections import OrderedDict\n",
        "i = OrderedDict()\n",
        "i['feat1'] = torch.rand(1, 5, 64, 64)\n",
        "i['feat2'] = torch.rand(1, 5, 32, 32)  # this feature won't be used in the pooling\n",
        "i['feat3'] = torch.rand(1, 5, 16, 16)\n",
        "# create some random bounding boxes\n",
        "boxes = torch.rand(6, 4) * 256; boxes[:, 2:] += boxes[:, :2]\n",
        "# original image size, before computing the feature maps\n",
        "image_sizes = [(512, 512)]\n",
        "output = roi_pooler(i, [boxes], image_sizes)\n",
        "print(output.shape)\n",
        "\n",
        "\n",
        "\n",
        "# put the pieces together inside a FasterRCNN model\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=2,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "if (train_on_gpu ==1):\n",
        "    model=model.to(dev)\n",
        "else:\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "# For training\n",
        "images= torch.rand(1, 3, 600, 1200) \n",
        "boxes= torch.Tensor([[[10,10,100,100]]])  #torch.rand(1, 11,4)*100 #11, 4)\n",
        "\n",
        "labels = torch.randint(1, 91, (4, 11))\n",
        "#images = list(image for image in images)\n",
        "\n",
        "### Moving image data to the device\n",
        "imagelist=[]\n",
        "for ii in range(0,len(images)):\n",
        "    #imagelist.append(images[ii])\n",
        "    if (train_on_gpu):\n",
        "        imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32).to(dev))\n",
        "    else:\n",
        "        imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "targets = []\n",
        "for i in range(len(images)):\n",
        "    d = {}\n",
        "    d['boxes'] = boxes[i]\n",
        "    d['labels'] = labels[i]\n",
        "    targets.append(d)\n",
        "\n",
        "output = model(imagelist) #, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QX330WaZBLg"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer,\n",
        "                                       step_size=7,\n",
        "                                       gamma=0.1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#training_data set above by calling Custom Dataset\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(training_data,\n",
        "                                            batch_size=10,\n",
        "                                            shuffle=False, #True,\n",
        "                                            num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "images,targets = next(iter(data_loader))\n",
        "#images = list(image for image in images)\n",
        "\n",
        "imagelist=[]\n",
        "for ii in range(0,len(images)):\n",
        "    #imagelist.append(images[ii])\n",
        "    if (train_on_gpu):\n",
        "        imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32).to(dev))\n",
        "    else:\n",
        "        imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32))\n",
        "print(np.shape(images), len(images))\n",
        "print(np.shape(imagelist[0]))\n",
        "\n",
        "tdata=[]\n",
        "ddata={}\n",
        "\n",
        "import tensorflow as tf\n",
        "for ii in range(0,len(targets['boxes'])):\n",
        "    if (train_on_gpu):\n",
        "        ddata['boxes'] = targets['boxes'][ii].to(dev) #torch.DoubleTensor(targets['boxes'][ii])\n",
        "\n",
        "        ddata['labels']=targets['labels'][ii].to(dev)\n",
        "        ddata['area'] = targets['area'][ii].to(dev)\n",
        "        tdata.append(ddata)\n",
        "    else:\n",
        "        ddata['boxes'] = targets['boxes'][ii] #torch.DoubleTensor(targets['boxes'][ii])\n",
        "\n",
        "        ddata['labels']=targets['labels'][ii]\n",
        "        ddata['area'] = targets['area'][ii]\n",
        "        tdata.append(ddata)\n",
        "\n",
        "\n",
        "if (train_on_gpu ==1):\n",
        "    imagelist = [ t.to(dev) for t in imagelist ]\n",
        "#target_list = [ {'boxes':d['boxes'].to(device), 'labels':d['labels']} for d in target_list ]\n",
        "\n",
        "model.eval()  # Set model to training mode\n",
        "\n",
        "print('!!! FORWARD PASS !!!!')\n",
        "out=model(imagelist,tdata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44H7SaIHtRDi"
      },
      "source": [
        "model_children = list(model.children())\n",
        "print(model_children)\n",
        "no_of_layers=0\n",
        "conv_layers=[]\n",
        "\n",
        "rpn = model_children[2]\n",
        "#print(model_children[2])\n",
        "\n",
        "\n",
        "img = np.zeros((10,3,600,600))\n",
        "\n",
        "img= np.float32(img)\n",
        "result = model(torch.tensor(img).to(dev))\n",
        "\n",
        "'''\n",
        "    print('--  ',type(child))\n",
        "    if type(child)==nn.Conv2d:\n",
        "        no_of_layers+=1\n",
        "        conv_layers.append(child)\n",
        "    elif type(child)==nn.Sequential:\n",
        "        for layer in child.children():\n",
        "            if type(layer)==nn.Conv2d:\n",
        "                no_of_layers+=1\n",
        "                conv_layers.append(layer)\n",
        "print(no_of_layers)\n",
        "\n",
        "\n",
        "results = [conv_layers[0](img)]\n",
        "for i in range(1, len(conv_layers)):\n",
        "    results.append(conv_layers[i](results[-1]))\n",
        "outputs = results\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG5KgWUumXQD"
      },
      "source": [
        "model.roi_heads.box_predictor.cls_score(torch.tensor(img).to(dev))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VaveGUssEz1"
      },
      "source": [
        "### RUN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcRYVBE5TrgV"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# load a pre-trained model for classification and return\n",
        "# only the features\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "# FasterRCNN needs to know the number of\n",
        "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
        "# so we need to add it here\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "# let's make the RPN generate 5 x 3 anchors per spatial\n",
        "# location, with 5 different sizes and 3 different aspect\n",
        "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
        "# map could potentially have different sizes and\n",
        "# aspect ratios\n",
        "#anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "anchor_generator = AnchorGenerator(sizes=((8, 16, 32, 64, 128),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# let's define what are the feature maps that we will\n",
        "# use to perform the region of interest cropping, as well as\n",
        "# the size of the crop after rescaling.\n",
        "# if your backbone returns a Tensor, featmap_names is expected to\n",
        "# be [0]. More generally, the backbone should return an\n",
        "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
        "# feature maps to use.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "# put the pieces together inside a FasterRCNN model\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=2,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLWa6D72_kQB"
      },
      "source": [
        "'''\n",
        "\n",
        "#Show summary of model setup and move model to the GPU\n",
        " #train_on_gpu = torch.cuda.is_available()\n",
        "from torchsummary import summary\n",
        "\n",
        "if (train_on_gpu == 1):\n",
        "    #dev=torch.device(\"cuda\") \n",
        "    model.to(dev)\n",
        "    model.eval()\n",
        "    summary(model,(3,600,800), batch_size = 300, device='cuda')\n",
        "elif ( (train_on_tpu == 1) and (train_on_gpu == 0)):\n",
        "    ### TPU with pytorch has some issues\n",
        "    model_vgg16.to(dev)\n",
        "    summary(model_vgg16,(3,244,244), batch_size = bsize, device=dev)\n",
        "else:\n",
        "    summary(model,(3,600,800), batch_size = bsize)\n",
        "'''\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWURd5-GuUAJ"
      },
      "source": [
        "#MODEL RUN\n",
        "These are the assertions for the pretrained model\n",
        "'''\n",
        "    739                 floating_point_types = (torch.float, torch.double, torch.half)\n",
        "    740                 assert t[\"boxes\"].dtype in floating_point_types, 'target boxes must of float type'\n",
        "--> 741                 assert t[\"labels\"].dtype == torch.int64, 'target labels must of int64 type'\n",
        "    742                 if self.has_keypoint():\n",
        "    743                     assert t[\"keypoints\"].dtype == torch.float32, 'target keypoints must of float type'\n",
        "'''\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaO7Cn-VT4z2"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "\n",
        "#model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "#dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 2  # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "\n",
        "#from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "train_on_gpu = 0\n",
        "if (train_on_gpu):\n",
        "    model.to(dev)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer,\n",
        "                                       step_size=7,\n",
        "                                       gamma=0.1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#training_data set above by calling Custom Dataset\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(training_data,\n",
        "                                            batch_size=10,\n",
        "                                            shuffle=False, #True,\n",
        "                                            num_workers=2)\n",
        "\n",
        "##dataloader_training = DataLoader(train_subset, batch_size=bsize,shuffle=True, num_workers=2) #only 2 workers for Colab CPU\n",
        "# For Training\n",
        "\n",
        "\n",
        "images,targets = next(iter(data_loader))\n",
        "#images = list(image for image in images)\n",
        "\n",
        "imagelist=[]\n",
        "for ii in range(0,len(images)):\n",
        "    #imagelist.append(images[ii])\n",
        "    if (train_on_gpu):\n",
        "        imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32).to(dev))\n",
        "    else:\n",
        "        imagelist.append(torch.as_tensor(images[ii], dtype=torch.float32))\n",
        "print(np.shape(images), len(images))\n",
        "print(np.shape(imagelist[0]))\n",
        "\n",
        "tdata=[]\n",
        "ddata={}\n",
        "\n",
        "import tensorflow as tf\n",
        "for ii in range(0,len(targets['boxes'])):\n",
        "    if (train_on_gpu):\n",
        "        ddata['boxes'] = targets['boxes'][ii].to(dev) #torch.DoubleTensor(targets['boxes'][ii])\n",
        "\n",
        "        ddata['labels']=targets['labels'][ii].to(dev)\n",
        "        ddata['area'] = targets['area'][ii].to(dev)\n",
        "        tdata.append(ddata)\n",
        "    else:\n",
        "        ddata['boxes'] = targets['boxes'][ii] #torch.DoubleTensor(targets['boxes'][ii])\n",
        "\n",
        "        ddata['labels']=targets['labels'][ii]\n",
        "        ddata['area'] = targets['area'][ii]\n",
        "        tdata.append(ddata)\n",
        "\n",
        "\n",
        "if (train_on_gpu ==1):\n",
        "    imagelist = [ t.to(dev) for t in imagelist ]\n",
        "#target_list = [ {'boxes':d['boxes'].to(device), 'labels':d['labels']} for d in target_list ]\n",
        "\n",
        "model.eval()  # Set model to training mode\n",
        "\n",
        "print('!!! FORWARD PASS !!!!')\n",
        "out=model(imagelist,tdata)\n",
        "#out = model(images, tdata)\n",
        "\n",
        "losses = sum(loss for loss in out.values())\n",
        "\n",
        "\n",
        "#targets_formatted = [{'boxes', targets['boxes'],\n",
        "#            'labels',targets['labels']}]\n",
        "#output = model(images,targets)   # Returns losses and detections\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)           # Returns predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfjUvHjXyl-4"
      },
      "source": [
        "#PLOT ANCHORS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYOXySHubZ9R"
      },
      "source": [
        "out[0]\n",
        "\n",
        "\n",
        "for fcount,numfiles in enumerate(range(0,len(out),1)):\n",
        "    idata = imagelist[fcount]\n",
        "    plt.figure(figsize=(8, 6), dpi=30)\n",
        "    plt.imshow(idata[0,:,:].cpu(),cmap='gray')\n",
        "    plt.title(str(fcount))\n",
        "\n",
        "\n",
        "    for counter,blist in enumerate(range(0,len(out[fcount]['boxes']))): #out['boxes']):\n",
        "        for ii in range(0,blist):\n",
        "            points = out[fcount]['boxes'][ii]\n",
        "\n",
        "            #ii = out[counter]['boxes'][0]\n",
        "            newbox = [torch.detach(points[0]),torch.detach(points[1]),torch.detach(points[2]),torch.detach(points[3])]\n",
        "            #idata = imagelist[counter]\n",
        "\n",
        "            rect = patches.Rectangle((np.uint(newbox[0].cpu()),\n",
        "                                    np.uint(newbox[1].cpu())),\n",
        "                                    np.uint(newbox[2].cpu())-(np.uint(newbox[0].cpu())),\n",
        "                                    np.uint(newbox[3].cpu())-(np.uint(newbox[1].cpu())),\n",
        "                                    linewidth=0.8,\n",
        "                                    edgecolor='r',\n",
        "                                    facecolor='none')\n",
        "\n",
        "\n",
        "\n",
        "                # Get the current reference\n",
        "            ax = plt.gca()\n",
        "                # Add the patch to the Axes\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "print('counter is ',counter)\n",
        "    \n",
        "#x = torchvision.conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(out) #(base_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RQc79uabOjo"
      },
      "source": [
        "idata = imagelist[5]\n",
        "plt.figure(figsize=(8, 6), dpi=80)\n",
        "plt.imshow(idata[0,:,:].cpu(),cmap='gray')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASlpLcLS3wJr"
      },
      "source": [
        "print(len(targets[\"boxes\"]))\n",
        "#print(targets['boxes'][\n",
        "\n",
        "fdata = []\n",
        "box_data={}\n",
        "label_data={}\n",
        "ddata={}\n",
        "\n",
        "for ii in range(0,3):\n",
        "    ddata['boxes']=targets['boxes'][ii]\n",
        "    ddata['labels']=targets['labels'][ii]\n",
        "    fdata.append(ddata)\n",
        "\n",
        "  \n",
        "\n",
        "print(fdata)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR9duulEKD8C"
      },
      "source": [
        "targets.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcxPKk2lfQLl"
      },
      "source": [
        "x = [torch.rand(3, 300, 400), torch.rand(3, 300, 400)]\n",
        "a = torch.tensor([[100,100,200,200],[110,110,350,400]], dtype=torch.float64)\n",
        "b = torch.tensor([1,2],dtype=torch.int64)\n",
        "targets2 = [{'boxes': a, 'labels':  b},\n",
        "            {'boxes': a, 'labels':  b}]\n",
        "#print(targets2)\n",
        "for target in targets2:\n",
        "    #print(target)\n",
        "    boxes = target[\"boxes\"]\n",
        "\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "targets_formatted = [{k: v for k, v in t.items()} for t in targets2]\n",
        "targets_formatted\n",
        "#imagesx = images[0:1]\n",
        "#output = model(imagesx,targets)   # Returns losses and detections\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQpUW1ZwZDgt"
      },
      "source": [
        "print(type(targets['boxes']))\n",
        "a={}\n",
        "a[\"t1\"]=4\n",
        "a[\"t2\"] = [1,2]\n",
        "a=list(a)\n",
        "for counter,aa in enumerate(a):\n",
        "    print('aa = ',aa)\n",
        "    for bb in aa.items():\n",
        "        print(bb,counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zO96z47PRis"
      },
      "source": [
        "#a = [{k: v for k, v in t.items()} for t in targets]\n",
        "a=targets[\"boxes\"]\n",
        "if isinstance(a,torch.Tensor):\n",
        "    print('is instance')\n",
        "for t in targets:\n",
        "    print(t)\n",
        "    for k,v in t.items():\n",
        "        print(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJJh5XVWgPmG"
      },
      "source": [
        "import torchvision\n",
        "#model = torchvision.models.vgg16(pretrained=True)\n",
        "model=torchvision.models.resnet50(pretrained=True)\n",
        "#fe = list(model.features)\n",
        "\n",
        "model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXMzB_4zGlAc"
      },
      "source": [
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=3,\n",
        "                                                gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc-2gzhSHesm"
      },
      "source": [
        "    num_epochs = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # train for one epoch, printing every 10 iterations\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "        # update the learning rate\n",
        "        lr_scheduler.step()\n",
        "        # evaluate on the test dataset\n",
        "        evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "    print(\"That's it!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KDkNEYNt5Fi"
      },
      "source": [
        "import json\n",
        "#x = box_info\n",
        "#json.loads(x)\n",
        "#type(box_info)\n",
        "\n",
        "from ast import literal_eval #as make_tuple\n",
        "a=literal_eval(box_info)\n",
        "print(a[0])\n",
        "type(a[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WJ5mZROnju7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgzzBJKxPUtV"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# Create a ZipFile Object and load sample.zip in it\n",
        "full_file = os.path.join(label_data_dir, label_files[0])\n",
        "with ZipFile(full_file, 'r') as zipObj:\n",
        "   # Get a list of all archived file names from the zip\n",
        "   listOfFileNames = zipObj.namelist()\n",
        "   for fnames in listOfFileNames:\n",
        "       print(fnames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQxlbtgCRovN"
      },
      "source": [
        "#SCRATCH AREA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZIOIkPGR1H8"
      },
      "source": [
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=''):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "        # convert the PIL Image into a numpy array\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4El0WZb2SAL7"
      },
      "source": [
        "import transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZK1RHwiRrTn"
      },
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "dataset = PennFudanDataset('PennFudanPed')# get_transform(train=True))\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        " dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        " collate_fn=utils.collate_fn)\n",
        "# For Training\n",
        "images,targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "output = model(images,targets)   # Returns losses and detections\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)      "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}